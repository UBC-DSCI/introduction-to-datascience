{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textbook Data Retrieval "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides code to create the data sets used in this textbook. It also contains a summary of the data, their original source location, the license for the data, the date the data were accessed to generate the committed repository version of the original/processed data, and any other relevant meta-information.\n",
    "\n",
    "Running all cells in this notebook will:\n",
    "\n",
    "1. obtain all data from their original sources\n",
    "- process the raw data to create the clean copy\n",
    "- check that the cleaned data matches with a stored hash\n",
    "  - if it does not, a warning is raised to the user to possibly update the data, and the new data are stored in a temporary file for inspection\n",
    "- check that the stored data in the `data/` folder matches the stored hash\n",
    "  - if it does not, a warning is raised to check the stored data\n",
    "  \n",
    "In order to add a new dataset to this notebook, just create a new section in the same format as each section below (make sure it comes *before* the `Validate All Datasets` code block). You need to implement a `retrieve_[your_data_name]` function that builds a string representation of the dataset, store its hash using the `hash_data()` function. The `Validate All Datasets` code block at the end is responsible for looping over all the data and checking it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Name** | **Chapters**| **R Dataset** | **Remote Database** |\n",
    "| -------- | ----------- | ----------- | ---------------- |\n",
    "| US 2016 Census/Vote Data | 1, 2, 3 | N | N |\n",
    "| Canadian Movies | 2 | N | Y |\n",
    "| Historical US Vote | 3 | N | N |\n",
    "| mtcars | 3 | Y| N |\n",
    "| Mauna Loa CO2 | 4 | N | N |\n",
    "| Islands | 4 | Y | N |\n",
    "| Old Faithful | 4 | Y | N |\n",
    "| Speed of Light | 4 | Y | N |\n",
    "| Wisconsin Breast Cancer | 6, 7 | N | N |\n",
    "| Sacramento Real Estate | 8, 9 | Y | N | \n",
    "| Marketing Data | 10 | N | N |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses the following Python3 packages to obtain and process data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                #for manipulating arrays\n",
    "import pandas as pd               #for loading/writing/manipulating tabular data\n",
    "import requests, ftplib           #for downloading files\n",
    "import os                         #for handling files\n",
    "import hashlib                    #for validating files\n",
    "import io                         #for creating byte streams for xlsx files\n",
    "import rpy2.robjects as robjects  #for obtaining datasets included in R\n",
    "robjects.r('library(caret)')      #for the Sacramento dataset in R\n",
    "from bs4 import BeautifulSoup     #for scraping html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "\n",
    "# This function takes in a string and outputs its SHA1 hash\n",
    "def hash_data(data):\n",
    "    data_bytes = data.encode()\n",
    "    sha1 = hashlib.sha1()\n",
    "    sha1.update(data_bytes)\n",
    "    return sha1.hexdigest()\n",
    "\n",
    "def download_ftp(url, folder_path, filename):\n",
    "    print('Downloading ' + filename + ' from ' + url)\n",
    "    raw_data = ''\n",
    "    try:\n",
    "        with ftplib.FTP(url) as ftp:\n",
    "            ftp.login()\n",
    "            ftp.cwd(folder_path)\n",
    "            resp = []\n",
    "            ftp.retrlines('RETR '+filename, callback = lambda ln : resp.append(ln))\n",
    "            raw_data = '\\n'.join(resp)\n",
    "    except Exception as e:\n",
    "        print('Exception while downloading ' + filename + ' from ' + url)\n",
    "        print(e)\n",
    "    \n",
    "    return raw_data\n",
    "\n",
    "def download_http(url):\n",
    "    print('Downloading from ' + url)\n",
    "    return requests.get(url).content.decode('utf-8')    \n",
    "\n",
    "def retrieve_r_table(name):\n",
    "    print('Obtaining ' + name + ' from the R kernel')\n",
    "    robjects.r('data('+name+')')\n",
    "    table = robjects.r(name)\n",
    "    colnames = list(table.names)\n",
    "    cols = []\n",
    "    for colname in colnames:\n",
    "        if type(table.rx2(colname)) == robjects.vectors.FactorVector:\n",
    "            levels = list(table.rx2(colname).levels)\n",
    "            cols.append([levels[lv-1] for lv in list(table.rx2(colname))])\n",
    "        else:\n",
    "            cols.append(list(table.rx2(colname)))\n",
    "    data = ','.join(colnames)+'\\n'\n",
    "    for r in range(len(cols[0])):\n",
    "        data += ','.join([str(col[r]) for col in cols]) + '\\n'\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US 2016 Census / Vote Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Source:** [DataUSA](https://datausa.io) and [Federal Election Commission](https://www.fec.gov)\n",
    "- **Data URL:**\n",
    "    - Census data: http://datausa.io/api/data?drilldowns=State&measure=Average%20Commute%20Time,Property%20Value,Median%20Household%20Income,Population&year=2016\n",
    "    - Election data: https://www.fec.gov/documents/1890/federalelections2016.xlsx\n",
    "- **Date Accessed:** July 5, 2020\n",
    "- **License:** \n",
    "\n",
    "```\n",
    "You can copy, download or print content for your own use, and you can also include excerpts from Data USA, databases and multimedia products in your own documents, presentations, blogs, websites and teaching materials, provided that suitable acknowledgment of Data USA as source is given.\n",
    "\n",
    "All requests for commercial use and translation rights should be submitted to usage@datausa.io.\n",
    "```\n",
    "- **Further Instructions:** You must run `Rscript create_state_property_vote_variants.R` in this folder to generate the non-csv versions of the data. These are used in the reading data chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def retrieve_state_property_vote():\n",
    "    print('Downloading 2016 census data from http://datausa.io')\n",
    "    #add a user agent header so that the API doesn't return 403\n",
    "    useragent = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "    #create the query URL\n",
    "    state_data_url = \"http://datausa.io/api/data?drilldowns=State&measure=Average Commute Time,Property Value,Median Household Income,Population&year=2016\"\n",
    "    state_data = requests.get(state_data_url, headers=useragent).json()['data']\n",
    "    #for some reason, DataUSA API does not return Median Household Income for Puerto Rico or DC\n",
    "    #insert it manually based on their website\n",
    "    for state in state_data:\n",
    "        if state['State'] == 'Puerto Rico':\n",
    "            state['Median Household Income'] = 20078\n",
    "        if state['State'] == 'District of Columbia':\n",
    "            state['Median Household Income'] = 75506\n",
    "    #for some reason, DataUSA API\n",
    "    #convert to a list of lists, removing DC, Puerto Rico\n",
    "    #format numerical entries as either a 2-decimal float or integer\n",
    "    colnames = ['State', 'Population', 'Property Value', 'Median Household Income', 'Average Commute Time']\n",
    "    datalines = [[d[colname] if type(d[colname]) == str else format(d[colname], '.2f').rstrip('0').rstrip('.') for colname in colnames] for d in state_data] # if d['State'] != 'Puerto Rico' and d['State'] != 'District of Columbia']\n",
    "    #rename column names to shorten and avoid spaces\n",
    "    colnames = ['state', 'pop', 'med_prop_val', 'med_income', 'avg_commute']\n",
    "    #obtain general presidential election 2016 results data\n",
    "    print('Downloading 2016 vote data from http://www.fec.gov')\n",
    "    elec_data_url = \"https://www.fec.gov/documents/1890/federalelections2016.xlsx\"\n",
    "    stream = io.BytesIO(requests.get(elec_data_url).content)\n",
    "    elec_data = pd.read_excel(stream,sheet_name=8)\n",
    "    #extract one row per state with winner\n",
    "    elec_data = elec_data[elec_data['WINNER INDICATOR'] == 'W']\n",
    "    elec_dict = {}\n",
    "    for i in range(elec_data.shape[0]):        \n",
    "        party = elec_data.iloc[i]['PARTY']\n",
    "        #some rows assign the \"winner\" to combined parties; check the winner name for these rows\n",
    "        #otherwise convert DEM/REP to long form names\n",
    "        if party != 'REP' and party != 'DEM': \n",
    "            if elec_data.iloc[i]['LAST NAME'] == 'Trump':\n",
    "                party = 'Republican'\n",
    "            else:\n",
    "                party = 'Democratic'\n",
    "        elif party == 'REP':\n",
    "            party = 'Republican'\n",
    "        elif party == 'DEM':\n",
    "            party = 'Democratic'\n",
    "        elec_dict[elec_data.iloc[i]['STATE']] = party\n",
    "    #combine the two data\n",
    "    data = [','.join(colnames+['party'])]\n",
    "    for line in datalines:\n",
    "        data.append(','.join(line)+',' + (elec_dict[line[0]] if line[0] in elec_dict.keys() else \"Not Applicable\"))\n",
    "    return '\\n'.join(data)\n",
    "\n",
    "datasets['state_property_vote'] = {}\n",
    "datasets['state_property_vote']['compare_hash'] = 'f703237fb2da402e9d945de1e76783d014b60cfe'\n",
    "datasets['state_property_vote']['retrieve_data'] = retrieve_state_property_vote\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Historical US Vote Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Source:** [Wikipedia](https://wikipedia.org)\n",
    "- **Data URL:** https://en.wikipedia.org/wiki/List_of_United_States_presidential_elections_by_popular_vote_margin\n",
    "- **Date Accessed:** July 6, 2020\n",
    "- **License:** None\n",
    "- **Further Instructions:** Right now, the wrangling chapter uses a few different versions of this data. These are stored in the `data/` folder under the name `historical_vote*.csv` but are not created from the `us_vote` data. These were not made reproducible because the intention is to move away from this US vote data shortly and replace with another dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_us_vote():\n",
    "    #scrape the wiki HTML\n",
    "    us_vote_url = 'https://en.wikipedia.org/wiki/List_of_United_States_presidential_elections_by_popular_vote_margin'\n",
    "    us_vote_html = requests.get(us_vote_url).content\n",
    "    soup = BeautifulSoup(us_vote_html, 'html.parser')\n",
    "    tables = soup.find_all('table')\n",
    "    data = ''\n",
    "    for table in tables:\n",
    "        #search for the actual election results table\n",
    "        if 'wikitable' in table['class']:\n",
    "            #create the column names synthetically, since the actual HTML table is organized with multiple col name levels\n",
    "            colnames = ['election_num', 'election_year', 'winner', 'winner_party', 'elec_coll_votes_count',\n",
    "                        'elec_coll_votes_tot','elec_coll_votes_perc', 'pop_votes_perc', 'pop_votes_perc_marg','pop_votes_count',\n",
    "                        'pop_votes_count_marg','runnerup','runnerup_party','turnout']\n",
    "            data += ','.join(colnames)+'\\n'\n",
    "            #obtain the rows of the table\n",
    "            list_rows = []\n",
    "            table_rows = table.find_all('tr')   \n",
    "            for i in range(3, len(table_rows)):\n",
    "                #convert the entries to just plain text, remove tags\n",
    "                entries = table_rows[i].find_all('td')\n",
    "                entries_txt = [entry.text.strip() for entry in entries]\n",
    "                #names are in the format Lastname,Firstname Lastname. Remove the redundant lastname\n",
    "                entries_txt[2] = entries_txt[2].split(',')[1]\n",
    "                entries_txt[10] = entries_txt[10].split(',')[1]\n",
    "                #if this is the first election, the year is 1788-89. just replace the year with 1788\n",
    "                if '-' in entries_txt[1]:\n",
    "                    entries_txt[1] = entries_txt[1].split('-')[0]\n",
    "                #there is one D.-R. party entry with [Note 1] -- remove that\n",
    "                if entries_txt[11] == 'D.-R.[Note 1]':\n",
    "                    entries_txt[11] = 'D.-R.'\n",
    "                #remove commas and percent symbols\n",
    "                for j in range(len(entries_txt)):\n",
    "                    entries_txt[j] = entries_txt[j].replace(',','').replace('%','')\n",
    "                #split the electoral college vote/total into two entries\n",
    "                elecvotes = entries_txt[4].split('/')\n",
    "                entries_txt[4] = elecvotes[0]\n",
    "                entries_txt.insert(5, elecvotes[1])\n",
    "                #add this row to the list\n",
    "                list_rows.append(entries_txt)\n",
    "            #sort the rows by first element\n",
    "            list_rows = sorted(list_rows, key = lambda x : int(x[0]))\n",
    "            #append to the data\n",
    "            for row in list_rows:\n",
    "                data += ','.join(row)+'\\n'\n",
    "    return data\n",
    "\n",
    "datasets['us_vote'] = {}\n",
    "datasets['us_vote']['compare_hash'] = '1f505f91c81684a48dd631663b3ff8777600bf70'\n",
    "datasets['us_vote']['retrieve_data'] = retrieve_us_vote\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motor Trend Car Road Tests Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Source:** Base R dataset\n",
    "- **Attribution:** Henderson and Velleman (1981), Building multiple regression models interactively. Biometrics, 37, 391–411. From the 1974 Motor Trends magazine. See https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html\n",
    "- **Date Accessed:** July 4, 2020\n",
    "- **License:** None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_mtcars():\n",
    "    return retrieve_r_table('mtcars')\n",
    "\n",
    "datasets['mtcars'] = {}\n",
    "datasets['mtcars']['compare_hash'] = '474fd20770e546b99982923697add9c44409feaf'\n",
    "datasets['mtcars']['retrieve_data'] = retrieve_mtcars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mauna Loa CO2 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Source:** [National Ocean and Atmospheric Administration (NOAA)](https://www.esrl.noaa.gov/gmd/ccgg/trends/data.html)\n",
    "- **Data URL:** ftp://aftp.cmdl.noaa.gov/products/trends/co2/co2_weekly_mlo.txt\n",
    "- **Date Accessed:** July 7, 2020\n",
    "- **Attribution:** Dr. Pieter Tans, [NOAA/GML](www.esrl.noaa.gov/gmd/ccgg/trends/) and Dr. Ralph Keeling, [Scripps Institution of Oceanography](https://scrippsco2.ucsd.edu/).\n",
    "- **License:** The authors have contacted Dr. Tans, who relayed the following information:\n",
    "\n",
    "> You are very welcome to use the CO2 data for your textbook. All data and graphs on our web pages are in the public domain. We only request that you mention us as the source.\n",
    "\n",
    "\n",
    "```\n",
    "# --------------------------------------------------------------------\n",
    "# USE OF NOAA ESRL DATA\n",
    "# \n",
    "# These data are made freely available to the public and the\n",
    "# scientific community in the belief that their wide dissemination\n",
    "# will lead to greater understanding and new scientific insights.\n",
    "# The availability of these data does not constitute publication\n",
    "# of the data.  NOAA relies on the ethics and integrity of the user to\n",
    "# ensure that ESRL receives fair credit for their work.  If the data \n",
    "# are obtained for potential use in a publication or presentation, \n",
    "# ESRL should be informed at the outset of the nature of this work.  \n",
    "# If the ESRL data are essential to the work, or if an important \n",
    "# result or conclusion depends on the ESRL data, co-authorship\n",
    "# may be appropriate.  This should be discussed at an early stage in\n",
    "# the work.  Manuscripts using the ESRL data should be sent to ESRL\n",
    "# for review before they are submitted for publication so we can\n",
    "# ensure that the quality and limitations of the data are accurately\n",
    "# represented.\n",
    "# \n",
    "# Contact:   Pieter Tans (303 497 6678; pieter.tans@noaa.gov)\n",
    "# \n",
    "# File Creation:  Sat Jul  4 05:00:25 2020\n",
    "# \n",
    "# RECIPROCITY\n",
    "# \n",
    "# Use of these data implies an agreement to reciprocate.\n",
    "# Laboratories making similar measurements agree to make their\n",
    "# own data available to the general public and to the scientific\n",
    "# community in an equally complete and easily accessible form.\n",
    "# Modelers are encouraged to make available to the community,\n",
    "# upon request, their own tools used in the interpretation\n",
    "# of the ESRL data, namely well documented model code, transport\n",
    "# fields, and additional information necessary for other\n",
    "# scientists to repeat the work and to run modified versions.\n",
    "# Model availability includes collaborative support for new\n",
    "# users of the models.\n",
    "# --------------------------------------------------------------------\n",
    "#  \n",
    "#  \n",
    "# See www.esrl.noaa.gov/gmd/ccgg/trends/ for additional details.\n",
    "#  \n",
    "# NOTE: DATA FOR THE LAST SEVERAL MONTHS ARE PRELIMINARY, ARE STILL SUBJECT\n",
    "# TO QUALITY CONTROL PROCEDURES.\n",
    "# NOTE: The week \"1 yr ago\" is exactly 365 days ago, and thus does not run from\n",
    "# Sunday through Saturday. 365 also ignores the possibility of a leap year.\n",
    "# The week \"10 yr ago\" is exactly 10*365 days +3 days (for leap years) ago.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_mauna_loa():\n",
    "    data = download_ftp('aftp.cmdl.noaa.gov', 'products/trends/co2/', 'co2_mm_mlo.txt')\n",
    "    # remove the lines beginning with # (these are for meta information)\n",
    "    no_meta_info = [s for s in data.split('\\n') if s[0] != '#']\n",
    "    # replace all whitespace with a single space, strip from beginning and end, keep only first 4 cols\n",
    "    standardized_whitespace = [','.join([num for num in s.strip().split(' ') if len(num)>0][:4]) for s in no_meta_info]\n",
    "    # stitch together into a string with col names at the head\n",
    "    clean_data = 'year,month,date_decimal,ppm\\n'+'\\n'.join(standardized_whitespace)\n",
    "    return clean_data\n",
    "\n",
    "datasets['mauna_loa'] = {}\n",
    "datasets['mauna_loa']['compare_hash'] = '0a4ee4c99c009424d65ebeeef747ffebd4a8557d'\n",
    "datasets['mauna_loa']['retrieve_data'] = retrieve_mauna_loa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Island Landmasses Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Source:** Base R dataset\n",
    "- **Attribution:** The World Almanac and Book of Facts, 1975, page 406. See https://stat.ethz.ch/R-manual/R-patched/library/datasets/html/islands.html \n",
    "- **Date Accessed:** July 4, 2020\n",
    "- **License:** None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_islands():\n",
    "    print('Obtaining islands data from the R kernel')\n",
    "    isl = robjects.r('islands')\n",
    "    names = list(isl.names)\n",
    "    vals = list(isl)\n",
    "    data = 'landmass,size\\n'\n",
    "    data = data + '\\n'.join([d[0]+','+str(int(d[1])) for d in zip(names, vals)])\n",
    "    return data\n",
    "\n",
    "datasets['islands'] = {}\n",
    "datasets['islands']['compare_hash'] = 'c127b83a920db02021d378b4f368b66f90f4dc90'\n",
    "datasets['islands']['retrieve_data'] = retrieve_islands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Faithful Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Source:** Base R dataset\n",
    "- **Attribution:** Azzalini, A. and Bowman, A. W. (1990). A look at some data on the Old Faithful geyser. Applied Statistics, 39, 357–365. doi: 10.2307/2347385. See https://stat.ethz.ch/R-manual/R-patched/library/datasets/html/faithful.html\n",
    "- **Date Accessed:** July 4, 2020\n",
    "- **License:** None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_faithful():\n",
    "    return retrieve_r_table('faithful')\n",
    "\n",
    "datasets['faithful'] = {}\n",
    "datasets['faithful']['compare_hash'] = '6e838cd7f190cbb365bec6bb98099052e63d252b'\n",
    "datasets['faithful']['retrieve_data'] = retrieve_faithful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Michelson Speed of Light Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Source:** Base R dataset\n",
    "- **Attribution:** A. A. Michelson (1882) Experimental determination of the velocity of light made at the United States Naval Academy, Annapolis. Astronomic Papers 1 135–8. U.S. Nautical Almanac Office. (See Table 24.) See https://stat.ethz.ch/R-manual/R-patched/library/datasets/html/morley.html\n",
    "- **Date Accessed:** July 4, 2020\n",
    "- **License:** None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_michelson():\n",
    "    return retrieve_r_table('morley')\n",
    "\n",
    "datasets['michelson'] = {}\n",
    "datasets['michelson']['compare_hash'] = 'bb0492aa16d01bb5d6f7a0839d266a607b2e9a6e'\n",
    "datasets['michelson']['retrieve_data'] = retrieve_michelson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wisconsin Breast Cancer Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Source:** [The UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic))\n",
    "- **Data URL:** https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\n",
    "- **Date Accessed:** July 4, 2020\n",
    "- **Attribution:** Dr. William H. Wolberg, W. Nick Street, and Olvi L. Mangasarian, University of Wisconsin.\n",
    "- **License:** None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_unscaled_wdbc():\n",
    "    data = download_http('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data')\n",
    "    #create list of variable names\n",
    "    names = ['ID', 'Class', 'Radius', 'Texture', 'Perimeter', 'Area', 'Smoothness', 'Compactness', 'Concavity', 'Concave_Points', 'Symmetry', 'Fractal_Dimension']\n",
    "    #remove all but the class label (B/M) and first 10 entries (means of each value)\n",
    "    data_lines = [line.split(',')[:12] for line in data.split('\\n')]\n",
    "    clean_data = ','.join(names) + '\\n' + '\\n'.join([','.join(line) for line in data_lines])\n",
    "    return clean_data\n",
    "\n",
    "datasets['unscaled_wdbc'] = {}\n",
    "datasets['unscaled_wdbc']['compare_hash'] = '57dd9bb52f8c5e9f1aa831de7ce199e7a97dffda'\n",
    "datasets['unscaled_wdbc']['retrieve_data'] = retrieve_unscaled_wdbc\n",
    "\n",
    "def retrieve_wdbc():\n",
    "    data = retrieve_unscaled_wdbc()\n",
    "    tbl = pd.read_csv(io.StringIO(data))\n",
    "    sel = (tbl.columns != 'ID') & (tbl.columns != 'Class')\n",
    "    tbl.loc[:,sel] = (tbl.loc[:,sel] - tbl.loc[:,sel].mean())/tbl.loc[:,sel].std()\n",
    "    out = io.StringIO()\n",
    "    tbl.to_csv(out, index=False)\n",
    "    return out.getvalue()\n",
    "    \n",
    "datasets['wdbc'] = {}\n",
    "datasets['wdbc']['compare_hash'] = '5529b157e36fec8d5f688ed8c87c0f2c13bb7dcf'\n",
    "datasets['wdbc']['retrieve_data'] = retrieve_wdbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sacramento Real Estate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Source:** The [caret package](http://topepo.github.io/caret/index.html)\n",
    "- **Attribution:** The Sacramento Bee. See http://topepo.github.io/caret/data-sets.html\n",
    "- **Date Accessed:** July 5, 2020\n",
    "- **License:** None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_sacramento():\n",
    "    return retrieve_r_table('Sacramento')\n",
    "\n",
    "datasets['sacramento'] = {}\n",
    "datasets['sacramento']['compare_hash'] = '04bcb4bb63d13570591e937f93411b3d84802b58'\n",
    "datasets['sacramento']['retrieve_data'] = retrieve_sacramento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marketing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Source:** Synthetic, created by the authors. Inspired by https://www.segmentationstudyguide.com/using-cluster-analysis-for-market-segmentation/ .\n",
    "- **Date Accessed:** July 4, 2020\n",
    "- **License:** None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "marketing_data = '''loyalty,csat,cluster\n",
    "7,1,1\n",
    "7.5,1,1\n",
    "8,2,1\n",
    "7,2,1\n",
    "8,3,1\n",
    "1.5,1.75,3\n",
    "1,3,3\n",
    "0.5,4,3\n",
    "2,4,3\n",
    "7,6,2\n",
    "6,6,2\n",
    "7,7,2\n",
    "6,7,2\n",
    "5,7,2\n",
    "9.5,8,2\n",
    "7,8,2\n",
    "8.3,9,2\n",
    "4,8,2\n",
    "2,3,3\n",
    "'''\n",
    "\n",
    "def retrieve_marketing():\n",
    "    print('Creating marketing data')\n",
    "    return marketing_data\n",
    "\n",
    "datasets['marketing'] = {}\n",
    "datasets['marketing']['compare_hash'] = '9b49d36e53fd474bb7ca04b879a5219f41589c37'\n",
    "datasets['marketing']['retrieve_data'] = retrieve_marketing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating all datasets\n",
      "\n",
      "Processing marketing\n",
      "Obtaining data from source...\n",
      "Creating marketing data\n",
      "Computing hash...\n",
      "Hash mismatch for fresh copy of marketing; source changed.\n",
      "Original Hash: 9b49d36e53fd474bb7ca04b879a5219f41589c37\n",
      "New Hash: 7fee2c0c0689406eb077d1ffb7492d56fb56220f\n",
      "Saving new cleaned data in marketing_new.csv\n",
      "If you want to use the new data, remove the old data and rename that file to marketing.csv\n",
      "Then update the hash in this notebook for marketing to 7fee2c0c0689406eb077d1ffb7492d56fb56220f\n",
      "Obtaining data from storage...\n",
      "Computing hash...\n",
      "Hash matches for storage copy of marketing\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Validating all datasets\\n')\n",
    "for dnm, ddict in datasets.items():\n",
    "    #Obtain data from source and verify the hash matches\n",
    "    print('Processing '+ dnm)\n",
    "    print('Obtaining data from source...')\n",
    "    new_data = ddict['retrieve_data']()    \n",
    "    print('Computing hash...')\n",
    "    dhash = hash_data(new_data)\n",
    "    if dhash == ddict['compare_hash']:\n",
    "        print('Hash matches for fresh copy of ' + dnm + ' obtained from source')\n",
    "    else:\n",
    "        print('Hash mismatch for fresh copy of ' + dnm +'; source changed.')\n",
    "        print('Original Hash: ' + ddict['compare_hash'])\n",
    "        print('New Hash: ' + dhash)\n",
    "        print('Saving new cleaned data in ' + dnm+'_new.csv')\n",
    "        print('If you want to use the new data, remove the old data and rename that file to ' + dnm+'.csv')\n",
    "        print('Then update the hash in this notebook for ' + dnm + ' to ' + dhash)\n",
    "        f = open(dnm+'_new.csv', 'w')\n",
    "        f.write(new_data)\n",
    "        f.close()\n",
    "    #Obtain data from storage and verify the hash matches\n",
    "    print('Obtaining data from storage...')\n",
    "    try:\n",
    "        f = open(dnm+'.csv', 'r')\n",
    "        old_data = f.read()\n",
    "        f.close()  \n",
    "        print('Computing hash...')\n",
    "        dhash = hash_data(old_data)\n",
    "        if dhash == ddict['compare_hash']:\n",
    "            print('Hash matches for storage copy of ' + dnm)\n",
    "        else:\n",
    "            print('Hash mismatch for stored copy of ' + dnm)\n",
    "            print('Either update the hash in this notebook to the stored file hash: ' + dhash)\n",
    "            print('Or investigate why the data appears to have changed.')\n",
    "            print('Saving new data to ' + dnm+'_new.csv for comparison')\n",
    "            f = open(dnm+'_new.csv', 'w')\n",
    "            f.write(new_data)\n",
    "            f.close()\n",
    "    except:\n",
    "        print('Cannot load ' + dnm + ' from storage. Check that the file exists.')   \n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
