<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Regression I: K-nearest neighbours | Introduction to Data Science</title>
  <meta name="description" content="This is an open source textbook for teaching introductory data science." />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Regression I: K-nearest neighbours | Introduction to Data Science" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is an open source textbook for teaching introductory data science." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Regression I: K-nearest neighbours | Introduction to Data Science" />
  
  <meta name="twitter:description" content="This is an open source textbook for teaching introductory data science." />
  

<meta name="author" content="Tiffany-Anne Timbers" />
<meta name="author" content="Trevor Campbell" />
<meta name="author" content="Melissa Lee" />


<meta name="date" content="2020-12-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="classification-continued.html"/>
<link rel="next" href="regression2.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.9.2.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.52.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.52.2/plotly-latest.min.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Data Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> R, Jupyter, and the tidyverse</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#chapter-learning-objectives"><i class="fa fa-check"></i><b>1.1</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#jupyter-notebooks"><i class="fa fa-check"></i><b>1.2</b> Jupyter notebooks</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#loading-a-spreadsheet-like-dataset"><i class="fa fa-check"></i><b>1.3</b> Loading a spreadsheet-like dataset</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#assigning-value-to-a-data-frame"><i class="fa fa-check"></i><b>1.4</b> Assigning value to a data frame</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#creating-subsets-of-data-frames-with-select-filter"><i class="fa fa-check"></i><b>1.5</b> Creating subsets of data frames with <code>select</code> &amp; <code>filter</code></a><ul>
<li class="chapter" data-level="1.5.1" data-path="index.html"><a href="index.html#using-select-to-extract-multiple-columns"><i class="fa fa-check"></i><b>1.5.1</b> Using <code>select</code> to extract multiple columns</a></li>
<li class="chapter" data-level="1.5.2" data-path="index.html"><a href="index.html#using-select-to-extract-a-range-of-columns"><i class="fa fa-check"></i><b>1.5.2</b> Using <code>select</code> to extract a range of columns</a></li>
<li class="chapter" data-level="1.5.3" data-path="index.html"><a href="index.html#using-filter-to-extract-a-single-row"><i class="fa fa-check"></i><b>1.5.3</b> Using <code>filter</code> to extract a single row</a></li>
<li class="chapter" data-level="1.5.4" data-path="index.html"><a href="index.html#using-filter-to-extract-rows-with-values-above-a-threshold"><i class="fa fa-check"></i><b>1.5.4</b> Using <code>filter</code> to extract rows with values above a threshold</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#exploring-data-with-visualizations"><i class="fa fa-check"></i><b>1.6</b> Exploring data with visualizations</a><ul>
<li class="chapter" data-level="1.6.1" data-path="index.html"><a href="index.html#using-ggplot-to-create-a-scatter-plot"><i class="fa fa-check"></i><b>1.6.1</b> Using <code>ggplot</code> to create a scatter plot</a></li>
<li class="chapter" data-level="1.6.2" data-path="index.html"><a href="index.html#formatting-ggplot-objects"><i class="fa fa-check"></i><b>1.6.2</b> Formatting ggplot objects</a></li>
<li class="chapter" data-level="1.6.3" data-path="index.html"><a href="index.html#changing-the-units"><i class="fa fa-check"></i><b>1.6.3</b> Changing the units</a></li>
<li class="chapter" data-level="1.6.4" data-path="index.html"><a href="index.html#coloring-points-by-group"><i class="fa fa-check"></i><b>1.6.4</b> Coloring points by group</a></li>
<li class="chapter" data-level="1.6.5" data-path="index.html"><a href="index.html#putting-it-all-together"><i class="fa fa-check"></i><b>1.6.5</b> Putting it all together</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="reading.html"><a href="reading.html"><i class="fa fa-check"></i><b>2</b> Reading in data locally and from the web</a><ul>
<li class="chapter" data-level="2.1" data-path="reading.html"><a href="reading.html#overview"><i class="fa fa-check"></i><b>2.1</b> Overview</a></li>
<li class="chapter" data-level="2.2" data-path="reading.html"><a href="reading.html#chapter-learning-objectives-1"><i class="fa fa-check"></i><b>2.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="2.3" data-path="reading.html"><a href="reading.html#absolute-and-relative-file-paths"><i class="fa fa-check"></i><b>2.3</b> Absolute and relative file paths</a></li>
<li class="chapter" data-level="2.4" data-path="reading.html"><a href="reading.html#reading-tabular-data-from-a-plain-text-file-into-r"><i class="fa fa-check"></i><b>2.4</b> Reading tabular data from a plain text file into R</a><ul>
<li class="chapter" data-level="2.4.1" data-path="reading.html"><a href="reading.html#skipping-rows-when-reading-in-data"><i class="fa fa-check"></i><b>2.4.1</b> Skipping rows when reading in data</a></li>
<li class="chapter" data-level="2.4.2" data-path="reading.html"><a href="reading.html#read_delim-as-a-more-flexible-method-to-get-tabular-data-into-r"><i class="fa fa-check"></i><b>2.4.2</b> <code>read_delim</code> as a more flexible method to get tabular data into R</a></li>
<li class="chapter" data-level="2.4.3" data-path="reading.html"><a href="reading.html#reading-tabular-data-directly-from-a-url"><i class="fa fa-check"></i><b>2.4.3</b> Reading tabular data directly from a URL</a></li>
<li class="chapter" data-level="2.4.4" data-path="reading.html"><a href="reading.html#previewing-a-data-file-before-reading-it-into-r"><i class="fa fa-check"></i><b>2.4.4</b> Previewing a data file before reading it into R</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="reading.html"><a href="reading.html#reading-data-from-an-microsoft-excel-file"><i class="fa fa-check"></i><b>2.5</b> Reading data from an Microsoft Excel file</a></li>
<li class="chapter" data-level="2.6" data-path="reading.html"><a href="reading.html#reading-data-from-a-database"><i class="fa fa-check"></i><b>2.6</b> Reading data from a database</a><ul>
<li class="chapter" data-level="2.6.1" data-path="reading.html"><a href="reading.html#reading-data-from-a-sqlite-database"><i class="fa fa-check"></i><b>2.6.1</b> Reading data from a SQLite database</a></li>
<li class="chapter" data-level="2.6.2" data-path="reading.html"><a href="reading.html#reading-data-from-a-postgresql-database"><i class="fa fa-check"></i><b>2.6.2</b> Reading data from a PostgreSQL database</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="reading.html"><a href="reading.html#writing-data-from-r-to-a-.csv-file"><i class="fa fa-check"></i><b>2.7</b> Writing data from R to a <code>.csv</code> file</a></li>
<li class="chapter" data-level="2.8" data-path="reading.html"><a href="reading.html#scraping-data-off-the-web-using-r"><i class="fa fa-check"></i><b>2.8</b> Scraping data off the web using R</a><ul>
<li class="chapter" data-level="2.8.1" data-path="reading.html"><a href="reading.html#html-and-css-selectors"><i class="fa fa-check"></i><b>2.8.1</b> HTML and CSS selectors</a></li>
<li class="chapter" data-level="2.8.2" data-path="reading.html"><a href="reading.html#are-you-allowed-to-scrape-that-website"><i class="fa fa-check"></i><b>2.8.2</b> Are you allowed to scrape that website?</a></li>
<li class="chapter" data-level="2.8.3" data-path="reading.html"><a href="reading.html#using-rvest"><i class="fa fa-check"></i><b>2.8.3</b> Using <code>rvest</code></a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="reading.html"><a href="reading.html#additional-readingsresources"><i class="fa fa-check"></i><b>2.9</b> Additional readings/resources</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="wrangling.html"><a href="wrangling.html"><i class="fa fa-check"></i><b>3</b> Cleaning and wrangling data</a><ul>
<li class="chapter" data-level="3.1" data-path="wrangling.html"><a href="wrangling.html#overview-1"><i class="fa fa-check"></i><b>3.1</b> Overview</a></li>
<li class="chapter" data-level="3.2" data-path="wrangling.html"><a href="wrangling.html#chapter-learning-objectives-2"><i class="fa fa-check"></i><b>3.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="3.3" data-path="wrangling.html"><a href="wrangling.html#vectors-and-data-frames"><i class="fa fa-check"></i><b>3.3</b> Vectors and Data frames</a><ul>
<li class="chapter" data-level="3.3.1" data-path="wrangling.html"><a href="wrangling.html#what-is-a-data-frame"><i class="fa fa-check"></i><b>3.3.1</b> What is a data frame?</a></li>
<li class="chapter" data-level="3.3.2" data-path="wrangling.html"><a href="wrangling.html#what-is-a-vector"><i class="fa fa-check"></i><b>3.3.2</b> What is a vector?</a></li>
<li class="chapter" data-level="3.3.3" data-path="wrangling.html"><a href="wrangling.html#how-are-vectors-different-from-a-list"><i class="fa fa-check"></i><b>3.3.3</b> How are vectors different from a list?</a></li>
<li class="chapter" data-level="3.3.4" data-path="wrangling.html"><a href="wrangling.html#what-does-this-have-to-do-with-data-frames"><i class="fa fa-check"></i><b>3.3.4</b> What does this have to do with data frames?</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="wrangling.html"><a href="wrangling.html#tidy-data"><i class="fa fa-check"></i><b>3.4</b> Tidy Data</a><ul>
<li class="chapter" data-level="3.4.1" data-path="wrangling.html"><a href="wrangling.html#what-is-tidy-data"><i class="fa fa-check"></i><b>3.4.1</b> What is tidy data?</a></li>
<li class="chapter" data-level="3.4.2" data-path="wrangling.html"><a href="wrangling.html#why-is-tidy-data-important-in-r"><i class="fa fa-check"></i><b>3.4.2</b> Why is tidy data important in R?</a></li>
<li class="chapter" data-level="3.4.3" data-path="wrangling.html"><a href="wrangling.html#going-from-wide-to-long-or-tidy-using-gather"><i class="fa fa-check"></i><b>3.4.3</b> Going from wide to long (or tidy!) using <code>gather</code></a></li>
<li class="chapter" data-level="3.4.4" data-path="wrangling.html"><a href="wrangling.html#using-separate-to-deal-with-multiple-delimiters"><i class="fa fa-check"></i><b>3.4.4</b> Using separate to deal with multiple delimiters</a></li>
<li class="chapter" data-level="3.4.5" data-path="wrangling.html"><a href="wrangling.html#notes-on-defining-tidy-data"><i class="fa fa-check"></i><b>3.4.5</b> Notes on defining tidy data</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="wrangling.html"><a href="wrangling.html#combining-functions-using-the-pipe-operator"><i class="fa fa-check"></i><b>3.5</b> Combining functions using the pipe operator, <code>%&gt;%</code>:</a><ul>
<li class="chapter" data-level="3.5.1" data-path="wrangling.html"><a href="wrangling.html#using-to-combine-filter-and-select"><i class="fa fa-check"></i><b>3.5.1</b> Using <code>%&gt;%</code> to combine <code>filter</code> and <code>select</code></a></li>
<li class="chapter" data-level="3.5.2" data-path="wrangling.html"><a href="wrangling.html#using-with-more-than-two-functions"><i class="fa fa-check"></i><b>3.5.2</b> Using <code>%&gt;%</code> with more than two functions</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="wrangling.html"><a href="wrangling.html#iterating-over-data-with-group_by-summarize"><i class="fa fa-check"></i><b>3.6</b> Iterating over data with <code>group_by</code> + <code>summarize</code></a><ul>
<li class="chapter" data-level="3.6.1" data-path="wrangling.html"><a href="wrangling.html#calculating-summary-statistics"><i class="fa fa-check"></i><b>3.6.1</b> Calculating summary statistics:</a></li>
<li class="chapter" data-level="3.6.2" data-path="wrangling.html"><a href="wrangling.html#calculating-group-summary-statistics"><i class="fa fa-check"></i><b>3.6.2</b> Calculating group summary statistics:</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="wrangling.html"><a href="wrangling.html#additional-reading-on-the-dplyr-functions"><i class="fa fa-check"></i><b>3.7</b> Additional reading on the <code>dplyr</code> functions</a></li>
<li class="chapter" data-level="3.8" data-path="wrangling.html"><a href="wrangling.html#using-purrrs-map-functions-to-iterate"><i class="fa fa-check"></i><b>3.8</b> Using <code>purrr</code>’s <code>map*</code> functions to iterate</a><ul>
<li class="chapter" data-level="3.8.1" data-path="wrangling.html"><a href="wrangling.html#a-bit-more-about-the-map_-functions"><i class="fa fa-check"></i><b>3.8.1</b> A bit more about the <code>map_*</code> functions</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="wrangling.html"><a href="wrangling.html#additional-readingsresources-1"><i class="fa fa-check"></i><b>3.9</b> Additional readings/resources</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="viz.html"><a href="viz.html"><i class="fa fa-check"></i><b>4</b> Effective data visualization</a><ul>
<li class="chapter" data-level="4.1" data-path="viz.html"><a href="viz.html#overview-2"><i class="fa fa-check"></i><b>4.1</b> Overview</a></li>
<li class="chapter" data-level="4.2" data-path="viz.html"><a href="viz.html#chapter-learning-objectives-3"><i class="fa fa-check"></i><b>4.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="4.3" data-path="viz.html"><a href="viz.html#choosing-the-visualization"><i class="fa fa-check"></i><b>4.3</b> Choosing the visualization</a></li>
<li class="chapter" data-level="4.4" data-path="viz.html"><a href="viz.html#refining-the-visualization"><i class="fa fa-check"></i><b>4.4</b> Refining the visualization</a></li>
<li class="chapter" data-level="4.5" data-path="viz.html"><a href="viz.html#creating-visualizations-with-ggplot2"><i class="fa fa-check"></i><b>4.5</b> Creating visualizations with <code>ggplot2</code></a><ul>
<li class="chapter" data-level="4.5.1" data-path="viz.html"><a href="viz.html#the-mauna-loa-co2-data-set"><i class="fa fa-check"></i><b>4.5.1</b> The Mauna Loa CO2 data set</a></li>
<li class="chapter" data-level="4.5.2" data-path="viz.html"><a href="viz.html#the-island-landmass-data-set"><i class="fa fa-check"></i><b>4.5.2</b> The island landmass data set</a></li>
<li class="chapter" data-level="4.5.3" data-path="viz.html"><a href="viz.html#the-old-faithful-eruption-waiting-time-data-set"><i class="fa fa-check"></i><b>4.5.3</b> The Old Faithful eruption / waiting time data set</a></li>
<li class="chapter" data-level="4.5.4" data-path="viz.html"><a href="viz.html#the-michelson-speed-of-light-data-set"><i class="fa fa-check"></i><b>4.5.4</b> The Michelson speed of light data set</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="viz.html"><a href="viz.html#explaining-the-visualization"><i class="fa fa-check"></i><b>4.6</b> Explaining the visualization</a></li>
<li class="chapter" data-level="4.7" data-path="viz.html"><a href="viz.html#saving-the-visualization"><i class="fa fa-check"></i><b>4.7</b> Saving the visualization</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="GitHub.html"><a href="GitHub.html"><i class="fa fa-check"></i><b>5</b> Version control with GitHub</a><ul>
<li class="chapter" data-level="5.1" data-path="GitHub.html"><a href="GitHub.html#overview-3"><i class="fa fa-check"></i><b>5.1</b> Overview</a></li>
<li class="chapter" data-level="5.2" data-path="GitHub.html"><a href="GitHub.html#videos-to-learn-about-version-control-with-github-and-git"><i class="fa fa-check"></i><b>5.2</b> Videos to learn about version control with GitHub and Git</a><ul>
<li class="chapter" data-level="5.2.1" data-path="GitHub.html"><a href="GitHub.html#creating-a-github-repository"><i class="fa fa-check"></i><b>5.2.1</b> Creating a GitHub repository</a></li>
<li class="chapter" data-level="5.2.2" data-path="GitHub.html"><a href="GitHub.html#exploring-a-github-repository"><i class="fa fa-check"></i><b>5.2.2</b> Exploring a GitHub repository</a></li>
<li class="chapter" data-level="5.2.3" data-path="GitHub.html"><a href="GitHub.html#directly-editing-files-on-github"><i class="fa fa-check"></i><b>5.2.3</b> Directly editing files on GitHub</a></li>
<li class="chapter" data-level="5.2.4" data-path="GitHub.html"><a href="GitHub.html#logging-changes-and-pushing-them-to-github"><i class="fa fa-check"></i><b>5.2.4</b> Logging changes and pushing them to GitHub</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="GitHub.html"><a href="GitHub.html#git-command-cheatsheet"><i class="fa fa-check"></i><b>5.3</b> Git command cheatsheet</a><ul>
<li class="chapter" data-level="5.3.1" data-path="GitHub.html"><a href="GitHub.html#getting-a-repository-from-github-onto-the-server-for-the-first-time"><i class="fa fa-check"></i><b>5.3.1</b> Getting a repository from GitHub onto the server for the first time</a></li>
<li class="chapter" data-level="5.3.2" data-path="GitHub.html"><a href="GitHub.html#logging-changes"><i class="fa fa-check"></i><b>5.3.2</b> Logging changes</a></li>
<li class="chapter" data-level="5.3.3" data-path="GitHub.html"><a href="GitHub.html#sending-your-changes-back-to-github"><i class="fa fa-check"></i><b>5.3.3</b> Sending your changes back to GitHub</a></li>
<li class="chapter" data-level="5.3.4" data-path="GitHub.html"><a href="GitHub.html#getting-changes"><i class="fa fa-check"></i><b>5.3.4</b> Getting changes</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="GitHub.html"><a href="GitHub.html#terminal-cheatsheet"><i class="fa fa-check"></i><b>5.4</b> Terminal cheatsheet</a><ul>
<li class="chapter" data-level="5.4.1" data-path="GitHub.html"><a href="GitHub.html#see-where-you-are"><i class="fa fa-check"></i><b>5.4.1</b> See where you are:</a></li>
<li class="chapter" data-level="5.4.2" data-path="GitHub.html"><a href="GitHub.html#see-what-is-inside-the-directory-where-you-are"><i class="fa fa-check"></i><b>5.4.2</b> See what is inside the directory where you are:</a></li>
<li class="chapter" data-level="5.4.3" data-path="GitHub.html"><a href="GitHub.html#move-to-a-different-directory"><i class="fa fa-check"></i><b>5.4.3</b> Move to a different directory</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>6</b> Classification I: training &amp; predicting</a><ul>
<li class="chapter" data-level="6.1" data-path="classification.html"><a href="classification.html#overview-4"><i class="fa fa-check"></i><b>6.1</b> Overview</a></li>
<li class="chapter" data-level="6.2" data-path="classification.html"><a href="classification.html#chapter-learning-objectives-4"><i class="fa fa-check"></i><b>6.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="6.3" data-path="classification.html"><a href="classification.html#the-classification-problem"><i class="fa fa-check"></i><b>6.3</b> The classification problem</a></li>
<li class="chapter" data-level="6.4" data-path="classification.html"><a href="classification.html#exploring-a-labelled-data-set"><i class="fa fa-check"></i><b>6.4</b> Exploring a labelled data set</a></li>
<li class="chapter" data-level="6.5" data-path="classification.html"><a href="classification.html#classification-with-k-nearest-neighbours"><i class="fa fa-check"></i><b>6.5</b> Classification with K-nearest neighbours</a></li>
<li class="chapter" data-level="6.6" data-path="classification.html"><a href="classification.html#k-nearest-neighbours-with-tidymodels"><i class="fa fa-check"></i><b>6.6</b> K-nearest neighbours with <code>tidymodels</code></a></li>
<li class="chapter" data-level="6.7" data-path="classification.html"><a href="classification.html#data-preprocessing-with-tidymodels"><i class="fa fa-check"></i><b>6.7</b> Data preprocessing with <code>tidymodels</code></a><ul>
<li class="chapter" data-level="6.7.1" data-path="classification.html"><a href="classification.html#centering-and-scaling"><i class="fa fa-check"></i><b>6.7.1</b> Centering and scaling</a></li>
<li class="chapter" data-level="6.7.2" data-path="classification.html"><a href="classification.html#balancing"><i class="fa fa-check"></i><b>6.7.2</b> Balancing</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="classification.html"><a href="classification.html#putting-it-together-in-a-workflow"><i class="fa fa-check"></i><b>6.8</b> Putting it together in a <code>workflow</code></a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="classification-continued.html"><a href="classification-continued.html"><i class="fa fa-check"></i><b>7</b> Classification II: evaluation &amp; tuning</a><ul>
<li class="chapter" data-level="7.1" data-path="classification-continued.html"><a href="classification-continued.html#overview-5"><i class="fa fa-check"></i><b>7.1</b> Overview</a></li>
<li class="chapter" data-level="7.2" data-path="classification-continued.html"><a href="classification-continued.html#chapter-learning-objectives-5"><i class="fa fa-check"></i><b>7.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="7.3" data-path="classification-continued.html"><a href="classification-continued.html#evaluating-accuracy"><i class="fa fa-check"></i><b>7.3</b> Evaluating accuracy</a></li>
<li class="chapter" data-level="7.4" data-path="classification-continued.html"><a href="classification-continued.html#tuning-the-classifier"><i class="fa fa-check"></i><b>7.4</b> Tuning the classifier</a><ul>
<li class="chapter" data-level="7.4.1" data-path="classification-continued.html"><a href="classification-continued.html#cross-validation"><i class="fa fa-check"></i><b>7.4.1</b> Cross-validation</a></li>
<li class="chapter" data-level="7.4.2" data-path="classification-continued.html"><a href="classification-continued.html#parameter-value-selection"><i class="fa fa-check"></i><b>7.4.2</b> Parameter value selection</a></li>
<li class="chapter" data-level="7.4.3" data-path="classification-continued.html"><a href="classification-continued.html#underoverfitting"><i class="fa fa-check"></i><b>7.4.3</b> Under/overfitting</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="classification-continued.html"><a href="classification-continued.html#splitting-data"><i class="fa fa-check"></i><b>7.5</b> Splitting data</a></li>
<li class="chapter" data-level="7.6" data-path="classification-continued.html"><a href="classification-continued.html#summary"><i class="fa fa-check"></i><b>7.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="regression1.html"><a href="regression1.html"><i class="fa fa-check"></i><b>8</b> Regression I: K-nearest neighbours</a><ul>
<li class="chapter" data-level="8.1" data-path="regression1.html"><a href="regression1.html#overview-6"><i class="fa fa-check"></i><b>8.1</b> Overview</a></li>
<li class="chapter" data-level="8.2" data-path="regression1.html"><a href="regression1.html#chapter-learning-objectives-6"><i class="fa fa-check"></i><b>8.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="8.3" data-path="regression1.html"><a href="regression1.html#regression"><i class="fa fa-check"></i><b>8.3</b> Regression</a></li>
<li class="chapter" data-level="8.4" data-path="regression1.html"><a href="regression1.html#sacremento-real-estate-example"><i class="fa fa-check"></i><b>8.4</b> Sacremento real estate example</a></li>
<li class="chapter" data-level="8.5" data-path="regression1.html"><a href="regression1.html#k-nearest-neighbours-regression"><i class="fa fa-check"></i><b>8.5</b> K-nearest neighbours regression</a></li>
<li class="chapter" data-level="8.6" data-path="regression1.html"><a href="regression1.html#training-evaluating-and-tuning-the-model"><i class="fa fa-check"></i><b>8.6</b> Training, evaluating, and tuning the model</a></li>
<li class="chapter" data-level="8.7" data-path="regression1.html"><a href="regression1.html#underfitting-and-overfitting"><i class="fa fa-check"></i><b>8.7</b> Underfitting and overfitting</a></li>
<li class="chapter" data-level="8.8" data-path="regression1.html"><a href="regression1.html#evaluating-on-the-test-set"><i class="fa fa-check"></i><b>8.8</b> Evaluating on the test set</a></li>
<li class="chapter" data-level="8.9" data-path="regression1.html"><a href="regression1.html#strengths-and-limitations-of-k-nn-regression"><i class="fa fa-check"></i><b>8.9</b> Strengths and limitations of K-NN regression</a></li>
<li class="chapter" data-level="8.10" data-path="regression1.html"><a href="regression1.html#multivariate-k-nn-regression"><i class="fa fa-check"></i><b>8.10</b> Multivariate K-NN regression</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="regression2.html"><a href="regression2.html"><i class="fa fa-check"></i><b>9</b> Regression II: linear regression</a><ul>
<li class="chapter" data-level="9.1" data-path="regression2.html"><a href="regression2.html#overview-7"><i class="fa fa-check"></i><b>9.1</b> Overview</a></li>
<li class="chapter" data-level="9.2" data-path="regression2.html"><a href="regression2.html#chapter-learning-objectives-7"><i class="fa fa-check"></i><b>9.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="9.3" data-path="regression2.html"><a href="regression2.html#simple-linear-regression"><i class="fa fa-check"></i><b>9.3</b> Simple linear regression</a></li>
<li class="chapter" data-level="9.4" data-path="regression2.html"><a href="regression2.html#linear-regression-in-r"><i class="fa fa-check"></i><b>9.4</b> Linear regression in R</a></li>
<li class="chapter" data-level="9.5" data-path="regression2.html"><a href="regression2.html#comparing-simple-linear-and-k-nn-regression"><i class="fa fa-check"></i><b>9.5</b> Comparing simple linear and K-NN regression</a></li>
<li class="chapter" data-level="9.6" data-path="regression2.html"><a href="regression2.html#multivariate-linear-regression"><i class="fa fa-check"></i><b>9.6</b> Multivariate linear regression</a></li>
<li class="chapter" data-level="9.7" data-path="regression2.html"><a href="regression2.html#the-other-side-of-regression"><i class="fa fa-check"></i><b>9.7</b> The other side of regression</a></li>
<li class="chapter" data-level="9.8" data-path="regression2.html"><a href="regression2.html#additional-readingsresources-2"><i class="fa fa-check"></i><b>9.8</b> Additional readings/resources</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>10</b> Clustering</a><ul>
<li class="chapter" data-level="10.1" data-path="clustering.html"><a href="clustering.html#overview-8"><i class="fa fa-check"></i><b>10.1</b> Overview</a></li>
<li class="chapter" data-level="10.2" data-path="clustering.html"><a href="clustering.html#chapter-learning-objectives-8"><i class="fa fa-check"></i><b>10.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="10.3" data-path="clustering.html"><a href="clustering.html#clustering-1"><i class="fa fa-check"></i><b>10.3</b> Clustering</a></li>
<li class="chapter" data-level="10.4" data-path="clustering.html"><a href="clustering.html#k-means"><i class="fa fa-check"></i><b>10.4</b> K-means</a><ul>
<li class="chapter" data-level="10.4.1" data-path="clustering.html"><a href="clustering.html#measuring-cluster-quality"><i class="fa fa-check"></i><b>10.4.1</b> Measuring cluster quality</a></li>
<li class="chapter" data-level="10.4.2" data-path="clustering.html"><a href="clustering.html#the-clustering-algorithm"><i class="fa fa-check"></i><b>10.4.2</b> The clustering algorithm</a></li>
<li class="chapter" data-level="10.4.3" data-path="clustering.html"><a href="clustering.html#random-restarts"><i class="fa fa-check"></i><b>10.4.3</b> Random restarts</a></li>
<li class="chapter" data-level="10.4.4" data-path="clustering.html"><a href="clustering.html#choosing-k"><i class="fa fa-check"></i><b>10.4.4</b> Choosing K</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="clustering.html"><a href="clustering.html#k-means-in-r"><i class="fa fa-check"></i><b>10.5</b> K-means in R</a></li>
<li class="chapter" data-level="10.6" data-path="clustering.html"><a href="clustering.html#additional-readings"><i class="fa fa-check"></i><b>10.6</b> Additional readings:</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>11</b> Introduction to Statistical Inference</a><ul>
<li class="chapter" data-level="11.1" data-path="inference.html"><a href="inference.html#overview-9"><i class="fa fa-check"></i><b>11.1</b> Overview</a></li>
<li class="chapter" data-level="11.2" data-path="inference.html"><a href="inference.html#chapter-learning-objectives-9"><i class="fa fa-check"></i><b>11.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="11.3" data-path="inference.html"><a href="inference.html#why-do-we-need-sampling"><i class="fa fa-check"></i><b>11.3</b> Why do we need sampling?</a></li>
<li class="chapter" data-level="11.4" data-path="inference.html"><a href="inference.html#sampling-distributions"><i class="fa fa-check"></i><b>11.4</b> Sampling distributions</a><ul>
<li class="chapter" data-level="11.4.1" data-path="inference.html"><a href="inference.html#sampling-distributions-for-proportions"><i class="fa fa-check"></i><b>11.4.1</b> Sampling distributions for proportions</a></li>
<li class="chapter" data-level="11.4.2" data-path="inference.html"><a href="inference.html#sampling-distributions-for-means"><i class="fa fa-check"></i><b>11.4.2</b> Sampling distributions for means</a></li>
<li class="chapter" data-level="11.4.3" data-path="inference.html"><a href="inference.html#summary-1"><i class="fa fa-check"></i><b>11.4.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="inference.html"><a href="inference.html#bootstrapping"><i class="fa fa-check"></i><b>11.5</b> Bootstrapping</a><ul>
<li class="chapter" data-level="11.5.1" data-path="inference.html"><a href="inference.html#overview-10"><i class="fa fa-check"></i><b>11.5.1</b> Overview</a></li>
<li class="chapter" data-level="11.5.2" data-path="inference.html"><a href="inference.html#bootstrapping-in-r"><i class="fa fa-check"></i><b>11.5.2</b> Bootstrapping in R</a></li>
<li class="chapter" data-level="11.5.3" data-path="inference.html"><a href="inference.html#using-the-bootstrap-to-calculate-a-plausible-range"><i class="fa fa-check"></i><b>11.5.3</b> Using the bootstrap to calculate a plausible range</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="inference.html"><a href="inference.html#additional-readings-1"><i class="fa fa-check"></i><b>11.6</b> Additional readings</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression1" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Regression I: K-nearest neighbours</h1>
<div id="overview-6" class="section level2">
<h2><span class="header-section-number">8.1</span> Overview</h2>
<p>This chapter will provide an introduction to regression through K-nearest
neighbours (K-NN) in a predictive context, focusing primarily on the case where
there is a single predictor and single response variable of interest. The
chapter concludes with an example of K-nearest neighbours regression with
multiple predictors.</p>
</div>
<div id="chapter-learning-objectives-6" class="section level2">
<h2><span class="header-section-number">8.2</span> Chapter learning objectives</h2>
<p>By the end of the chapter, students will be able to:</p>
<ul>
<li>Recognize situations where a simple regression analysis would be appropriate for making predictions.</li>
<li>Explain the K-nearest neighbour (K-NN) regression algorithm and describe how it differs from K-NN classification.</li>
<li>Interpret the output of a K-NN regression.</li>
<li>In a dataset with two or more variables, perform K-nearest neighbour regression in R using a <code>tidymodels</code> workflow</li>
<li>Execute cross-validation in R to choose the number of neighbours.</li>
<li>Evaluate K-NN regression prediction accuracy in R using a test data set and an appropriate metric (<em>e.g.</em>, root means square prediction error).</li>
<li>In the context of K-NN regression, compare and contrast goodness of fit and prediction properties (namely RMSE vs RMSPE).</li>
<li>Describe advantages and disadvantages of the K-nearest neighbour regression approach.</li>
</ul>
</div>
<div id="regression" class="section level2">
<h2><span class="header-section-number">8.3</span> Regression</h2>
<p>Regression, like classification, is a predictive problem setting where we want
to use past information to predict future observations. But in the case of
regression, the goal is to predict numerical values instead of class labels.
For example, we could try to use the number of hours a person spends on
exercise each week to predict whether they would qualify for the annual Boston
marathon (<em>classification</em>) or to predict their race time itself
(<em>regression</em>). As another example, we could try to use the size of a house to
predict whether it sold for more than $500,000 (<em>classification</em>) or to
predict its sale price itself (<em>regression</em>). We will use K-nearest neighbours
to explore this question in the rest of this chapter, using a real estate data
set from Sacremento, California.</p>
</div>
<div id="sacremento-real-estate-example" class="section level2">
<h2><span class="header-section-number">8.4</span> Sacremento real estate example</h2>
<p>Let’s start by loading the libraries we need and doing some preliminary
exploratory analysis. The Sacramento real estate data set we will study in this chapter
was <a href="https://support.spatialkey.com/spatialkey-sample-csv-data/">originally reported in the Sacramento Bee</a>,
but we have provided it with this repository as a stable source for the data.</p>
<div class="sourceCode" id="cb234"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb234-1"><a href="regression1.html#cb234-1"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb234-2"><a href="regression1.html#cb234-2"></a><span class="kw">library</span>(tidymodels)</span>
<span id="cb234-3"><a href="regression1.html#cb234-3"></a><span class="kw">library</span>(gridExtra)</span>
<span id="cb234-4"><a href="regression1.html#cb234-4"></a></span>
<span id="cb234-5"><a href="regression1.html#cb234-5"></a>sacramento &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&#39;data/sacramento.csv&#39;</span>)</span>
<span id="cb234-6"><a href="regression1.html#cb234-6"></a><span class="kw">head</span>(sacramento)</span></code></pre></div>
<pre><code>## # A tibble: 6 x 9
##   city       zip     beds baths  sqft type        price latitude longitude
##   &lt;chr&gt;      &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1 SACRAMENTO z95838     2     1   836 Residential 59222     38.6     -121.
## 2 SACRAMENTO z95823     3     1  1167 Residential 68212     38.5     -121.
## 3 SACRAMENTO z95815     2     1   796 Residential 68880     38.6     -121.
## 4 SACRAMENTO z95815     2     1   852 Residential 69307     38.6     -121.
## 5 SACRAMENTO z95824     2     1   797 Residential 81900     38.5     -121.
## 6 SACRAMENTO z95841     3     1  1122 Condo       89921     38.7     -121.</code></pre>
<p>The purpose of this exercise is to understand whether we can we use house size
to predict house sale price in the Sacramento, CA area. The columns in this
data that we are interested in are <code>sqft</code> (house size, in livable square feet)
and <code>price</code> (house price, in US dollars (USD). The first step is to visualize
the data as a scatter plot where we place the predictor/explanatory variable
(house size) on the x-axis, and we place the target/response variable that we
want to predict (price) on the y-axis:</p>
<div class="sourceCode" id="cb236"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb236-1"><a href="regression1.html#cb236-1"></a>eda &lt;-<span class="st"> </span><span class="kw">ggplot</span>(sacramento, <span class="kw">aes</span>(<span class="dt">x =</span> sqft, <span class="dt">y =</span> price)) <span class="op">+</span></span>
<span id="cb236-2"><a href="regression1.html#cb236-2"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">0.4</span>) <span class="op">+</span></span>
<span id="cb236-3"><a href="regression1.html#cb236-3"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;House size (square footage)&quot;</span>) <span class="op">+</span></span>
<span id="cb236-4"><a href="regression1.html#cb236-4"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Price (USD)&quot;</span>) <span class="op">+</span></span>
<span id="cb236-5"><a href="regression1.html#cb236-5"></a><span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">labels =</span> <span class="kw">dollar_format</span>()) </span>
<span id="cb236-6"><a href="regression1.html#cb236-6"></a>eda</span></code></pre></div>
<p><img src="_main_files/figure-html/07-edaRegr-1.png" width="480" /></p>
<p>Based on the visualization above, we can see that in Sacramento, CA, as the
size of a house increases, so does its sale price. Thus, we can reason that we
may be able to use the size of a not-yet-sold house (for which we don’t know
the sale price) to predict its final sale price.</p>
</div>
<div id="k-nearest-neighbours-regression" class="section level2">
<h2><span class="header-section-number">8.5</span> K-nearest neighbours regression</h2>
<p>Much like in the case of classification, we can use a K-nearest
neighbours-based approach in regression to make predictions. Let’s take a small
sample of the data above and walk through how K-nearest neighbours (knn) works
in a regression context before we dive in to creating our model and assessing
how well it predicts house price. This subsample is taken to allow us to
illustrate the mechanics of K-NN regression with a few data points; later in
this chapter we will use all the data.</p>
<p>To take a small random sample of size 30, we’ll use the function <code>sample_n</code>.
This function takes two arguments:</p>
<ol style="list-style-type: decimal">
<li><code>tbl</code> (a data frame-like object to sample from)</li>
<li><code>size</code> (the number of observations/rows to be randomly selected/sampled)</li>
</ol>
<div class="sourceCode" id="cb237"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb237-1"><a href="regression1.html#cb237-1"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb237-2"><a href="regression1.html#cb237-2"></a>small_sacramento &lt;-<span class="st"> </span><span class="kw">sample_n</span>(sacramento, <span class="dt">size =</span> <span class="dv">30</span>)</span></code></pre></div>
<p>Next let’s say we come across a 2,000 square-foot house in Sacramento we are
interested in purchasing, with an advertised list price of $350,000. Should we
offer to pay the asking price for this house, or is it overpriced and we should
offer less? Absent any other information, we can get a sense for a good answer
to this question by using the data we have to predict the sale price given the
sale prices we have already observed. But in the plot below, we have no
observations of a house of size <em>exactly</em> 2000 square feet. How can we predict
the price?</p>
<div class="sourceCode" id="cb238"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb238-1"><a href="regression1.html#cb238-1"></a>small_plot &lt;-<span class="st"> </span><span class="kw">ggplot</span>(small_sacramento, <span class="kw">aes</span>(<span class="dt">x =</span> sqft, <span class="dt">y =</span> price)) <span class="op">+</span></span>
<span id="cb238-2"><a href="regression1.html#cb238-2"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb238-3"><a href="regression1.html#cb238-3"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;House size (square footage)&quot;</span>) <span class="op">+</span></span>
<span id="cb238-4"><a href="regression1.html#cb238-4"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Price (USD)&quot;</span>) <span class="op">+</span></span>
<span id="cb238-5"><a href="regression1.html#cb238-5"></a><span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">labels=</span><span class="kw">dollar_format</span>()) <span class="op">+</span></span>
<span id="cb238-6"><a href="regression1.html#cb238-6"></a><span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="dv">2000</span>, <span class="dt">linetype =</span> <span class="st">&quot;dotted&quot;</span>) </span>
<span id="cb238-7"><a href="regression1.html#cb238-7"></a>small_plot</span></code></pre></div>
<p><img src="_main_files/figure-html/07-small-eda-regr-1.png" width="480" /></p>
<p>We will employ the same intuition from the classification chapter, and use the
neighbouring points to the new point of interest to suggest/predict what its
price should be. For the example above, we find and label the 5 nearest
neighbours to our observation of a house that is 2000 square feet:</p>
<div class="sourceCode" id="cb239"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb239-1"><a href="regression1.html#cb239-1"></a>nearest_neighbours &lt;-<span class="st"> </span>small_sacramento <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb239-2"><a href="regression1.html#cb239-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">diff =</span> <span class="kw">abs</span>(<span class="dv">2000</span> <span class="op">-</span><span class="st"> </span>sqft)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb239-3"><a href="regression1.html#cb239-3"></a><span class="st">  </span><span class="kw">arrange</span>(diff) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb239-4"><a href="regression1.html#cb239-4"></a><span class="st">  </span><span class="kw">head</span>(<span class="dv">5</span>)</span>
<span id="cb239-5"><a href="regression1.html#cb239-5"></a>nearest_neighbours</span></code></pre></div>
<pre><code>## # A tibble: 5 x 10
##   city           zip     beds baths  sqft type         price latitude longitude  diff
##   &lt;chr&gt;          &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;
## 1 GOLD_RIVER     z95670     3     2  1981 Residential 305000     38.6     -121.    19
## 2 ELK_GROVE      z95758     4     2  2056 Residential 275000     38.4     -121.    56
## 3 ELK_GROVE      z95624     5     3  2136 Residential 223058     38.4     -121.   136
## 4 RANCHO_CORDOVA z95742     4     2  1713 Residential 263500     38.6     -121.   287
## 5 RIO_LINDA      z95673     2     2  1690 Residential 136500     38.7     -121.   310</code></pre>
<p><img src="_main_files/figure-html/07-knn3-example-1.png" width="480" /></p>
<p>Now that we have the 5 nearest neighbours (in terms of house size) to our new
2,000 square-foot house of interest, we can use their values to predict a
selling price for the new home. Specifically, we can take the mean (or
average) of these 5 values as our predicted value.</p>
<div class="sourceCode" id="cb241"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb241-1"><a href="regression1.html#cb241-1"></a>prediction &lt;-<span class="st"> </span>nearest_neighbours <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb241-2"><a href="regression1.html#cb241-2"></a><span class="st">  </span><span class="kw">summarise</span>(<span class="dt">predicted =</span> <span class="kw">mean</span>(price))</span>
<span id="cb241-3"><a href="regression1.html#cb241-3"></a>prediction</span></code></pre></div>
<pre><code>## # A tibble: 1 x 1
##   predicted
##       &lt;dbl&gt;
## 1   240612.</code></pre>
<p><img src="_main_files/figure-html/07-predictedViz-knn-1.png" width="480" /></p>
<p>Our predicted price is $240612
(shown as a red point above), which is much less than $350,000; perhaps we
might want to offer less than the list price at which the house is advertised.
But this is only the very beginning of the story. We still have all the same
unanswered questions here with K-NN regression that we had with K-NN
classification: which <span class="math inline">\(K\)</span> do we choose, and is our model any good at making
predictions? In the next few sections, we will address these questions in the
context of K-NN regression.</p>
</div>
<div id="training-evaluating-and-tuning-the-model" class="section level2">
<h2><span class="header-section-number">8.6</span> Training, evaluating, and tuning the model</h2>
<p>As usual, we must start by putting some test data away in a lock box that we
will come back to only after we choose our final model. Let’s take care of that
now. Note that for the remainder of the chapter we’ll be working with the
entire Sacramento data set, as opposed to the smaller sample of 30 points above.</p>
<div class="sourceCode" id="cb243"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb243-1"><a href="regression1.html#cb243-1"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb243-2"><a href="regression1.html#cb243-2"></a>sacramento_split &lt;-<span class="st"> </span><span class="kw">initial_split</span>(sacramento, <span class="dt">prop =</span> <span class="fl">0.6</span>, <span class="dt">strata =</span> price)</span>
<span id="cb243-3"><a href="regression1.html#cb243-3"></a>sacramento_train &lt;-<span class="st"> </span><span class="kw">training</span>(sacramento_split)</span>
<span id="cb243-4"><a href="regression1.html#cb243-4"></a>sacramento_test &lt;-<span class="st"> </span><span class="kw">testing</span>(sacramento_split)</span></code></pre></div>
<p>Next, we’ll use cross-validation to choose <span class="math inline">\(K\)</span>. In K-NN classification, we used
accuracy to see how well our predictions matched the true labels. Here in the
context of K-NN regression we will use root mean square prediction error
(RMSPE) instead. The mathematical formula for calculating RMSPE is:</p>
<p><span class="math display">\[\text{RMSPE} = \sqrt{\frac{1}{n}\sum\limits_{i=1}^{n}(y_i - \hat{y}_i)^2}\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(n\)</span> is the number of observations</li>
<li><span class="math inline">\(y_i\)</span> is the observed value for the <span class="math inline">\(i^\text{th}\)</span> observation</li>
<li><span class="math inline">\(\hat{y}_i\)</span> is the forcasted/predicted value for the <span class="math inline">\(i^\text{th}\)</span> observation</li>
</ul>
<p>A key feature of the formula for RMPSE is the squared difference between the observed
target/response variable value, <span class="math inline">\(y\)</span>, and the prediction target/response
variable value, <span class="math inline">\(\hat{y}_i\)</span>, for each observation (from 1 to <span class="math inline">\(i\)</span>).
If the predictions are very close to the true values, then
RMSPE will be small. If, on the other-hand, the predictions are very
different to the true values, then RMSPE will be quite large. When we
use cross validation, we will choose the <span class="math inline">\(K\)</span> that gives
us the smallest RMSPE.</p>
<blockquote>
<p><strong>RMSPE versus RMSE</strong>
When using many code packages (<code>tidymodels</code> included), the evaluation output
we will get to assess the prediction quality of
our K-NN regression models is labelled “RMSE”, or “root mean squared
error”. Why is this so, and why not just RMSPE?
In statistics, we try to be very precise with our
language to indicate whether we are calculating the prediction error on the
training data (<em>in-sample</em> prediction) versus on the testing data
(<em>out-of-sample</em> prediction). When predicting and evaluating prediction quality on the training data, we
say RMSE. By contrast, when predicting and evaluating prediction quality
on the testing or validation data, we say RMSPE.
The equation for calculating RMSE and RMSPE is exactly the same; all that changes is whether the <span class="math inline">\(y\)</span>s are
training or testing data. But many people just use RMSE for both,
and rely on context to denote which data the root mean squared error is being calculated on.</p>
</blockquote>
<p>Now that we know how we can assess how well our model predicts a numerical
value, let’s use R to perform cross-validation and to choose the optimal <span class="math inline">\(K\)</span>.
First, we will create a model specification for K-nearest neighbours regression,
as well as a recipe for preprocessing our data. Note that we use <code>set_mode("regression")</code>
now in the model specification to denote a regression problem, as opposed to the classification
problems from the previous chapters. Note also that we include standardization
in our preprocessing to build good habits, but since we only have one
predictor it is technically not necessary; there is no risk of comparing two predictors
of different scales.</p>
<div class="sourceCode" id="cb244"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb244-1"><a href="regression1.html#cb244-1"></a>sacr_recipe &lt;-<span class="st"> </span><span class="kw">recipe</span>(price <span class="op">~</span><span class="st"> </span>sqft, <span class="dt">data =</span> sacramento_train) <span class="op">%&gt;%</span></span>
<span id="cb244-2"><a href="regression1.html#cb244-2"></a><span class="st">                  </span><span class="kw">step_scale</span>(<span class="kw">all_predictors</span>()) <span class="op">%&gt;%</span></span>
<span id="cb244-3"><a href="regression1.html#cb244-3"></a><span class="st">                  </span><span class="kw">step_center</span>(<span class="kw">all_predictors</span>())</span>
<span id="cb244-4"><a href="regression1.html#cb244-4"></a></span>
<span id="cb244-5"><a href="regression1.html#cb244-5"></a>sacr_spec &lt;-<span class="st"> </span><span class="kw">nearest_neighbor</span>(<span class="dt">weight_func =</span> <span class="st">&quot;rectangular&quot;</span>, <span class="dt">neighbors =</span> <span class="kw">tune</span>()) <span class="op">%&gt;%</span></span>
<span id="cb244-6"><a href="regression1.html#cb244-6"></a><span class="st">                  </span><span class="kw">set_engine</span>(<span class="st">&quot;kknn&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb244-7"><a href="regression1.html#cb244-7"></a><span class="st">                  </span><span class="kw">set_mode</span>(<span class="st">&quot;regression&quot;</span>)</span>
<span id="cb244-8"><a href="regression1.html#cb244-8"></a></span>
<span id="cb244-9"><a href="regression1.html#cb244-9"></a>sacr_vfold &lt;-<span class="st"> </span><span class="kw">vfold_cv</span>(sacramento_train, <span class="dt">v =</span> <span class="dv">5</span>, <span class="dt">strata =</span> price)</span>
<span id="cb244-10"><a href="regression1.html#cb244-10"></a></span>
<span id="cb244-11"><a href="regression1.html#cb244-11"></a>sacr_wkflw &lt;-<span class="st"> </span><span class="kw">workflow</span>() <span class="op">%&gt;%</span></span>
<span id="cb244-12"><a href="regression1.html#cb244-12"></a><span class="st">                 </span><span class="kw">add_recipe</span>(sacr_recipe) <span class="op">%&gt;%</span></span>
<span id="cb244-13"><a href="regression1.html#cb244-13"></a><span class="st">                 </span><span class="kw">add_model</span>(sacr_spec)</span>
<span id="cb244-14"><a href="regression1.html#cb244-14"></a>sacr_wkflw</span></code></pre></div>
<pre><code>## ══ Workflow ═════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════
## Preprocessor: Recipe
## Model: nearest_neighbor()
## 
## ── Preprocessor ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
## 2 Recipe Steps
## 
## ● step_scale()
## ● step_center()
## 
## ── Model ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
## K-Nearest Neighbor Model Specification (regression)
## 
## Main Arguments:
##   neighbors = tune()
##   weight_func = rectangular
## 
## Computational engine: kknn</code></pre>
<p>The major difference you can see in the above workflow compared to previous
chapters is that we are running regression rather than classification. The fact
that we use <code>set_mode("regression")</code> essentially
tells <code>tidymodels</code> that we need to use different metrics (RMSPE, not accuracy)
for tuning and evaluation. You can see this in the following code, which tunes
the model and returns the RMSPE for each number of neighbours.</p>
<div class="sourceCode" id="cb246"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb246-1"><a href="regression1.html#cb246-1"></a>gridvals &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">neighbors =</span> <span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">200</span>))</span>
<span id="cb246-2"><a href="regression1.html#cb246-2"></a></span>
<span id="cb246-3"><a href="regression1.html#cb246-3"></a>sacr_results &lt;-<span class="st"> </span>sacr_wkflw <span class="op">%&gt;%</span></span>
<span id="cb246-4"><a href="regression1.html#cb246-4"></a><span class="st">                   </span><span class="kw">tune_grid</span>(<span class="dt">resamples =</span> sacr_vfold, <span class="dt">grid =</span> gridvals) <span class="op">%&gt;%</span></span>
<span id="cb246-5"><a href="regression1.html#cb246-5"></a><span class="st">                   </span><span class="kw">collect_metrics</span>() </span>
<span id="cb246-6"><a href="regression1.html#cb246-6"></a></span>
<span id="cb246-7"><a href="regression1.html#cb246-7"></a><span class="co"># show all the results</span></span>
<span id="cb246-8"><a href="regression1.html#cb246-8"></a>sacr_results</span></code></pre></div>
<pre><code>## # A tibble: 400 x 6
##    neighbors .metric .estimator       mean     n   std_err
##        &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt;
##  1         1 rmse    standard   116654.        5 7186.    
##  2         1 rsq     standard        0.372     5    0.0463
##  3         2 rmse    standard   101103.        5 5872.    
##  4         2 rsq     standard        0.453     5    0.0386
##  5         3 rmse    standard    96708.        5 4211.    
##  6         3 rsq     standard        0.483     5    0.0277
##  7         4 rmse    standard    93927.        5 3502.    
##  8         4 rsq     standard        0.503     5    0.0298
##  9         5 rmse    standard    89642.        5 3768.    
## 10         5 rsq     standard        0.543     5    0.0303
## # … with 390 more rows</code></pre>
<p>We take the <em>minimum</em> RMSPE to find the best setting for the number of neighbours:</p>
<div class="sourceCode" id="cb248"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb248-1"><a href="regression1.html#cb248-1"></a><span class="co"># show only the row of minimum RMSPE</span></span>
<span id="cb248-2"><a href="regression1.html#cb248-2"></a>sacr_min &lt;-<span class="st"> </span>sacr_results <span class="op">%&gt;%</span></span>
<span id="cb248-3"><a href="regression1.html#cb248-3"></a><span class="st">               </span><span class="kw">filter</span>(.metric <span class="op">==</span><span class="st"> &#39;rmse&#39;</span>) <span class="op">%&gt;%</span></span>
<span id="cb248-4"><a href="regression1.html#cb248-4"></a><span class="st">               </span><span class="kw">filter</span>(mean <span class="op">==</span><span class="st"> </span><span class="kw">min</span>(mean))</span>
<span id="cb248-5"><a href="regression1.html#cb248-5"></a>sacr_min</span></code></pre></div>
<pre><code>## # A tibble: 1 x 6
##   neighbors .metric .estimator   mean     n std_err
##       &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
## 1        14 rmse    standard   84356.     5   4050.</code></pre>
<p>Here we can see that the smallest RMSPE occurs when <span class="math inline">\(K =\)</span> 14.</p>
</div>
<div id="underfitting-and-overfitting" class="section level2">
<h2><span class="header-section-number">8.7</span> Underfitting and overfitting</h2>
<p>Similar to the setting of classification, by setting the number of neighbours
to be too small or too large, we cause the RMSPE to increase:</p>
<p><img src="_main_files/figure-html/07-choose-k-knn-plot-1.png" width="480" /></p>
<p>What is happening here? To visualize the effect of different settings of <span class="math inline">\(K\)</span> on the
regression model, we will plot the predicted values for house price from
our K-NN regression models for 6 different values for <span class="math inline">\(K\)</span>. For each model, we
predict a price for every possible home size across the range of home sizes we
observed in the data set (here 500 to 4250 square feet) and we plot the
predicted prices as a blue line:</p>
<p><img src="_main_files/figure-html/07-howK-1.png" width="960" /></p>
<p>Based on the plots above, we see that when <span class="math inline">\(K\)</span> = 1, the blue line runs perfectly
through almost all of our training observations. This happens because our
predicted values for a given region depend on just a single observation. A
model like this has high variance and low bias (intuitively, it provides unreliable
predictions). It has high variance because
the flexible blue line follows the training observations very closely, and if
we were to change any one of the training observation data points we would
change the flexible blue line quite a lot. This means that the blue line
matches the data we happen to have in this training data set, however, if we
were to collect another training data set from the Sacramento real estate
market it likely wouldn’t match those observations as well. Another term
that we use to collectively describe this phenomenon is <em>overfitting</em>.</p>
<p>What about the plot where <span class="math inline">\(K\)</span> is quite large, say <span class="math inline">\(K\)</span> = 450, for example? When
<span class="math inline">\(K\)</span> = 450 for this data set, the blue line is extremely smooth, and almost
flat. This happens because our predicted values for a given x value (here home
size), depend on many many (450) neighbouring observations. A model
like this has low variance and high bias (intuitively, it provides very reliable,
but generally very inaccurate predictions). It has low variance because the
smooth, inflexible blue line does not follow the training observations very
closely, and if we were to change any one of the training observation data
points it really wouldn’t affect the shape of the smooth blue line at all. This
means that although the blue line matches does not match the data we happen to
have in this particular training data set perfectly, if we were to collect
another training data set from the Sacramento real estate market it likely
would match those observations equally as well as it matches those in this
training data set. Another term that
we use to collectively describe this kind of model is <em>underfitting</em>.</p>
<p>Ideally, what we want is neither of the two situations discussed above. Instead,
we would like a model with low variance (so that it will transfer/generalize
well to other data sets, and isn’t too dependent on the
observations that happen to be in the training set) <strong>and</strong> low bias
(where the model does not completely ignore our training data). If we explore
the other values for <span class="math inline">\(K\)</span>, in particular <span class="math inline">\(K\)</span> = 14
(as suggested by cross-validation),
we can see it has a lower bias than our model with a very high <span class="math inline">\(K\)</span> (e.g., 450),
and thus the model/predicted values better match the actual observed values
than the high <span class="math inline">\(K\)</span> model. Additionally, it has lower variance than our model
with a very low <span class="math inline">\(K\)</span> (e.g., 1) and thus it should better transer/generalize to
other data sets compared to the low <span class="math inline">\(K\)</span> model. All of this is similar to how
the choice of <span class="math inline">\(K\)</span> affects K-NN classification (discussed in the previous
chapter).</p>
</div>
<div id="evaluating-on-the-test-set" class="section level2">
<h2><span class="header-section-number">8.8</span> Evaluating on the test set</h2>
<p>To assess how well our model might do at predicting on unseen data, we will
assess its RMSPE on the test data. To do this, we will first
re-train our K-NN regression model on the entire training data set,
using <span class="math inline">\(K =\)</span> 14 neighbours. Then we will
use <code>predict</code> to make predictions on the test data, and use the <code>metrics</code>
function again to compute the summary of regression quality. Because
we specify that we are performing regression in <code>set_mode</code>, the <code>metrics</code>
function knows to output a quality summary related to regression, and not, say, classification.</p>
<div class="sourceCode" id="cb250"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb250-1"><a href="regression1.html#cb250-1"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb250-2"><a href="regression1.html#cb250-2"></a>kmin &lt;-<span class="st"> </span>sacr_min <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(neighbors)</span>
<span id="cb250-3"><a href="regression1.html#cb250-3"></a>sacr_spec &lt;-<span class="st"> </span><span class="kw">nearest_neighbor</span>(<span class="dt">weight_func =</span> <span class="st">&quot;rectangular&quot;</span>, <span class="dt">neighbors =</span> kmin) <span class="op">%&gt;%</span></span>
<span id="cb250-4"><a href="regression1.html#cb250-4"></a><span class="st">            </span><span class="kw">set_engine</span>(<span class="st">&quot;kknn&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb250-5"><a href="regression1.html#cb250-5"></a><span class="st">            </span><span class="kw">set_mode</span>(<span class="st">&quot;regression&quot;</span>)</span>
<span id="cb250-6"><a href="regression1.html#cb250-6"></a></span>
<span id="cb250-7"><a href="regression1.html#cb250-7"></a>sacr_fit &lt;-<span class="st"> </span><span class="kw">workflow</span>() <span class="op">%&gt;%</span></span>
<span id="cb250-8"><a href="regression1.html#cb250-8"></a><span class="st">           </span><span class="kw">add_recipe</span>(sacr_recipe) <span class="op">%&gt;%</span></span>
<span id="cb250-9"><a href="regression1.html#cb250-9"></a><span class="st">           </span><span class="kw">add_model</span>(sacr_spec) <span class="op">%&gt;%</span></span>
<span id="cb250-10"><a href="regression1.html#cb250-10"></a><span class="st">           </span><span class="kw">fit</span>(<span class="dt">data =</span> sacramento_train)</span>
<span id="cb250-11"><a href="regression1.html#cb250-11"></a></span>
<span id="cb250-12"><a href="regression1.html#cb250-12"></a>sacr_summary &lt;-<span class="st"> </span>sacr_fit <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb250-13"><a href="regression1.html#cb250-13"></a><span class="st">           </span><span class="kw">predict</span>(sacramento_test) <span class="op">%&gt;%</span></span>
<span id="cb250-14"><a href="regression1.html#cb250-14"></a><span class="st">           </span><span class="kw">bind_cols</span>(sacramento_test) <span class="op">%&gt;%</span></span>
<span id="cb250-15"><a href="regression1.html#cb250-15"></a><span class="st">           </span><span class="kw">metrics</span>(<span class="dt">truth =</span> price, <span class="dt">estimate =</span> .pred) </span>
<span id="cb250-16"><a href="regression1.html#cb250-16"></a></span>
<span id="cb250-17"><a href="regression1.html#cb250-17"></a>sacr_summary</span></code></pre></div>
<pre><code>## # A tibble: 3 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 rmse    standard   87737.   
## 2 rsq     standard       0.546
## 3 mae     standard   65400.</code></pre>
<p>Our final model’s test error as assessed by RMSPE
is 87737.
But what does this RMSPE score mean? When we calculated test set prediction accuracy in K-NN
classification, the highest possible value was 1 and the lowest possible value was 0.
If we got a value close to 1, our model was “good;” and otherwise,
the model was “not good.” What about RMSPE? Unfortunately there is no default scale
for RMSPE. Instead, it is measured in
the units of the target/response variable, and so it is a bit hard to
interpret. For now, let’s consider this approach to thinking about RMSPE from
our testing data set: as long as its not significantly worse than the cross-validation
RMSPE of our best model, then we can say that we’re not doing too much worse on
the test data than we did on the training data. So the model appears to be
generalizing well to a new data set it has never seen before. In future courses
on statistical/machine learning, you will learn more about how to interpret RMSPE
from testing data and other ways to assess models.</p>
<p>Finally, what does our model look like when we predict across all possible
house sizes we might encounter in the Sacramento area? We plotted it above
where we explored how <span class="math inline">\(k\)</span> affects K-NN regression, but we show it again now,
along with the code that generated it:</p>
<div class="sourceCode" id="cb252"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb252-1"><a href="regression1.html#cb252-1"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb252-2"><a href="regression1.html#cb252-2"></a>sacr_preds &lt;-<span class="st"> </span>sacr_fit <span class="op">%&gt;%</span></span>
<span id="cb252-3"><a href="regression1.html#cb252-3"></a><span class="st">                </span><span class="kw">predict</span>(sacramento_train) <span class="op">%&gt;%</span></span>
<span id="cb252-4"><a href="regression1.html#cb252-4"></a><span class="st">                </span><span class="kw">bind_cols</span>(sacramento_train)</span>
<span id="cb252-5"><a href="regression1.html#cb252-5"></a></span>
<span id="cb252-6"><a href="regression1.html#cb252-6"></a>plot_final &lt;-<span class="st"> </span><span class="kw">ggplot</span>(sacr_preds, <span class="kw">aes</span>(<span class="dt">x =</span> sqft, <span class="dt">y =</span> price)) <span class="op">+</span></span>
<span id="cb252-7"><a href="regression1.html#cb252-7"></a><span class="st">            </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">0.4</span>) <span class="op">+</span></span>
<span id="cb252-8"><a href="regression1.html#cb252-8"></a><span class="st">            </span><span class="kw">xlab</span>(<span class="st">&quot;House size (square footage)&quot;</span>) <span class="op">+</span></span>
<span id="cb252-9"><a href="regression1.html#cb252-9"></a><span class="st">            </span><span class="kw">ylab</span>(<span class="st">&quot;Price (USD)&quot;</span>) <span class="op">+</span></span>
<span id="cb252-10"><a href="regression1.html#cb252-10"></a><span class="st">            </span><span class="kw">scale_y_continuous</span>(<span class="dt">labels =</span> <span class="kw">dollar_format</span>())  <span class="op">+</span></span>
<span id="cb252-11"><a href="regression1.html#cb252-11"></a><span class="st">            </span><span class="kw">geom_line</span>(<span class="dt">data =</span> sacr_preds, <span class="kw">aes</span>(<span class="dt">x =</span> sqft, <span class="dt">y =</span> .pred), <span class="dt">color =</span> <span class="st">&quot;blue&quot;</span>) <span class="op">+</span></span>
<span id="cb252-12"><a href="regression1.html#cb252-12"></a><span class="st">            </span><span class="kw">ggtitle</span>(<span class="kw">paste0</span>(<span class="st">&quot;K = &quot;</span>, kmin))</span>
<span id="cb252-13"><a href="regression1.html#cb252-13"></a></span>
<span id="cb252-14"><a href="regression1.html#cb252-14"></a>plot_final</span></code></pre></div>
<p><img src="_main_files/figure-html/07-predict-all-1.png" width="480" /></p>
</div>
<div id="strengths-and-limitations-of-k-nn-regression" class="section level2">
<h2><span class="header-section-number">8.9</span> Strengths and limitations of K-NN regression</h2>
<p>As with K-NN classification (or any prediction algorithm for that manner), K-NN regression has both strengths and weaknesses. Some are listed here:</p>
<p><strong>Strengths of K-NN regression</strong></p>
<ol style="list-style-type: decimal">
<li>Simple and easy to understand</li>
<li>No assumptions about what the data must look like</li>
<li>Works well with non-linear relationships (i.e., if the relationship is not a straight line)</li>
</ol>
<p><strong>Limitations of K-NN regression</strong></p>
<ol style="list-style-type: decimal">
<li>As data gets bigger and bigger, K-NN gets slower and slower, quite quickly 2. Does not perform well with a large number of predictors unless the size of the training set is exponentially larger</li>
<li>Does not predict well beyond the range of values input in your training data</li>
</ol>
</div>
<div id="multivariate-k-nn-regression" class="section level2">
<h2><span class="header-section-number">8.10</span> Multivariate K-NN regression</h2>
<p>As in K-NN classification, in K-NN regression we can have multiple predictors.
When we have multiple predictors in K-NN regression, we have the same concern
regarding the scale of the predictors. This is because once again,
predictions are made by identifying the <span class="math inline">\(K\)</span>
observations that are nearest to the new point we want to predict, and any
variables that are on a large scale will have a much larger effect than
variables on a small scale. Since the <code>recipe</code> we built above scales and centers
all predictor variables, this is handled for us.</p>
<p>We will now demonstrate a multivariate K-NN regression analysis of the
Sacramento real estate data using <code>tidymodels</code>. This time we will use
house size (measured in square feet) as well as number of bedrooms as our
predictors, and continue to use house sale price as our outcome/target variable
that we are trying to predict.</p>
<p>It is always a good practice to do exploratory data analysis, such as
visualizing the data, before we start modeling the data. Thus the first thing
we will do is use ggpairs (from the <code>GGally</code> package) to plot all the variables
we are interested in using in our analyses:</p>
<div class="sourceCode" id="cb253"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb253-1"><a href="regression1.html#cb253-1"></a><span class="kw">library</span>(GGally)</span>
<span id="cb253-2"><a href="regression1.html#cb253-2"></a>plot_pairs &lt;-<span class="st"> </span>sacramento <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb253-3"><a href="regression1.html#cb253-3"></a><span class="st">  </span><span class="kw">select</span>(price, sqft, beds) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb253-4"><a href="regression1.html#cb253-4"></a><span class="st">  </span><span class="kw">ggpairs</span>()</span>
<span id="cb253-5"><a href="regression1.html#cb253-5"></a>plot_pairs</span></code></pre></div>
<p><img src="_main_files/figure-html/07-ggpairs-1.png" width="576" /></p>
<p>From this we can see that generally, as both house size and number of bedrooms increase, so does price. Does adding the number of bedrooms to our model improve our ability to predict house price? To answer that question, we will have to come up with the test error for a K-NN regression model using house size and number of bedrooms, and then we can compare it to the test error for the model we previously came up with that only used house size to see if it is smaller (decreased test error indicates increased prediction quality). Let’s do that now!</p>
<p>First we’ll build a new model specification and recipe for the analysis. Note that
we use the formula <code>price ~ sqft + beds</code> to denote that we have two predictors,
and set <code>neighbors = tune()</code> to tell <code>tidymodels</code> to tune the number of neighbours for us.</p>
<div class="sourceCode" id="cb254"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb254-1"><a href="regression1.html#cb254-1"></a>sacr_recipe &lt;-<span class="st"> </span><span class="kw">recipe</span>(price <span class="op">~</span><span class="st"> </span>sqft <span class="op">+</span><span class="st"> </span>beds, <span class="dt">data =</span> sacramento_train) <span class="op">%&gt;%</span></span>
<span id="cb254-2"><a href="regression1.html#cb254-2"></a><span class="st">                  </span><span class="kw">step_scale</span>(<span class="kw">all_predictors</span>()) <span class="op">%&gt;%</span></span>
<span id="cb254-3"><a href="regression1.html#cb254-3"></a><span class="st">                  </span><span class="kw">step_center</span>(<span class="kw">all_predictors</span>())</span>
<span id="cb254-4"><a href="regression1.html#cb254-4"></a></span>
<span id="cb254-5"><a href="regression1.html#cb254-5"></a>sacr_spec &lt;-<span class="st"> </span><span class="kw">nearest_neighbor</span>(<span class="dt">weight_func =</span> <span class="st">&quot;rectangular&quot;</span>, <span class="dt">neighbors =</span> <span class="kw">tune</span>()) <span class="op">%&gt;%</span></span>
<span id="cb254-6"><a href="regression1.html#cb254-6"></a><span class="st">                  </span><span class="kw">set_engine</span>(<span class="st">&quot;kknn&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb254-7"><a href="regression1.html#cb254-7"></a><span class="st">                  </span><span class="kw">set_mode</span>(<span class="st">&quot;regression&quot;</span>)</span></code></pre></div>
<p>Next, we’ll use 5-fold cross-validation to choose the number of neighbours via the minimum RMSPE:</p>
<div class="sourceCode" id="cb255"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb255-1"><a href="regression1.html#cb255-1"></a>gridvals &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">neighbors =</span> <span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">200</span>))</span>
<span id="cb255-2"><a href="regression1.html#cb255-2"></a>sacr_k &lt;-<span class="st"> </span><span class="kw">workflow</span>() <span class="op">%&gt;%</span></span>
<span id="cb255-3"><a href="regression1.html#cb255-3"></a><span class="st">                 </span><span class="kw">add_recipe</span>(sacr_recipe) <span class="op">%&gt;%</span></span>
<span id="cb255-4"><a href="regression1.html#cb255-4"></a><span class="st">                 </span><span class="kw">add_model</span>(sacr_spec) <span class="op">%&gt;%</span></span>
<span id="cb255-5"><a href="regression1.html#cb255-5"></a><span class="st">                 </span><span class="kw">tune_grid</span>(sacr_vfold, <span class="dt">grid =</span> gridvals) <span class="op">%&gt;%</span></span>
<span id="cb255-6"><a href="regression1.html#cb255-6"></a><span class="st">                 </span><span class="kw">collect_metrics</span>() <span class="op">%&gt;%</span></span>
<span id="cb255-7"><a href="regression1.html#cb255-7"></a><span class="st">                 </span><span class="kw">filter</span>(.metric <span class="op">==</span><span class="st"> &#39;rmse&#39;</span>) <span class="op">%&gt;%</span></span>
<span id="cb255-8"><a href="regression1.html#cb255-8"></a><span class="st">                 </span><span class="kw">filter</span>(mean <span class="op">==</span><span class="st"> </span><span class="kw">min</span>(mean)) <span class="op">%&gt;%</span></span>
<span id="cb255-9"><a href="regression1.html#cb255-9"></a><span class="st">                 </span><span class="kw">pull</span>(neighbors)</span>
<span id="cb255-10"><a href="regression1.html#cb255-10"></a>sacr_k</span></code></pre></div>
<pre><code>## [1] 14</code></pre>
<p>Here we see that the smallest RMSPE occurs when <span class="math inline">\(K =\)</span> 14.</p>
<p>Now that we have chosen <span class="math inline">\(K\)</span>, we need to re-train the model on the entire
training data set, and after that we can use that model to predict
on the test data to get our test error.</p>
<div class="sourceCode" id="cb257"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb257-1"><a href="regression1.html#cb257-1"></a>sacr_spec &lt;-<span class="st"> </span><span class="kw">nearest_neighbor</span>(<span class="dt">weight_func =</span> <span class="st">&quot;rectangular&quot;</span>, <span class="dt">neighbors =</span> sacr_k) <span class="op">%&gt;%</span></span>
<span id="cb257-2"><a href="regression1.html#cb257-2"></a><span class="st">                  </span><span class="kw">set_engine</span>(<span class="st">&quot;kknn&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb257-3"><a href="regression1.html#cb257-3"></a><span class="st">                  </span><span class="kw">set_mode</span>(<span class="st">&quot;regression&quot;</span>)</span>
<span id="cb257-4"><a href="regression1.html#cb257-4"></a></span>
<span id="cb257-5"><a href="regression1.html#cb257-5"></a>knn_mult_fit &lt;-<span class="st"> </span><span class="kw">workflow</span>() <span class="op">%&gt;%</span></span>
<span id="cb257-6"><a href="regression1.html#cb257-6"></a><span class="st">                </span><span class="kw">add_recipe</span>(sacr_recipe) <span class="op">%&gt;%</span></span>
<span id="cb257-7"><a href="regression1.html#cb257-7"></a><span class="st">                </span><span class="kw">add_model</span>(sacr_spec) <span class="op">%&gt;%</span></span>
<span id="cb257-8"><a href="regression1.html#cb257-8"></a><span class="st">                </span><span class="kw">fit</span>(<span class="dt">data =</span> sacramento_train)</span>
<span id="cb257-9"><a href="regression1.html#cb257-9"></a></span>
<span id="cb257-10"><a href="regression1.html#cb257-10"></a>knn_mult_preds &lt;-<span class="st"> </span>knn_mult_fit <span class="op">%&gt;%</span></span>
<span id="cb257-11"><a href="regression1.html#cb257-11"></a><span class="st">                </span><span class="kw">predict</span>(sacramento_test) <span class="op">%&gt;%</span></span>
<span id="cb257-12"><a href="regression1.html#cb257-12"></a><span class="st">                </span><span class="kw">bind_cols</span>(sacramento_test)</span>
<span id="cb257-13"><a href="regression1.html#cb257-13"></a></span>
<span id="cb257-14"><a href="regression1.html#cb257-14"></a>knn_mult_mets &lt;-<span class="st"> </span><span class="kw">metrics</span>(knn_mult_preds, <span class="dt">truth =</span> price, <span class="dt">estimate =</span> .pred) </span>
<span id="cb257-15"><a href="regression1.html#cb257-15"></a>knn_mult_mets</span></code></pre></div>
<pre><code>## # A tibble: 3 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 rmse    standard   85152.   
## 2 rsq     standard       0.572
## 3 mae     standard   63575.</code></pre>
<p>This time when we performed K-NN regression on the same data set, but also
included number of bedrooms as a predictor we obtained a RMSPE test error
of 85152.
This compares to a RMSPE test error
of 87737
when we used only house size as the
single predictor. Thus in this case, we did not improve the model
by a large amount by adding this additional predictor.</p>
<p>We can also visualize the model’s predictions overlaid on top of the data. This time the predictions
will be a surface in 3-D space, instead of a line in 2-D space, as we have 2
predictors instead of 1.
<div id="htmlwidget-241b21784818e0a38b84" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-241b21784818e0a38b84">{"x":{"visdat":{"4f0688d603f":["function () ","plotlyVisDat"],"4f02b634939":["function () ","data"]},"cur_data":"4f02b634939","attrs":{"4f02b634939":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":{},"y":{},"z":{},"type":"scatter3d","mode":"markers","marker":{"size":5,"opacity":0.4,"color":"red"},"inherit":true},"4f02b634939.1":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":{},"type":"surface","x":{},"y":{},"colorbar":{"title":"Price (USD)"},"inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"House size (square feet)"},"zaxis":{"title":"Price (USD)"},"yaxis":{"title":"Number of bedrooms"}},"hovermode":"closest","showlegend":false,"legend":{"yanchor":"top","y":0.5}},"source":"A","config":{"showSendToCloud":false},"data":[{"x":[836,796,797,1122,1104,941,909,1289,871,1022,1134,844,795,1118,1240,1601,901,963,1380,1248,1039,1152,1380,1116,1082,1472,1146,1056,1043,1587,1120,1580,1477,1185,1406,1943,1172,1851,1130,1420,1280,2162,1665,1511,1590,1596,2136,1478,1287,1277,1448,2235,1193,2163,1269,1473,2508,1305,1591,1326,1921,1541,1672,1380,975,1284,3009,1126,1843,1520,2309,2367,3516,1914,1690,2725,2354,1842,1801,1961,3164,2054,1830,1627,3440,2846,2359,2052,3433,3615,2687,2724,3508,2462,2900,3705,3527,4878,1099,840,800,746,1337,924,610,1516,1220,722,1080,1050,1110,1120,1080,957,1080,1266,994,1202,1039,722,1183,1320,1364,1310,810,1123,904,1156,1439,1671,1740,1265,1007,1716,1685,1555,1137,1174,1393,1415,2126,1289,1799,1308,1953,723,948,1578,1317,1360,1522,1751,1465,1605,1475,1216,1315,1567,1291,1453,1503,1456,1574,1595,1567,2943,1768,2030,1531,2056,2494,1450,2169,1440,1527,1401,1411,2990,1533,1910,1981,3072,2205,1449,1258,539,1108,1595,1900,1718,3389,2190,3260,2016,2724,3192,2829,3928,3577,1247,2068,3992,3881,2272,2791,3117,3984,2222,2484,1624,484,970,932,796,834,795,1250,795,918,1082,964,888,1331,1014,1448,1100,1207,1995,1366,901,1104,972,1390,1209,1245,1416,1300,1120,1590,1407,1646,1676,1370,1351,1152,990,1162,1280,1039,1159,1520,1353,1505,1009,1144,930,1940,1258,1758,2142,950,1739,1516,988,1212,1871,1302,756,1058,1427,1798,2652,1816,3076,1176,1160,1424,1255,1718,1904,1808,2711,1713,2724,1468,2550,1922,1343,960,2109,1876,2218,1394,1410,2346,1659,2590,1673,1810,2789,1606,3499,1119,1871,1800,1625,2752,1179,2724,3281,1993,1939,1788,1691,4303,4246,2274,2962,3056,2503,3198,1905,1320,3741,3357,2025,3788,3670,1000,838,1032,904,1080,990,900,861,1011,1089,832,800,1292,1064,1320,1410,1115,1164,1219,1127,1272,1120,1260,1132,1466,1092,1628,960,1075,1428,2475,1711,1483,1140,1549,1410,1240,1712,1580,1669,1650,1200,1170,1695,1157,1410,1593,1093,1770,1124,1139,1638,1328,1273,1082,1578,1386,1513,1232,1473,1127,1144,972,2306,1479,1040,1800,1120,1232,1462,2329,1351,1284,1376,1032,1419,1637,1776,1338,1257,2254,1991,1094,2111,1915,1962,1406,1789,1876,1899,1636,1438,1451,1520,1506,2605,1811,1540,1910,1846,2494,2214,2280,1443,1582,1857,1720,2160,1382,1282,1919,1982,1144,1623,2592,2048,1327,1800,2457,2264,2004,3134,1951,1276,1888,1548,2109,2484,2258,2212,3163,2960,2172,1924,2577,1655,3179,2049,3042,2875,2199,2242,2334,1944,3913,2787,2824,3261,2053,1704,2418,3282,2806,3173,3380,4090,2356,3220,3579,3885,4400,864,1158,1139,1058,1040,1354,682,1161,1229,1249,1161,1010,1188,1093,1127,1309,970,1144,1065,1206,884,1019,924,1217,1670,1373,1381,1917,1608,1344,1057,1177,1582,1204,1497,1194,1428,1039,1529,1294,1638,1175,1416,936,1358,1609,1968,1089,1189,795,1371,1310,1262,1416,1302,1418,1462,1627,1665,1040,1197,1456,1450,1358,1329,1715,1262,2280,1477,1216,1685,1362],"y":[2,2,2,3,3,2,3,3,1,2,2,2,2,3,4,3,3,3,4,3,2,3,3,3,3,4,4,3,2,4,4,4,3,3,3,4,3,4,3,3,3,4,2,3,4,4,5,3,3,4,4,4,3,3,3,3,5,3,4,2,4,3,3,3,3,2,4,2,3,2,4,5,5,4,2,3,4,3,4,3,5,4,4,3,4,3,4,3,5,5,5,4,5,4,5,4,4,6,4,2,2,2,3,3,2,3,2,1,3,3,3,3,3,3,3,3,2,3,2,2,4,3,3,4,2,2,2,3,3,3,3,3,2,3,4,3,3,3,3,3,4,3,4,3,4,2,2,4,3,3,3,4,3,4,4,3,2,3,3,3,3,4,3,4,3,4,4,4,3,4,4,3,4,3,3,3,3,4,2,4,3,5,4,2,3,2,3,4,4,4,5,4,5,3,4,4,3,5,5,2,3,4,5,4,3,5,5,4,5,4,1,3,2,2,2,2,3,2,2,3,2,2,3,3,4,2,3,4,3,2,3,2,4,3,3,3,3,3,4,3,3,3,3,4,3,2,2,4,3,3,3,3,3,3,3,2,4,3,3,3,2,4,3,3,3,4,3,2,3,3,4,4,4,5,4,3,3,2,3,4,4,4,3,4,3,4,3,3,2,4,4,4,3,3,5,4,4,3,3,4,4,5,2,4,3,3,4,3,5,5,3,4,2,3,4,5,3,4,4,3,3,3,3,5,4,3,5,5,1,2,2,2,2,2,2,2,2,3,2,2,4,4,3,3,3,3,3,4,3,3,3,2,3,3,4,3,3,3,6,3,4,3,4,3,3,5,4,3,3,3,3,4,3,3,3,3,3,3,4,3,3,3,3,4,3,3,3,3,3,3,3,4,3,2,4,3,3,3,4,3,3,4,2,4,2,4,3,2,5,4,3,4,4,3,2,3,4,4,3,4,3,3,3,4,3,3,3,4,5,4,5,3,4,3,3,4,4,3,3,4,2,2,5,4,3,4,5,3,4,5,4,3,4,3,4,4,4,3,5,3,3,3,4,3,5,3,4,5,4,4,3,3,5,5,5,4,4,3,3,4,4,3,4,4,4,4,5,3,4,2,4,2,3,3,3,1,3,3,3,3,3,2,3,3,4,2,3,1,3,2,3,2,3,3,3,4,4,4,3,2,2,3,3,4,1,3,3,3,3,3,3,3,1,3,4,4,2,2,2,3,3,3,3,3,3,3,4,3,3,3,3,3,3,4,4,3,4,3,3,4,3],"z":[59222,68880,81900,89921,90895,94905,100309,106250,106852,108750,110700,113263,116250,122000,123000,124100,125000,127281,131200,132000,133000,134555,136500,138750,147308,148750,149593,156896,161250,161500,164000,165000,168000,178480,178760,179580,181000,182587,182750,189000,192067,195000,206000,208000,212864,221000,223058,231477,234697,235000,236000,236685,240122,242638,244000,244500,245918,250000,250000,250134,254200,260000,265000,265000,271742,280908,280987,292024,298000,299000,311000,315537,320000,328360,334150,335750,335750,339500,346210,347029,375000,381942,387731,391000,394470,395000,400186,415000,425000,430000,460000,461000,510000,539000,585000,600000,606238,830000,70000,71000,78000,78400,90000,92000,93675,97750,98000,98000,106716,123225,123750,125000,126000,129000,140000,140000,142500,143500,145000,145000,146000,148500,150000,150000,156000,156000,157788,161653,168000,175000,176250,179000,180000,180400,182000,185000,194000,195000,200000,201000,202500,205000,205000,205000,205000,207000,211500,215000,215000,215500,225000,225000,225000,228000,229665,230000,230000,230000,236250,242000,245000,250000,255000,260000,261000,261800,265000,270000,270000,275000,280000,286013,292000,292000,293993,294000,296769,297500,300000,300500,305000,315000,319789,330000,330000,334000,339000,339000,361745,361948,370000,380000,385000,399000,406026,425000,433500,436746,438700,445000,460000,460000,471750,500500,504000,541000,572500,582000,699000,839000,48000,61500,65000,65000,68000,82732,84000,90000,91000,95000,97500,101000,112500,113000,114000,115000,119250,120000,120108,121500,123000,125000,125573,130000,139500,140000,140800,145000,147000,149600,150000,155000,155435,158000,158000,160000,164000,167293,168000,170000,170000,192000,197654,198000,200345,203000,208000,212000,217500,218000,220000,221000,222900,223139,228327,230000,230000,230522,233641,236073,238861,239700,240000,240000,247234,249862,251000,260000,261000,261000,262500,266000,266000,274425,275336,277980,284686,284893,285000,296000,299940,311328,313138,316630,320000,328578,331500,344755,345746,351000,353767,355000,360000,360552,362305,367554,368500,378000,383000,395100,400000,423000,427500,430922,450000,452000,470000,471000,475000,484500,487500,500000,506688,579093,668365,677048,691659,760000,30000,55422,65000,65000,65000,66500,71000,75000,85000,95625,96140,104250,105000,109000,115500,115620,116000,122500,124000,124000,124413,130000,137760,145000,150000,150000,151000,155000,155800,156142,159900,161500,161600,162000,165000,165000,167293,168000,168000,168750,173000,176095,176250,179000,180000,180000,181000,182000,182587,185833,186785,187000,188335,190000,190000,190000,191250,193500,194818,195000,199900,200000,201000,204918,205000,205000,207000,209000,210000,212500,213750,215000,215000,215000,220000,220000,220702,221250,222000,222500,222750,225000,229000,233500,240000,240971,242000,243450,243500,249000,250000,252000,255000,255000,255000,257200,263500,266510,271000,272700,276000,278000,279000,280000,280000,285000,290000,290000,293996,294000,298000,298000,299000,300000,303000,306000,310000,310000,312000,313000,315000,315000,315000,320000,325000,328370,330000,331200,332000,334000,347225,350000,350000,356200,367463,386222,389000,395500,396000,397000,400000,412500,415000,425000,433500,441000,445000,446000,450000,475000,490000,493000,511000,525000,533000,560000,600000,600000,610000,680000,884790,68566,80000,93600,95000,97750,104000,107666,109000,110000,112500,114800,116000,123675,127059,130000,131750,132000,134000,138000,142000,147000,148750,150454,151087,157296,160000,161250,165000,165750,166000,170000,171750,172000,174250,179500,188000,188700,189000,189000,191250,191675,200000,200000,200100,201528,204750,205000,205000,207000,207973,208250,208318,209347,216000,219794,220000,220000,223000,224000,224000,224500,225000,228000,229027,229500,230000,230000,232425,234000,235000,235301,235738],"type":"scatter3d","mode":"markers","marker":{"color":"red","size":5,"opacity":0.4,"line":{"color":"rgba(31,119,180,1)"}},"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null},{"colorbar":{"title":"Price (USD)","ticklen":2,"len":0.5,"lenmode":"fraction","y":1,"yanchor":"top"},"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666667","rgba(70,19,97,1)"],["0.0833333333333333","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333333","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":true,"z":[[143229.642857143,143229.642857143,143229.642857143,127413.571428571,98547.5,123748.357142857,122026.285714286,149667.285714286,151860.214285714,186082.857142857,191725.714285714,201410.857142857,213771.714285714,237307.428571429,237307.428571429,237307.428571429,247541.857142857,247541.857142857,249677.571428571,249677.571428571,249677.571428571,249677.571428571,267599.571428571,267599.571428571,267599.571428571,267599.571428571,322543,330257.285714286,365578.714285714,365578.714285714,405971.571428571,405971.571428571,405971.571428571,419543,419543,433421.428571429,433421.428571429,452278.571428571,450767.857142857,450767.857142857,450767.857142857,450767.857142857,499645.714285714,502860,502860,510717.142857143,510717.142857143,510717.142857143,510717.142857143,519412.714285714],[143229.642857143,143229.642857143,143229.642857143,127413.571428571,98547.5,123748.357142857,122026.285714286,149667.285714286,151860.214285714,189654.285714286,191725.714285714,209484.857142857,237307.428571429,237307.428571429,247541.857142857,247541.857142857,249677.571428571,249677.571428571,249677.571428571,267599.571428571,267599.571428571,267599.571428571,267599.571428571,267599.571428571,267599.571428571,297659.714285714,322543,354328.714285714,365578.714285714,378828.714285714,405971.571428571,405971.571428571,419543,419543,433421.428571429,433421.428571429,452278.571428571,450767.857142857,450767.857142857,450767.857142857,450767.857142857,469303.571428571,502860,502860,510717.142857143,510717.142857143,510717.142857143,510717.142857143,519412.714285714,519412.714285714],[143229.642857143,143229.642857143,143229.642857143,127413.571428571,98547.5,123748.357142857,122026.285714286,149667.285714286,167003.071428571,189654.285714286,215821,222549.714285714,237307.428571429,247541.857142857,249677.571428571,249677.571428571,267599.571428571,267599.571428571,267599.571428571,267599.571428571,267599.571428571,273635.285714286,273635.285714286,273635.285714286,281724.571428571,322543,351435.857142857,365578.714285714,378828.714285714,392400.142857143,405971.571428571,419543,433421.428571429,433421.428571429,452278.571428571,450767.857142857,450767.857142857,450767.857142857,450767.857142857,450767.857142857,450767.857142857,469303.571428571,502860,502860,510717.142857143,510717.142857143,510717.142857143,519412.714285714,519412.714285714,519412.714285714],[135710.5,135710.5,143229.642857143,127413.571428571,98547.5,123748.357142857,128954.857142857,160881.571428571,167003.071428571,198583.5,230178.142857143,229631.714285714,249677.571428571,267599.571428571,267599.571428571,267599.571428571,273635.285714286,273635.285714286,273635.285714286,273635.285714286,273635.285714286,273635.285714286,273635.285714286,273635.285714286,318838.285714286,343721.571428571,365364.428571429,378828.714285714,392400.142857143,392400.142857143,419543,433421.428571429,433421.428571429,452278.571428571,450767.857142857,450767.857142857,450767.857142857,450767.857142857,450767.857142857,450767.857142857,470732.142857143,502860,502860,510717.142857143,510717.142857143,510717.142857143,519412.714285714,519412.714285714,519412.714285714,519412.714285714],[146869.285714286,134156.928571429,134156.928571429,128840.214285714,98547.5,136034.071428571,128954.857142857,157519.714285714,189562.642857143,203219.071428571,236681.571428571,247660.285714286,273635.285714286,273635.285714286,273635.285714286,273635.285714286,273635.285714286,273635.285714286,273635.285714286,273635.285714286,273635.285714286,278778.142857143,278778.142857143,313921,343721.571428571,351435.857142857,365364.428571429,392400.142857143,392400.142857143,392400.142857143,433421.428571429,433421.428571429,452278.571428571,450767.857142857,450767.857142857,450767.857142857,450767.857142857,450767.857142857,444089.285714286,453089.285714286,469303.571428571,502860,510717.142857143,510717.142857143,510717.142857143,519412.714285714,519412.714285714,519412.714285714,519412.714285714,519412.714285714],[136292.714285714,136292.714285714,136357,122279.071428571,91054.5,140403.357142857,140500,179166,199160.642857143,220540.5,247140.5,280028.142857143,278778.142857143,278778.142857143,278778.142857143,278778.142857143,278778.142857143,278778.142857143,278778.142857143,278778.142857143,278778.142857143,278778.142857143,295206.714285714,331052.571428571,361435.857142857,362864.428571429,392400.142857143,392400.142857143,400614.428571429,433421.428571429,452278.571428571,450767.857142857,450767.857142857,450767.857142857,450767.857142857,450767.857142857,444089.285714286,444089.285714286,444089.285714286,470732.142857143,502860,502860,510717.142857143,510717.142857143,510717.142857143,519412.714285714,519412.714285714,519412.714285714,519412.714285714,519412.714285714],[134238,134238,134238,122279.071428571,91054.5,140403.357142857,140500,179166,199160.642857143,220540.5,247140.5,280028.142857143,278778.142857143,278778.142857143,278778.142857143,278778.142857143,278778.142857143,278778.142857143,278778.142857143,278778.142857143,278778.142857143,278778.142857143,323499,349185.857142857,367507.285714286,371150.142857143,392400.142857143,400614.428571429,418421.428571429,452278.571428571,450767.857142857,450767.857142857,450767.857142857,450767.857142857,444089.285714286,444089.285714286,444089.285714286,444089.285714286,453089.285714286,480017.857142857,502860,510717.142857143,510717.142857143,510717.142857143,519412.714285714,519412.714285714,519412.714285714,519412.714285714,519412.714285714,519412.714285714],[134238,134238,134238,122279.071428571,91054.5,140403.357142857,140500,179166,199160.642857143,220540.5,247140.5,280028.142857143,278778.142857143,278778.142857143,278778.142857143,278778.142857143,278778.142857143,278778.142857143,278778.142857143,278778.142857143,278778.142857143,322463.285714286,345132.285714286,365471.571428571,367507.285714286,387221.571428571,400614.428571429,418421.428571429,426064.285714286,450767.857142857,450767.857142857,450767.857142857,444089.285714286,444089.285714286,444089.285714286,444089.285714286,444089.285714286,444089.285714286,473589.285714286,482517.857142857,502860,510717.142857143,510717.142857143,519412.714285714,519412.714285714,519412.714285714,519412.714285714,519412.714285714,519412.714285714,519412.714285714],[134238,134238,134238,122279.071428571,91054.5,140403.357142857,140500,179166,199160.642857143,220540.5,247140.5,280028.142857143,278778.142857143,278778.142857143,278778.142857143,278778.142857143,278778.142857143,278778.142857143,278778.142857143,278778.142857143,300242.428571429,345132.285714286,351703.714285714,364257.285714286,383293,399364.428571429,426064.285714286,426053.571428571,426053.571428571,450767.857142857,450767.857142857,444089.285714286,444089.285714286,444089.285714286,444089.285714286,444089.285714286,444089.285714286,463089.285714286,480017.857142857,482517.857142857,510717.142857143,510717.142857143,519412.714285714,519412.714285714,519412.714285714,519412.714285714,519412.714285714,519412.714285714,519412.714285714,523607],[134238,134238,134238,122279.071428571,91054.5,140403.357142857,140500,179166,199160.642857143,220540.5,247140.5,280028.142857143,278778.142857143,278778.142857143,278778.142857143,278778.142857143,278778.142857143,278778.142857143,278778.142857143,293171,324382.285714286,351703.714285714,352114.428571429,380650.142857143,384221.571428571,426064.285714286,426053.571428571,426053.571428571,426053.571428571,444089.285714286,444089.285714286,444089.285714286,444089.285714286,444089.285714286,444089.285714286,444089.285714286,444089.285714286,473589.285714286,480017.857142857,482517.857142857,510717.142857143,519412.714285714,519412.714285714,519412.714285714,519412.714285714,519412.714285714,519412.714285714,523607,523607,523607],[134238,134238,134238,122279.071428571,91054.5,140403.357142857,140500,179166,199160.642857143,220540.5,247140.5,280028.142857143,278778.142857143,278778.142857143,278778.142857143,278778.142857143,278778.142857143,278778.142857143,292742.428571429,294070.714285714,324427.857142857,353132.285714286,351660,383078.714285714,402600,426053.571428571,426053.571428571,420517.857142857,420517.857142857,444089.285714286,444089.285714286,444089.285714286,444089.285714286,444089.285714286,444089.285714286,444089.285714286,463089.285714286,483320.571428571,493677.714285714,510717.142857143,519412.714285714,519412.714285714,519412.714285714,519412.714285714,519412.714285714,519412.714285714,523607,523607,523607,523607],[134238,134238,134238,122279.071428571,91054.5,140403.357142857,140500,179166,199160.642857143,220540.5,247140.5,280028.142857143,278778.142857143,278778.142857143,278778.142857143,278778.142857143,278778.142857143,301385.285714286,313606.428571429,310356.428571429,330070.714285714,343249.285714286,366038.428571429,386634.857142857,396063.428571429,420517.857142857,420517.857142857,420517.857142857,420517.857142857,444089.285714286,444089.285714286,444089.285714286,444089.285714286,444089.285714286,444089.285714286,444089.285714286,484034.857142857,493677.714285714,493677.714285714,510717.142857143,519412.714285714,519412.714285714,519412.714285714,519412.714285714,519412.714285714,523607,523607,523607,523607,525678.428571429],[134238,134238,134238,122279.071428571,91054.5,140403.357142857,140500,179166,199160.642857143,220540.5,247140.5,280028.142857143,278778.142857143,278778.142857143,278778.142857143,278778.142857143,291312.285714286,345421.428571429,346109.857142857,334749.285714286,341499.285714286,368556.285714286,391277.714285714,381099.142857143,381099.142857143,420517.857142857,420517.857142857,420517.857142857,420517.857142857,444089.285714286,444089.285714286,444089.285714286,444089.285714286,444089.285714286,444089.285714286,463089.285714286,478639.857142857,493677.714285714,490820.571428571,519412.714285714,519412.714285714,519412.714285714,519412.714285714,519412.714285714,523607,523607,523607,525678.428571429,525678.428571429,542454.5],[134238,134238,134238,122279.071428571,91054.5,140403.357142857,140500,179166,199160.642857143,220540.5,247140.5,280028.142857143,278778.142857143,278778.142857143,278778.142857143,296873.714285714,325159.857142857,351419.357142857,360395.571428571,375559.714285714,406584.714285714,406584.714285714,388956.285714286,381099.142857143,381099.142857143,420517.857142857,420517.857142857,420517.857142857,420517.857142857,444089.285714286,444089.285714286,444089.285714286,444089.285714286,444089.285714286,463534.857142857,478639.857142857,481854.142857143,486854.142857143,484389.857142857,519412.714285714,519412.714285714,519412.714285714,519412.714285714,523607,523607,523607,525678.428571429,525678.428571429,542454.5,542454.5],[134238,134238,134238,122279.071428571,91054.5,140403.357142857,140500,179166,199160.642857143,220540.5,247140.5,280028.142857143,278778.142857143,267660.642857143,272035.857142857,301775.214285714,321159.857142857,390506,372263.285714286,385299,404549,404549,388956.285714286,381099.142857143,381099.142857143,420517.857142857,420517.857142857,420517.857142857,420517.857142857,444089.285714286,444089.285714286,444089.285714286,444089.285714286,444089.285714286,473854.142857143,487068.428571429,484925.571428571,486854.142857143,484389.857142857,519412.714285714,519412.714285714,519412.714285714,523607,523607,525678.428571429,525678.428571429,542454.5,542454.5,542454.5,549454.5],[134238,136113,138910.071428571,134747.642857143,147265.785714286,147265.785714286,168438.785714286,152717.785714286,178329.357142857,217150.571428571,205281.857142857,220200.928571429,234776.642857143,245089.428571429,267619.214285714,290537.428571429,339241.857142857,376745.285714286,372084.714285714,385299,404549,404549,388956.285714286,381099.142857143,381099.142857143,420517.857142857,420517.857142857,420517.857142857,420517.857142857,444089.285714286,444089.285714286,444089.285714286,444089.285714286,448408.571428571,473854.142857143,487068.428571429,484925.571428571,490318.428571429,484389.857142857,519412.714285714,523607,523607,523607,525678.428571429,525678.428571429,542454.5,542454.5,549454.5,549454.5,549454.5],[161831.857142857,160497.928571429,147265.785714286,147265.785714286,147265.785714286,147265.785714286,168438.785714286,152717.785714286,178329.357142857,217150.571428571,205281.857142857,220200.928571429,234776.642857143,245089.428571429,267619.214285714,290537.428571429,339241.857142857,376745.285714286,372084.714285714,385299,404549,404549,388956.285714286,381099.142857143,381099.142857143,420517.857142857,420517.857142857,420517.857142857,420517.857142857,444089.285714286,444089.285714286,444089.285714286,467972.5,473854.142857143,487068.428571429,499808.785714286,498058.785714286,490318.428571429,484389.857142857,523607,523607,525678.428571429,525678.428571429,542454.5,542454.5,549454.5,549454.5,549454.5,549454.5,549454.5],[147265.785714286,147265.785714286,147265.785714286,147265.785714286,147265.785714286,147265.785714286,168438.785714286,152717.785714286,178329.357142857,217150.571428571,205281.857142857,220200.928571429,234776.642857143,245089.428571429,267619.214285714,290537.428571429,339241.857142857,376745.285714286,372084.714285714,385299,404549,404549,388956.285714286,381099.142857143,381099.142857143,420517.857142857,420517.857142857,420517.857142857,420517.857142857,444089.285714286,444089.285714286,444089.285714286,485329.642857143,491951.642857143,499808.785714286,500415.928571429,498058.785714286,490318.428571429,484389.857142857,523607,525678.428571429,542454.5,542454.5,542454.5,549454.5,549454.5,549454.5,549454.5,549454.5,549454.5],[147265.785714286,147265.785714286,147265.785714286,147265.785714286,147265.785714286,147265.785714286,168438.785714286,152717.785714286,178329.357142857,217150.571428571,205281.857142857,220200.928571429,234776.642857143,245089.428571429,267619.214285714,290537.428571429,339241.857142857,376745.285714286,372084.714285714,385299,404549,404549,388956.285714286,381099.142857143,381099.142857143,420517.857142857,420517.857142857,420517.857142857,420517.857142857,444089.285714286,444089.285714286,474232.142857143,483863.214285714,492165.928571429,500415.928571429,500415.928571429,498058.785714286,490318.428571429,523607,525678.428571429,542454.5,542454.5,549454.5,549454.5,549454.5,549454.5,549454.5,549454.5,556311.642857143,556311.642857143],[147265.785714286,147265.785714286,147265.785714286,147265.785714286,147265.785714286,147265.785714286,168438.785714286,152717.785714286,178329.357142857,217150.571428571,205281.857142857,220200.928571429,234776.642857143,245089.428571429,267619.214285714,290537.428571429,339241.857142857,376745.285714286,372084.714285714,385299,404549,404549,388956.285714286,381099.142857143,381099.142857143,420517.857142857,420517.857142857,420517.857142857,420517.857142857,444089.285714286,471089.285714286,474946.428571429,480720.357142857,489415.928571429,500415.928571429,500415.928571429,514076.642857143,514076.642857143,542454.5,542454.5,549454.5,549454.5,549454.5,549454.5,549454.5,556311.642857143,556311.642857143,556311.642857143,556311.642857143,556311.642857143],[147265.785714286,147265.785714286,147265.785714286,147265.785714286,147265.785714286,147265.785714286,168438.785714286,152717.785714286,178329.357142857,217150.571428571,205281.857142857,220200.928571429,234776.642857143,245089.428571429,267619.214285714,290537.428571429,339241.857142857,376745.285714286,372084.714285714,385299,404549,404549,388956.285714286,381099.142857143,381099.142857143,420517.857142857,420517.857142857,420517.857142857,416767.857142857,460089.285714286,474946.428571429,487686.785714286,488970.357142857,489415.928571429,500415.928571429,508290.928571429,514076.642857143,514469.5,549454.5,549454.5,549454.5,549454.5,556311.642857143,556311.642857143,556311.642857143,556311.642857143,556311.642857143,556311.642857143,556311.642857143,556740.214285714],[147265.785714286,147265.785714286,147265.785714286,147265.785714286,147265.785714286,147265.785714286,168438.785714286,152717.785714286,178329.357142857,217150.571428571,203317.642857143,220200.928571429,234776.642857143,245089.428571429,267619.214285714,290537.428571429,339241.857142857,376745.285714286,372084.714285714,385299,404549,404549,388956.285714286,381099.142857143,381099.142857143,420517.857142857,420517.857142857,414217.857142857,424266.928571429,419981.214285714,473875,491151.071428571,488970.357142857,489415.928571429,508290.928571429,505148.071428571,514183.785714286,518112.357142857,549454.5,556311.642857143,556311.642857143,556311.642857143,556311.642857143,556311.642857143,556311.642857143,556311.642857143,556311.642857143,556311.642857143,556740.214285714,556740.214285714],[147265.785714286,147265.785714286,147265.785714286,147265.785714286,147265.785714286,147265.785714286,168438.785714286,152717.785714286,178329.357142857,217150.571428571,203317.642857143,220200.928571429,234776.642857143,245089.428571429,267619.214285714,290537.428571429,339241.857142857,376745.285714286,372084.714285714,385299,404549,404549,388956.285714286,381099.142857143,381099.142857143,420517.857142857,432017.857142857,425825,408038.357142857,418624.071428571,439052.642857143,491151.071428571,488970.357142857,508290.928571429,514540.928571429,519183.785714286,528826.642857143,518112.357142857,556311.642857143,556311.642857143,556311.642857143,556311.642857143,556311.642857143,556311.642857143,556311.642857143,556311.642857143,556311.642857143,556740.214285714,556740.214285714,556740.214285714],[147265.785714286,147265.785714286,147265.785714286,147265.785714286,147265.785714286,147265.785714286,168438.785714286,152717.785714286,178329.357142857,217150.571428571,205281.857142857,220200.928571429,234776.642857143,245089.428571429,267619.214285714,290537.428571429,339241.857142857,376745.285714286,372084.714285714,385299,404549,404549,388956.285714286,381099.142857143,382649.142857143,433591.142857143,435912.571428571,406324.071428571,406966.928571429,426874.071428571,439052.642857143,480702.5,502648.071428571,514540.928571429,519183.785714286,519183.785714286,534183.785714286,535969.5,556311.642857143,556311.642857143,556311.642857143,556311.642857143,556311.642857143,556311.642857143,556311.642857143,556311.642857143,551097.357142857,551097.357142857,560133.071428571,556507.785714286],[147265.785714286,147265.785714286,147265.785714286,147265.785714286,147265.785714286,147265.785714286,168438.785714286,152717.785714286,178329.357142857,217150.571428571,205281.857142857,220200.928571429,234776.642857143,245089.428571429,267619.214285714,290537.428571429,339241.857142857,376745.285714286,372084.714285714,385299,404549,369844.571428571,401472.571428571,377935.214285714,361646,384314.357142857,384928.642857143,400002.642857143,408859.785714286,421591.928571429,455168,477182.857142857,477182.857142857,520255.214285714,528112.357142857,534183.785714286,534183.785714286,535969.5,556311.642857143,556311.642857143,561668.785714286,561668.785714286,561668.785714286,561668.785714286,561668.785714286,551097.357142857,551097.357142857,557168.785714286,556507.785714286,552257.785714286],[144426.5,144426.5,141355.071428571,144783.642857143,136901.214285714,134300.142857143,146832.5,145618.214285714,148546.785714286,169468.714285714,190590.642857143,190609.571428571,214454.571428571,279296.142857143,244826.428571429,270470.071428571,272527.5,268108.714285714,301210.142857143,318950.142857143,333617.5,352136.714285714,334455.142857143,359405.714285714,337356.714285714,332399.571428571,341259.857142857,361424.142857143,401372.285714286,452725.142857143,447651.571428571,473432.857142857,495040,495040,528112.357142857,534183.785714286,528826.642857143,557204.5,557204.5,557204.5,557204.5,557204.5,557204.5,557204.5,559133.071428571,557168.785714286,557168.785714286,552257.785714286,552257.785714286,552257.785714286],[119533.428571429,134137.214285714,134137.214285714,131851.5,145618.214285714,145618.214285714,145618.214285714,145618.214285714,148546.785714286,169468.714285714,190590.642857143,190609.571428571,214454.571428571,279296.142857143,244826.428571429,270470.071428571,272527.5,268108.714285714,301210.142857143,318950.142857143,333617.5,352136.714285714,334455.142857143,359405.714285714,337356.714285714,332399.571428571,341259.857142857,361424.142857143,410772.357142857,443546.571428571,453454.285714286,453454.285714286,477611.428571429,477611.428571429,476825.714285714,524076.642857143,527933.785714286,557204.5,557204.5,557204.5,557204.5,557204.5,557204.5,559133.071428571,557168.785714286,556507.785714286,552257.785714286,552257.785714286,552257.785714286,538213.857142857],[131851.5,139422.928571429,145618.214285714,145618.214285714,145618.214285714,145618.214285714,145618.214285714,145618.214285714,148546.785714286,169468.714285714,190590.642857143,190609.571428571,214454.571428571,279296.142857143,244826.428571429,270470.071428571,272527.5,268108.714285714,301210.142857143,318950.142857143,333617.5,352136.714285714,334455.142857143,359405.714285714,337356.714285714,332399.571428571,341259.857142857,361424.142857143,401372.285714286,443546.571428571,453454.285714286,453454.285714286,477611.428571429,477611.428571429,476825.714285714,495575.714285714,527933.785714286,557204.5,557204.5,557204.5,557204.5,557204.5,559133.071428571,557168.785714286,556507.785714286,552257.785714286,552257.785714286,538213.857142857,538213.857142857,538213.857142857],[145618.214285714,145618.214285714,145618.214285714,145618.214285714,145618.214285714,145618.214285714,145618.214285714,145618.214285714,148546.785714286,169468.714285714,190590.642857143,190609.571428571,214454.571428571,279296.142857143,244826.428571429,270470.071428571,272527.5,268108.714285714,301210.142857143,318950.142857143,333617.5,352136.714285714,334455.142857143,359405.714285714,337356.714285714,332399.571428571,341259.857142857,361424.142857143,397445.571428571,443546.571428571,453454.285714286,453454.285714286,477611.428571429,477611.428571429,476825.714285714,495575.714285714,507648.071428571,557204.5,557204.5,557204.5,557204.5,557204.5,557168.785714286,552257.785714286,552257.785714286,538213.857142857,538213.857142857,538213.857142857,549546.642857143,549546.642857143],[145618.214285714,145618.214285714,145618.214285714,145618.214285714,145618.214285714,145618.214285714,145618.214285714,145618.214285714,148546.785714286,169468.714285714,190590.642857143,190609.571428571,214454.571428571,279296.142857143,244826.428571429,270470.071428571,272527.5,268108.714285714,301210.142857143,318950.142857143,333617.5,352136.714285714,334455.142857143,359405.714285714,337356.714285714,332399.571428571,341259.857142857,361424.142857143,397445.571428571,443546.571428571,453454.285714286,453454.285714286,477611.428571429,477611.428571429,476825.714285714,495575.714285714,507648.071428571,542561.642857143,557204.5,557204.5,557204.5,557168.785714286,552257.785714286,552257.785714286,538213.857142857,538213.857142857,549546.642857143,549546.642857143,549546.642857143,549546.642857143],[145618.214285714,145618.214285714,145618.214285714,145618.214285714,145618.214285714,145618.214285714,145618.214285714,145618.214285714,148546.785714286,169468.714285714,190590.642857143,190609.571428571,214454.571428571,279296.142857143,244826.428571429,270470.071428571,272527.5,268108.714285714,301210.142857143,318950.142857143,333617.5,352136.714285714,334455.142857143,359405.714285714,337356.714285714,332399.571428571,341259.857142857,361424.142857143,397445.571428571,443546.571428571,453454.285714286,453454.285714286,477611.428571429,477611.428571429,476825.714285714,495575.714285714,507648.071428571,542561.642857143,542329.5,549525.928571429,538472.071428571,537382.785714286,537382.785714286,539046.642857143,539046.642857143,549546.642857143,549546.642857143,562734,562734,562734],[145618.214285714,145618.214285714,145618.214285714,145618.214285714,145618.214285714,145618.214285714,145618.214285714,145618.214285714,148546.785714286,169468.714285714,190590.642857143,190609.571428571,214454.571428571,279296.142857143,244826.428571429,270470.071428571,272527.5,268108.714285714,301210.142857143,318950.142857143,333617.5,352136.714285714,334455.142857143,359405.714285714,337356.714285714,332399.571428571,341259.857142857,361424.142857143,397445.571428571,443546.571428571,453454.285714286,453454.285714286,477611.428571429,477611.428571429,476825.714285714,495575.714285714,507648.071428571,558037.285714286,542329.5,540311.357142857,537382.785714286,539046.642857143,542339,542339,542339,542339,542339,568448.285714286,568448.285714286,568448.285714286],[145618.214285714,145618.214285714,145618.214285714,145618.214285714,145618.214285714,145618.214285714,145618.214285714,145618.214285714,148546.785714286,169468.714285714,190590.642857143,190609.571428571,214454.571428571,279296.142857143,244826.428571429,270470.071428571,272527.5,268108.714285714,301210.142857143,318950.142857143,333617.5,352136.714285714,334455.142857143,359405.714285714,337356.714285714,332399.571428571,341259.857142857,361424.142857143,397445.571428571,443546.571428571,453454.285714286,453454.285714286,477611.428571429,477611.428571429,476825.714285714,506219.5,558344.642857143,539541.071428571,554501.285714286,554501.285714286,542339,542339,542339,568448.285714286,568448.285714286,568448.285714286,568448.285714286,568448.285714286,568448.285714286,568448.285714286],[145618.214285714,145618.214285714,145618.214285714,145618.214285714,145618.214285714,145618.214285714,145618.214285714,145618.214285714,148546.785714286,169468.714285714,190590.642857143,190609.571428571,214454.571428571,279296.142857143,244826.428571429,270470.071428571,272527.5,268108.714285714,301210.142857143,318950.142857143,333617.5,352136.714285714,334455.142857143,359405.714285714,337356.714285714,332399.571428571,341259.857142857,361424.142857143,401372.285714286,443546.571428571,453454.285714286,453454.285714286,477611.428571429,479504.285714286,499983.785714286,533198.071428571,559416.071428571,554808.642857143,564339,568448.285714286,568448.285714286,568448.285714286,568448.285714286,568448.285714286,568448.285714286,568448.285714286,568448.285714286,555859.857142857,555859.857142857,555859.857142857],[145618.214285714,145618.214285714,145618.214285714,145618.214285714,145618.214285714,145618.214285714,145618.214285714,145618.214285714,148546.785714286,169468.714285714,190590.642857143,190609.571428571,214454.571428571,279296.142857143,244826.428571429,270470.071428571,272527.5,268108.714285714,301210.142857143,318950.142857143,333617.5,352136.714285714,334455.142857143,359405.714285714,337356.714285714,353913.857142857,349367,386088.428571429,400449.071428571,396949.071428571,442648.357142857,451582.857142857,471869.5,483555.214285714,510269.5,535466.071428571,521815,546441.857142857,546441.857142857,555859.857142857,555859.857142857,555859.857142857,555859.857142857,555859.857142857,555859.857142857,555859.857142857,555859.857142857,555859.857142857,555859.857142857,584431.285714286],[145618.214285714,145618.214285714,145618.214285714,145618.214285714,145618.214285714,145618.214285714,145618.214285714,145618.214285714,148546.785714286,169468.714285714,190590.642857143,193238.142857143,210168.857142857,267081.857142857,232097.142857143,259585.642857143,263317.357142857,267053.571428571,294663.357142857,320003.714285714,319007.571428571,356585.642857143,350675.928571429,355727.5,387296.785714286,406312.071428571,389851.285714286,395444.642857143,396666.071428571,395737.5,393166.071428571,377308.928571429,412573.214285714,436430.357142857,491127.857142857,487960.571428571,502424.857142857,514924.857142857,518282,535424.857142857,555859.857142857,555859.857142857,555859.857142857,555859.857142857,555859.857142857,555859.857142857,555859.857142857,555859.857142857,584431.285714286,584431.285714286],[145618.214285714,145618.214285714,145618.214285714,145618.214285714,145618.214285714,145618.214285714,145618.214285714,145618.214285714,149832.5,171040.142857143,184733.5,193238.142857143,210168.857142857,264443.142857143,225887,253242.642857143,263439.285714286,285090.214285714,301697.357142857,328875.928571429,340870.571428571,351804.5,351804.5,364228.928571429,390103.928571429,386961.071428571,403279.857142857,395444.642857143,396666.071428571,395737.5,388166.071428571,377308.928571429,412573.214285714,436430.357142857,481521.571428571,487960.571428571,502424.857142857,502424.857142857,504353.428571429,504353.428571429,524710.571428571,556574.142857143,556574.142857143,556574.142857143,556574.142857143,556574.142857143,556574.142857143,572288.428571429,572288.428571429,572288.428571429],[145618.214285714,145618.214285714,145618.214285714,145618.214285714,145618.214285714,145618.214285714,148207.5,148207.5,149832.5,171040.142857143,184733.5,193238.142857143,210315.857142857,271711,243824.5,267076.642857143,289201.642857143,287447.357142857,321661.642857143,332018.785714286,351804.5,351804.5,351804.5,364228.928571429,390103.928571429,386961.071428571,403279.857142857,395444.642857143,396666.071428571,395737.5,388166.071428571,377308.928571429,412573.214285714,436430.357142857,481521.571428571,487960.571428571,502424.857142857,502424.857142857,504353.428571429,504353.428571429,504353.428571429,514710.571428571,555052.714285714,556574.142857143,556574.142857143,556574.142857143,556574.142857143,572288.428571429,572288.428571429,572288.428571429],[145618.214285714,145618.214285714,145618.214285714,145618.214285714,148207.5,148207.5,148207.5,148207.5,149832.5,171040.142857143,184733.5,197653,203369.428571429,275551.714285714,253110.214285714,301728.142857143,287390.214285714,323018.785714286,332018.785714286,332018.785714286,351804.5,351804.5,351804.5,364228.928571429,390103.928571429,386961.071428571,403279.857142857,395444.642857143,396666.071428571,395737.5,388166.071428571,377308.928571429,412573.214285714,436430.357142857,481521.571428571,487960.571428571,502424.857142857,502424.857142857,504353.428571429,504353.428571429,504353.428571429,504353.428571429,511139.142857143,555052.714285714,555052.714285714,556574.142857143,572288.428571429,572288.428571429,572288.428571429,572288.428571429],[145618.214285714,145618.214285714,148207.5,148207.5,148207.5,148207.5,148207.5,148207.5,149832.5,171040.142857143,191255.5,203523.071428571,215703.214285714,321301.714285714,292800.785714286,304621,312304.5,332018.785714286,332018.785714286,332018.785714286,351804.5,351804.5,351804.5,364228.928571429,390103.928571429,386961.071428571,403279.857142857,395444.642857143,396666.071428571,395737.5,388166.071428571,377308.928571429,412573.214285714,436430.357142857,481521.571428571,487960.571428571,502424.857142857,502424.857142857,504353.428571429,504353.428571429,504353.428571429,504353.428571429,504353.428571429,511139.142857143,555052.714285714,583002.714285714,583002.714285714,572288.428571429,572288.428571429,572288.428571429],[148207.5,148207.5,148207.5,148207.5,148207.5,148207.5,148207.5,148207.5,149832.5,178003.357142857,199523.357142857,221418.571428571,227488.928571429,328774.357142857,298015.071428571,328213.857142857,332018.785714286,332018.785714286,332018.785714286,332018.785714286,351804.5,351804.5,351804.5,364228.928571429,390103.928571429,386961.071428571,403279.857142857,395444.642857143,396666.071428571,395737.5,388166.071428571,377308.928571429,412573.214285714,436430.357142857,481521.571428571,487960.571428571,502424.857142857,502424.857142857,504353.428571429,504353.428571429,504353.428571429,504353.428571429,504353.428571429,504353.428571429,565052.714285714,574338.428571429,574338.428571429,583002.714285714,583002.714285714,572288.428571429],[148207.5,148207.5,148207.5,148207.5,148207.5,148207.5,148207.5,148207.5,156354.5,181325.071428571,218776,225704.285714286,271161.642857143,290488.642857143,316304.5,311940.214285714,332018.785714286,332018.785714286,332018.785714286,332018.785714286,351804.5,351804.5,351804.5,364228.928571429,390103.928571429,386961.071428571,403279.857142857,395444.642857143,396666.071428571,395737.5,388166.071428571,377308.928571429,412573.214285714,436430.357142857,481521.571428571,487960.571428571,502424.857142857,502424.857142857,504353.428571429,504353.428571429,504353.428571429,504353.428571429,504353.428571429,504353.428571429,538282,565052.714285714,574338.428571429,574338.428571429,574338.428571429,583002.714285714],[148207.5,148207.5,148207.5,148207.5,148207.5,148207.5,148207.5,156640.214285714,167265.214285714,194810.785714286,218776,275091.285714286,281018.785714286,308052.928571429,311940.214285714,311940.214285714,311940.214285714,311940.214285714,311940.214285714,332018.785714286,351804.5,351804.5,351804.5,364228.928571429,390103.928571429,386961.071428571,403279.857142857,395444.642857143,396666.071428571,395737.5,388166.071428571,377308.928571429,412573.214285714,436430.357142857,481521.571428571,487960.571428571,502424.857142857,502424.857142857,504353.428571429,504353.428571429,504353.428571429,504353.428571429,504353.428571429,538282,538282,538282,565052.714285714,574338.428571429,574338.428571429,574338.428571429],[148207.5,148207.5,148207.5,148207.5,148207.5,148207.5,156640.214285714,160601.428571429,179693.785714286,201992,275907.785714286,278755.571428571,296368.785714286,311940.214285714,311940.214285714,311940.214285714,311940.214285714,311940.214285714,311940.214285714,311940.214285714,331440.214285714,331440.214285714,351804.5,364228.928571429,390103.928571429,386961.071428571,403279.857142857,395444.642857143,396666.071428571,395737.5,388166.071428571,377308.928571429,412573.214285714,436430.357142857,481521.571428571,487960.571428571,502424.857142857,502424.857142857,504353.428571429,504353.428571429,504353.428571429,504353.428571429,538282,538282,538282,538282,538282,565052.714285714,574338.428571429,574338.428571429],[148207.5,148207.5,148207.5,148207.5,148207.5,156640.214285714,156640.214285714,163744.285714286,201425.5,249163.428571429,271972.071428571,278755.571428571,311940.214285714,311940.214285714,311940.214285714,311940.214285714,311940.214285714,311940.214285714,311940.214285714,311940.214285714,331440.214285714,331440.214285714,331440.214285714,347293.214285714,381596.785714286,375844.142857143,403279.857142857,395444.642857143,396666.071428571,395737.5,388166.071428571,377308.928571429,412573.214285714,436430.357142857,481521.571428571,487960.571428571,502424.857142857,502424.857142857,504353.428571429,504353.428571429,504353.428571429,504353.428571429,538282,538282,538282,538282,538282,538282,565052.714285714,574338.428571429],[148207.5,148207.5,148207.5,148207.5,156640.214285714,156640.214285714,163744.285714286,189925.5,209489.785714286,266318.285714286,272615.214285714,311940.214285714,311940.214285714,311940.214285714,311940.214285714,311940.214285714,311940.214285714,311940.214285714,311940.214285714,311940.214285714,331440.214285714,331440.214285714,331440.214285714,347293.214285714,381596.785714286,375844.142857143,364772.714285714,385223.214285714,396666.071428571,395737.5,388166.071428571,377308.928571429,412573.214285714,436430.357142857,481521.571428571,487960.571428571,502424.857142857,502424.857142857,504353.428571429,504353.428571429,504353.428571429,538282,538282,538282,538282,538282,538282,538282,538282,565052.714285714],[148207.5,148207.5,148207.5,156640.214285714,156640.214285714,163744.285714286,177815.714285714,183687.357142857,266956.928571429,271175.428571429,296100.928571429,311940.214285714,311940.214285714,311940.214285714,311940.214285714,311940.214285714,311940.214285714,311940.214285714,311940.214285714,311940.214285714,331440.214285714,331440.214285714,331440.214285714,347293.214285714,381596.785714286,375844.142857143,364772.714285714,385223.214285714,379866.071428571,380730.357142857,388166.071428571,377308.928571429,412573.214285714,436430.357142857,481521.571428571,487960.571428571,502424.857142857,502424.857142857,504353.428571429,504353.428571429,504353.428571429,538282,538282,538282,538282,538282,538282,538282,538282,538282],[148207.5,148207.5,156640.214285714,156640.214285714,163744.285714286,166380,183687.357142857,242401.642857143,266956.928571429,271175.428571429,311940.214285714,311940.214285714,311940.214285714,311940.214285714,311940.214285714,311940.214285714,311940.214285714,311940.214285714,311940.214285714,311940.214285714,331440.214285714,331440.214285714,331440.214285714,347293.214285714,381596.785714286,375844.142857143,364772.714285714,385223.214285714,379866.071428571,380730.357142857,374230.357142857,377308.928571429,412573.214285714,436430.357142857,481521.571428571,487960.571428571,502424.857142857,502424.857142857,504353.428571429,504353.428571429,538282,538282,538282,538282,538282,538282,538282,538282,538282,538282],[148207.5,156640.214285714,156640.214285714,160601.428571429,166380,183687.357142857,183687.357142857,255654.5,279814.071428571,292083.071428571,311940.214285714,311940.214285714,311940.214285714,311940.214285714,311940.214285714,311940.214285714,311940.214285714,311940.214285714,311940.214285714,311940.214285714,331440.214285714,331440.214285714,331440.214285714,347293.214285714,381596.785714286,375844.142857143,364772.714285714,385223.214285714,379866.071428571,380730.357142857,374230.357142857,365873.214285714,412573.214285714,436430.357142857,481521.571428571,487960.571428571,502424.857142857,502424.857142857,504353.428571429,538282,538282,538282,538282,538282,538282,538282,538282,538282,538282,538282],[156640.214285714,156640.214285714,155237.142857143,166380,178808.571428571,183687.357142857,245874.642857143,255654.5,279814.071428571,311940.214285714,311940.214285714,311940.214285714,311940.214285714,311940.214285714,311940.214285714,311940.214285714,311940.214285714,311940.214285714,311940.214285714,311940.214285714,331440.214285714,331440.214285714,331440.214285714,347293.214285714,381596.785714286,375844.142857143,364772.714285714,385223.214285714,379866.071428571,380730.357142857,374230.357142857,365873.214285714,385351.785714286,436430.357142857,481521.571428571,487960.571428571,502424.857142857,502424.857142857,504353.428571429,538282,538282,538282,538282,538282,538282,538282,538282,538282,538282,538282]],"type":"surface","x":[484,573.673469387755,663.34693877551,753.020408163265,842.69387755102,932.367346938776,1022.04081632653,1111.71428571429,1201.38775510204,1291.0612244898,1380.73469387755,1470.40816326531,1560.08163265306,1649.75510204082,1739.42857142857,1829.10204081633,1918.77551020408,2008.44897959184,2098.12244897959,2187.79591836735,2277.4693877551,2367.14285714286,2456.81632653061,2546.48979591837,2636.16326530612,2725.83673469388,2815.51020408163,2905.18367346939,2994.85714285714,3084.5306122449,3174.20408163265,3263.87755102041,3353.55102040816,3443.22448979592,3532.89795918367,3622.57142857143,3712.24489795918,3801.91836734694,3891.59183673469,3981.26530612245,4070.9387755102,4160.61224489796,4250.28571428571,4339.95918367347,4429.63265306122,4519.30612244898,4608.97959183673,4698.65306122449,4788.32653061224,4878],"y":[1,1.10204081632653,1.20408163265306,1.30612244897959,1.40816326530612,1.51020408163265,1.61224489795918,1.71428571428571,1.81632653061224,1.91836734693878,2.02040816326531,2.12244897959184,2.22448979591837,2.3265306122449,2.42857142857143,2.53061224489796,2.63265306122449,2.73469387755102,2.83673469387755,2.93877551020408,3.04081632653061,3.14285714285714,3.24489795918367,3.3469387755102,3.44897959183673,3.55102040816327,3.6530612244898,3.75510204081633,3.85714285714286,3.95918367346939,4.06122448979592,4.16326530612245,4.26530612244898,4.36734693877551,4.46938775510204,4.57142857142857,4.6734693877551,4.77551020408163,4.87755102040816,4.97959183673469,5.08163265306122,5.18367346938776,5.28571428571429,5.38775510204082,5.48979591836735,5.59183673469388,5.69387755102041,5.79591836734694,5.89795918367347,6],"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script></p>
<p>We can see that the predictions in this case, where we have 2 predictors, form
a surface instead of a line. Because the newly added predictor, number of
bedrooms, is correlated with price (USD) (meaning as price changes, so does
number of bedrooms) and not totally determined by house size (our other predictor),
we get additional and useful information for making our
predictions. For example, in this model we would predict that the cost of a
house with a size of 2,500 square feet generally increases slightly as the number
of bedrooms increases. Without having the additional predictor of number of
bedrooms, we would predict the same price for these two houses.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="classification-continued.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="regression2.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/07-regression1.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
