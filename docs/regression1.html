<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Introduction to regression through K-nearest neighbours | Introduction to Data Science</title>
  <meta name="description" content="This is an open source textbook for teaching introductory data science." />
  <meta name="generator" content="bookdown 0.13 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Introduction to regression through K-nearest neighbours | Introduction to Data Science" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is an open source textbook for teaching introductory data science." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Introduction to regression through K-nearest neighbours | Introduction to Data Science" />
  
  <meta name="twitter:description" content="This is an open source textbook for teaching introductory data science." />
  

<meta name="author" content="Tiffany-Anne Timbers" />
<meta name="author" content="Melissa Lee" />
<meta name="author" content="Trevor Campbell" />


<meta name="date" content="2019-10-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="classification-continued.html"/>
<link rel="next" href="regression2.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.3/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.9.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.46.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.46.1/plotly-latest.min.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Data Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction to Data Science</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#chapter-learning-objectives"><i class="fa fa-check"></i><b>1.1</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#jupyter-notebooks"><i class="fa fa-check"></i><b>1.2</b> Jupyter notebooks</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#loading-a-spreadsheet-like-dataset"><i class="fa fa-check"></i><b>1.3</b> Loading a spreadsheet-like dataset</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#assigning-value-to-a-data-frame"><i class="fa fa-check"></i><b>1.4</b> Assigning value to a data frame</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#creating-subsets-of-data-frames-with-select-filter"><i class="fa fa-check"></i><b>1.5</b> Creating subsets of data frames with <code>select</code> &amp; <code>filter</code></a><ul>
<li class="chapter" data-level="1.5.1" data-path="index.html"><a href="index.html#using-select-to-extract-multiple-columns"><i class="fa fa-check"></i><b>1.5.1</b> Using <code>select</code> to extract multiple columns</a></li>
<li class="chapter" data-level="1.5.2" data-path="index.html"><a href="index.html#using-select-to-extract-a-range-of-columns"><i class="fa fa-check"></i><b>1.5.2</b> Using <code>select</code> to extract a range of columns</a></li>
<li class="chapter" data-level="1.5.3" data-path="index.html"><a href="index.html#using-filter-to-extract-a-single-row"><i class="fa fa-check"></i><b>1.5.3</b> Using <code>filter</code> to extract a single row</a></li>
<li class="chapter" data-level="1.5.4" data-path="index.html"><a href="index.html#using-filter-to-extract-rows-with-values-above-a-threshold"><i class="fa fa-check"></i><b>1.5.4</b> Using <code>filter</code> to extract rows with values above a threshold</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#exploring-data-with-visualizations"><i class="fa fa-check"></i><b>1.6</b> Exploring data with visualizations</a><ul>
<li class="chapter" data-level="1.6.1" data-path="index.html"><a href="index.html#using-ggplot-to-create-a-scatter-plot"><i class="fa fa-check"></i><b>1.6.1</b> Using <code>ggplot</code> to create a scatter plot</a></li>
<li class="chapter" data-level="1.6.2" data-path="index.html"><a href="index.html#using-ggplot-to-create-a-scatter-plot-1"><i class="fa fa-check"></i><b>1.6.2</b> Using <code>ggplot</code> to create a scatter plot</a></li>
<li class="chapter" data-level="1.6.3" data-path="index.html"><a href="index.html#formatting-ggplot-objects"><i class="fa fa-check"></i><b>1.6.3</b> Formatting ggplot objects</a></li>
<li class="chapter" data-level="1.6.4" data-path="index.html"><a href="index.html#coloring-points-by-group"><i class="fa fa-check"></i><b>1.6.4</b> Coloring points by group</a></li>
<li class="chapter" data-level="1.6.5" data-path="index.html"><a href="index.html#putting-it-all-together"><i class="fa fa-check"></i><b>1.6.5</b> Putting it all together</a></li>
<li class="chapter" data-level="1.6.6" data-path="index.html"><a href="index.html#whats-next"><i class="fa fa-check"></i><b>1.6.6</b> What’s next?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="reading.html"><a href="reading.html"><i class="fa fa-check"></i><b>2</b> Reading in data locally and from the web</a><ul>
<li class="chapter" data-level="2.1" data-path="reading.html"><a href="reading.html#overview"><i class="fa fa-check"></i><b>2.1</b> Overview</a></li>
<li class="chapter" data-level="2.2" data-path="reading.html"><a href="reading.html#chapter-learning-objectives-1"><i class="fa fa-check"></i><b>2.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="2.3" data-path="reading.html"><a href="reading.html#absolute-and-relative-file-paths"><i class="fa fa-check"></i><b>2.3</b> Absolute and relative file paths</a></li>
<li class="chapter" data-level="2.4" data-path="reading.html"><a href="reading.html#reading-tabular-data-from-a-plain-text-file-into-r"><i class="fa fa-check"></i><b>2.4</b> Reading tabular data from a plain text file into R</a><ul>
<li class="chapter" data-level="2.4.1" data-path="reading.html"><a href="reading.html#skipping-rows-when-reading-in-data"><i class="fa fa-check"></i><b>2.4.1</b> Skipping rows when reading in data</a></li>
<li class="chapter" data-level="2.4.2" data-path="reading.html"><a href="reading.html#read_delim-as-a-more-flexible-method-to-get-tabular-data-into-r"><i class="fa fa-check"></i><b>2.4.2</b> <code>read_delim</code> as a more flexible method to get tabular data into R</a></li>
<li class="chapter" data-level="2.4.3" data-path="reading.html"><a href="reading.html#reading-tabular-data-directly-from-a-url"><i class="fa fa-check"></i><b>2.4.3</b> Reading tabular data directly from a URL</a></li>
<li class="chapter" data-level="2.4.4" data-path="reading.html"><a href="reading.html#previewing-a-data-file-before-reading-it-into-r"><i class="fa fa-check"></i><b>2.4.4</b> Previewing a data file before reading it into R</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="reading.html"><a href="reading.html#reading-data-from-an-microsoft-excel-file"><i class="fa fa-check"></i><b>2.5</b> Reading data from an Microsoft Excel file</a></li>
<li class="chapter" data-level="2.6" data-path="reading.html"><a href="reading.html#reading-data-from-a-database"><i class="fa fa-check"></i><b>2.6</b> Reading data from a database</a><ul>
<li class="chapter" data-level="2.6.1" data-path="reading.html"><a href="reading.html#reading-data-from-a-sqlite-database"><i class="fa fa-check"></i><b>2.6.1</b> Reading data from a SQLite database</a></li>
<li class="chapter" data-level="2.6.2" data-path="reading.html"><a href="reading.html#reading-data-from-a-postgresql-database"><i class="fa fa-check"></i><b>2.6.2</b> Reading data from a PostgreSQL database</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="reading.html"><a href="reading.html#writing-data-from-r-to-a-.csv-file"><i class="fa fa-check"></i><b>2.7</b> Writing data from R to a <code>.csv</code> file</a></li>
<li class="chapter" data-level="2.8" data-path="reading.html"><a href="reading.html#scraping-data-off-the-web-using-r"><i class="fa fa-check"></i><b>2.8</b> Scraping data off the web using R</a><ul>
<li class="chapter" data-level="2.8.1" data-path="reading.html"><a href="reading.html#html-and-css-selectors"><i class="fa fa-check"></i><b>2.8.1</b> HTML and CSS selectors</a></li>
<li class="chapter" data-level="2.8.2" data-path="reading.html"><a href="reading.html#are-you-allowed-to-scrape-that-website"><i class="fa fa-check"></i><b>2.8.2</b> Are you allowed to scrape that website?</a></li>
<li class="chapter" data-level="2.8.3" data-path="reading.html"><a href="reading.html#using-rvest"><i class="fa fa-check"></i><b>2.8.3</b> Using <code>rvest</code></a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="reading.html"><a href="reading.html#additional-readingsresources"><i class="fa fa-check"></i><b>2.9</b> Additional readings/resources</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="wrangling.html"><a href="wrangling.html"><i class="fa fa-check"></i><b>3</b> Cleaning and wrangling data</a><ul>
<li class="chapter" data-level="3.1" data-path="wrangling.html"><a href="wrangling.html#overview-1"><i class="fa fa-check"></i><b>3.1</b> Overview</a></li>
<li class="chapter" data-level="3.2" data-path="wrangling.html"><a href="wrangling.html#chapter-learning-objectives-2"><i class="fa fa-check"></i><b>3.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="3.3" data-path="wrangling.html"><a href="wrangling.html#vectors-and-data-frames"><i class="fa fa-check"></i><b>3.3</b> Vectors and Data frames</a><ul>
<li class="chapter" data-level="3.3.1" data-path="wrangling.html"><a href="wrangling.html#what-is-a-data-frame"><i class="fa fa-check"></i><b>3.3.1</b> What is a data frame?</a></li>
<li class="chapter" data-level="3.3.2" data-path="wrangling.html"><a href="wrangling.html#what-is-a-vector"><i class="fa fa-check"></i><b>3.3.2</b> What is a vector?</a></li>
<li class="chapter" data-level="3.3.3" data-path="wrangling.html"><a href="wrangling.html#how-are-vectors-different-from-a-list"><i class="fa fa-check"></i><b>3.3.3</b> How are vectors different from a list?</a></li>
<li class="chapter" data-level="3.3.4" data-path="wrangling.html"><a href="wrangling.html#what-does-this-have-to-do-with-data-frames"><i class="fa fa-check"></i><b>3.3.4</b> What does this have to do with data frames?</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="wrangling.html"><a href="wrangling.html#tidy-data"><i class="fa fa-check"></i><b>3.4</b> Tidy Data</a><ul>
<li class="chapter" data-level="3.4.1" data-path="wrangling.html"><a href="wrangling.html#what-is-tidy-data"><i class="fa fa-check"></i><b>3.4.1</b> What is tidy data?</a></li>
<li class="chapter" data-level="3.4.2" data-path="wrangling.html"><a href="wrangling.html#why-is-tidy-data-important-in-r"><i class="fa fa-check"></i><b>3.4.2</b> Why is tidy data important in R?</a></li>
<li class="chapter" data-level="3.4.3" data-path="wrangling.html"><a href="wrangling.html#going-from-wide-to-long-or-tidy-using-gather"><i class="fa fa-check"></i><b>3.4.3</b> Going from wide to long (or tidy!) using <code>gather</code></a></li>
<li class="chapter" data-level="3.4.4" data-path="wrangling.html"><a href="wrangling.html#using-separate-to-deal-with-multiple-delimiters"><i class="fa fa-check"></i><b>3.4.4</b> Using separate to deal with multiple delimiters</a></li>
<li class="chapter" data-level="3.4.5" data-path="wrangling.html"><a href="wrangling.html#notes-on-defining-tidy-data"><i class="fa fa-check"></i><b>3.4.5</b> Notes on defining tidy data</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="wrangling.html"><a href="wrangling.html#combining-functions-using-the-pipe-operator"><i class="fa fa-check"></i><b>3.5</b> Combining functions using the pipe operator, <code>%&gt;%</code>:</a><ul>
<li class="chapter" data-level="3.5.1" data-path="wrangling.html"><a href="wrangling.html#using-to-combine-filter-and-select"><i class="fa fa-check"></i><b>3.5.1</b> Using <code>%&gt;%</code> to combine <code>filter</code> and <code>select</code></a></li>
<li class="chapter" data-level="3.5.2" data-path="wrangling.html"><a href="wrangling.html#using-with-more-than-two-functions"><i class="fa fa-check"></i><b>3.5.2</b> Using <code>%&gt;%</code> with more than two functions</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="wrangling.html"><a href="wrangling.html#iterating-over-data-with-group_by-summarize"><i class="fa fa-check"></i><b>3.6</b> Iterating over data with <code>group_by</code> + <code>summarize</code></a><ul>
<li class="chapter" data-level="3.6.1" data-path="wrangling.html"><a href="wrangling.html#calculating-summary-statistics"><i class="fa fa-check"></i><b>3.6.1</b> Calculating summary statistics:</a></li>
<li class="chapter" data-level="3.6.2" data-path="wrangling.html"><a href="wrangling.html#calculating-group-summary-statistics"><i class="fa fa-check"></i><b>3.6.2</b> Calculating group summary statistics:</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="wrangling.html"><a href="wrangling.html#additional-reading-on-the-dplyr-functions"><i class="fa fa-check"></i><b>3.7</b> Additional reading on the <code>dplyr</code> functions</a></li>
<li class="chapter" data-level="3.8" data-path="wrangling.html"><a href="wrangling.html#using-purrrs-map-functions-to-iterate"><i class="fa fa-check"></i><b>3.8</b> Using <code>purrr</code>’s <code>map*</code> functions to iterate</a><ul>
<li class="chapter" data-level="3.8.1" data-path="wrangling.html"><a href="wrangling.html#a-bit-more-about-the-map_-functions"><i class="fa fa-check"></i><b>3.8.1</b> A bit more about the <code>map_*</code> functions</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="wrangling.html"><a href="wrangling.html#additional-readingsresources-1"><i class="fa fa-check"></i><b>3.9</b> Additional readings/resources</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="viz.html"><a href="viz.html"><i class="fa fa-check"></i><b>4</b> Effective data visualization</a><ul>
<li class="chapter" data-level="4.1" data-path="viz.html"><a href="viz.html#overview-2"><i class="fa fa-check"></i><b>4.1</b> Overview</a></li>
<li class="chapter" data-level="4.2" data-path="viz.html"><a href="viz.html#chapter-learning-objectives-3"><i class="fa fa-check"></i><b>4.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="4.3" data-path="viz.html"><a href="viz.html#choosing-the-visualization"><i class="fa fa-check"></i><b>4.3</b> Choosing the visualization</a></li>
<li class="chapter" data-level="4.4" data-path="viz.html"><a href="viz.html#refining-the-visualization"><i class="fa fa-check"></i><b>4.4</b> Refining the visualization</a></li>
<li class="chapter" data-level="4.5" data-path="viz.html"><a href="viz.html#creating-visualizations-with-ggplot2"><i class="fa fa-check"></i><b>4.5</b> Creating visualizations with <code>ggplot2</code></a><ul>
<li class="chapter" data-level="4.5.1" data-path="viz.html"><a href="viz.html#the-mauna-loa-co2-data-set"><i class="fa fa-check"></i><b>4.5.1</b> The Mauna Loa CO2 data set</a></li>
<li class="chapter" data-level="4.5.2" data-path="viz.html"><a href="viz.html#the-island-landmass-data-set"><i class="fa fa-check"></i><b>4.5.2</b> The island landmass data set</a></li>
<li class="chapter" data-level="4.5.3" data-path="viz.html"><a href="viz.html#the-old-faithful-eruption-waiting-time-data-set"><i class="fa fa-check"></i><b>4.5.3</b> The Old Faithful eruption / waiting time data set</a></li>
<li class="chapter" data-level="4.5.4" data-path="viz.html"><a href="viz.html#the-michelson-speed-of-light-data-set"><i class="fa fa-check"></i><b>4.5.4</b> The Michelson speed of light data set</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="viz.html"><a href="viz.html#explaining-the-visualization"><i class="fa fa-check"></i><b>4.6</b> Explaining the visualization</a></li>
<li class="chapter" data-level="4.7" data-path="viz.html"><a href="viz.html#saving-the-visualization"><i class="fa fa-check"></i><b>4.7</b> Saving the visualization</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="GitHub.html"><a href="GitHub.html"><i class="fa fa-check"></i><b>5</b> Version control with GitHub</a><ul>
<li class="chapter" data-level="5.1" data-path="GitHub.html"><a href="GitHub.html#overview-3"><i class="fa fa-check"></i><b>5.1</b> Overview</a></li>
<li class="chapter" data-level="5.2" data-path="GitHub.html"><a href="GitHub.html#videos-to-learn-about-version-control-with-github-and-git"><i class="fa fa-check"></i><b>5.2</b> Videos to learn about version control with GitHub and Git</a><ul>
<li class="chapter" data-level="5.2.1" data-path="GitHub.html"><a href="GitHub.html#creating-a-github-repository"><i class="fa fa-check"></i><b>5.2.1</b> Creating a GitHub repository</a></li>
<li class="chapter" data-level="5.2.2" data-path="GitHub.html"><a href="GitHub.html#exploring-a-github-repository"><i class="fa fa-check"></i><b>5.2.2</b> Exploring a GitHub repository</a></li>
<li class="chapter" data-level="5.2.3" data-path="GitHub.html"><a href="GitHub.html#directly-editing-files-on-github"><i class="fa fa-check"></i><b>5.2.3</b> Directly editing files on GitHub</a></li>
<li class="chapter" data-level="5.2.4" data-path="GitHub.html"><a href="GitHub.html#logging-changes-and-pushing-them-to-github"><i class="fa fa-check"></i><b>5.2.4</b> Logging changes and pushing them to GitHub</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="GitHub.html"><a href="GitHub.html#git-command-cheatsheet"><i class="fa fa-check"></i><b>5.3</b> Git command cheatsheet</a><ul>
<li class="chapter" data-level="5.3.1" data-path="GitHub.html"><a href="GitHub.html#getting-a-repository-from-github-onto-the-server-for-the-first-time"><i class="fa fa-check"></i><b>5.3.1</b> Getting a repository from GitHub onto the server for the first time</a></li>
<li class="chapter" data-level="5.3.2" data-path="GitHub.html"><a href="GitHub.html#logging-changes"><i class="fa fa-check"></i><b>5.3.2</b> Logging changes</a></li>
<li class="chapter" data-level="5.3.3" data-path="GitHub.html"><a href="GitHub.html#sending-your-changes-back-to-github"><i class="fa fa-check"></i><b>5.3.3</b> Sending your changes back to GitHub</a></li>
<li class="chapter" data-level="5.3.4" data-path="GitHub.html"><a href="GitHub.html#getting-changes"><i class="fa fa-check"></i><b>5.3.4</b> Getting changes</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="GitHub.html"><a href="GitHub.html#terminal-cheatsheet"><i class="fa fa-check"></i><b>5.4</b> Terminal cheatsheet</a><ul>
<li class="chapter" data-level="5.4.1" data-path="GitHub.html"><a href="GitHub.html#see-where-you-are"><i class="fa fa-check"></i><b>5.4.1</b> See where you are:</a></li>
<li class="chapter" data-level="5.4.2" data-path="GitHub.html"><a href="GitHub.html#see-what-is-inside-the-directory-where-you-are"><i class="fa fa-check"></i><b>5.4.2</b> See what is inside the directory where you are:</a></li>
<li class="chapter" data-level="5.4.3" data-path="GitHub.html"><a href="GitHub.html#move-to-a-different-directory"><i class="fa fa-check"></i><b>5.4.3</b> Move to a different directory</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>6</b> Classification I: Training &amp; predicting</a><ul>
<li class="chapter" data-level="6.1" data-path="classification.html"><a href="classification.html#overview-4"><i class="fa fa-check"></i><b>6.1</b> Overview</a></li>
<li class="chapter" data-level="6.2" data-path="classification.html"><a href="classification.html#chapter-learning-objectives-4"><i class="fa fa-check"></i><b>6.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="6.3" data-path="classification.html"><a href="classification.html#the-classification-problem"><i class="fa fa-check"></i><b>6.3</b> The classification problem</a></li>
<li class="chapter" data-level="6.4" data-path="classification.html"><a href="classification.html#exploring-a-labelled-data-set"><i class="fa fa-check"></i><b>6.4</b> Exploring a labelled data set</a></li>
<li class="chapter" data-level="6.5" data-path="classification.html"><a href="classification.html#classification-with-k-nearest-neighbours"><i class="fa fa-check"></i><b>6.5</b> Classification with K-nearest neighbours</a></li>
<li class="chapter" data-level="6.6" data-path="classification.html"><a href="classification.html#k-nearest-neighbours-in-r"><i class="fa fa-check"></i><b>6.6</b> K-nearest neighbours in R</a></li>
<li class="chapter" data-level="6.7" data-path="classification.html"><a href="classification.html#data-preprocessing"><i class="fa fa-check"></i><b>6.7</b> Data preprocessing</a><ul>
<li class="chapter" data-level="6.7.1" data-path="classification.html"><a href="classification.html#shifting-and-scaling"><i class="fa fa-check"></i><b>6.7.1</b> Shifting and scaling</a></li>
<li class="chapter" data-level="6.7.2" data-path="classification.html"><a href="classification.html#balancing"><i class="fa fa-check"></i><b>6.7.2</b> Balancing</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="classification-continued.html"><a href="classification-continued.html"><i class="fa fa-check"></i><b>7</b> Classification II: Evaluation &amp; tuning</a><ul>
<li class="chapter" data-level="7.1" data-path="classification-continued.html"><a href="classification-continued.html#overview-5"><i class="fa fa-check"></i><b>7.1</b> Overview</a></li>
<li class="chapter" data-level="7.2" data-path="classification-continued.html"><a href="classification-continued.html#chapter-learning-objectives-5"><i class="fa fa-check"></i><b>7.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="7.3" data-path="classification-continued.html"><a href="classification-continued.html#evaluating-accuracy"><i class="fa fa-check"></i><b>7.3</b> Evaluating accuracy</a></li>
<li class="chapter" data-level="7.4" data-path="classification-continued.html"><a href="classification-continued.html#tuning-the-classifier"><i class="fa fa-check"></i><b>7.4</b> Tuning the classifier</a><ul>
<li class="chapter" data-level="7.4.1" data-path="classification-continued.html"><a href="classification-continued.html#cross-validation"><i class="fa fa-check"></i><b>7.4.1</b> Cross-validation</a></li>
<li class="chapter" data-level="7.4.2" data-path="classification-continued.html"><a href="classification-continued.html#parameter-value-selection"><i class="fa fa-check"></i><b>7.4.2</b> Parameter value selection</a></li>
<li class="chapter" data-level="7.4.3" data-path="classification-continued.html"><a href="classification-continued.html#underoverfitting"><i class="fa fa-check"></i><b>7.4.3</b> Under/overfitting</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="classification-continued.html"><a href="classification-continued.html#splitting-data"><i class="fa fa-check"></i><b>7.5</b> Splitting data</a></li>
<li class="chapter" data-level="7.6" data-path="classification-continued.html"><a href="classification-continued.html#summary"><i class="fa fa-check"></i><b>7.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="regression1.html"><a href="regression1.html"><i class="fa fa-check"></i><b>8</b> Introduction to regression through K-nearest neighbours</a><ul>
<li class="chapter" data-level="8.1" data-path="regression1.html"><a href="regression1.html#overview-6"><i class="fa fa-check"></i><b>8.1</b> Overview</a></li>
<li class="chapter" data-level="8.2" data-path="regression1.html"><a href="regression1.html#chapter-learning-objectives-6"><i class="fa fa-check"></i><b>8.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="8.3" data-path="regression1.html"><a href="regression1.html#regression"><i class="fa fa-check"></i><b>8.3</b> Regression</a></li>
<li class="chapter" data-level="8.4" data-path="regression1.html"><a href="regression1.html#sacremento-real-estate-example"><i class="fa fa-check"></i><b>8.4</b> Sacremento real estate example</a></li>
<li class="chapter" data-level="8.5" data-path="regression1.html"><a href="regression1.html#k-nearest-neighbours-regression"><i class="fa fa-check"></i><b>8.5</b> K-nearest neighbours regression</a></li>
<li class="chapter" data-level="8.6" data-path="regression1.html"><a href="regression1.html#assessing-a-knn-regression-model"><i class="fa fa-check"></i><b>8.6</b> Assessing a knn regression model</a><ul>
<li class="chapter" data-level="8.6.1" data-path="regression1.html"><a href="regression1.html#rmspe-versus-rmse"><i class="fa fa-check"></i><b>8.6.1</b> <span class="math inline">\(RMSPE\)</span> versus <span class="math inline">\(RMSE\)</span></a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="regression1.html"><a href="regression1.html#how-do-different-ks-affect-k-nn-regression-predictions"><i class="fa fa-check"></i><b>8.7</b> How do different k’s affect k-nn regression predictions</a></li>
<li class="chapter" data-level="8.8" data-path="regression1.html"><a href="regression1.html#assessing-how-well-the-model-predicts-on-unseen-data-with-the-test-set"><i class="fa fa-check"></i><b>8.8</b> Assessing how well the model predicts on unseen data with the test set</a></li>
<li class="chapter" data-level="8.9" data-path="regression1.html"><a href="regression1.html#strengths-and-limitations-of-k-nn-regression"><i class="fa fa-check"></i><b>8.9</b> Strengths and limitations of k-nn regression</a><ul>
<li class="chapter" data-level="8.9.1" data-path="regression1.html"><a href="regression1.html#strengths-of-k-nn-regression"><i class="fa fa-check"></i><b>8.9.1</b> Strengths of k-nn regression</a></li>
<li class="chapter" data-level="8.9.2" data-path="regression1.html"><a href="regression1.html#limitations-of-k-nn-regression"><i class="fa fa-check"></i><b>8.9.2</b> Limitations of k-nn regression</a></li>
</ul></li>
<li class="chapter" data-level="8.10" data-path="regression1.html"><a href="regression1.html#multivariate-k-nn-regression"><i class="fa fa-check"></i><b>8.10</b> Multivariate k-nn regression</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="regression2.html"><a href="regression2.html"><i class="fa fa-check"></i><b>9</b> Regression, continued</a><ul>
<li class="chapter" data-level="9.1" data-path="regression2.html"><a href="regression2.html#overview-7"><i class="fa fa-check"></i><b>9.1</b> Overview</a></li>
<li class="chapter" data-level="9.2" data-path="regression2.html"><a href="regression2.html#chapter-learning-objectives-7"><i class="fa fa-check"></i><b>9.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="9.3" data-path="regression2.html"><a href="regression2.html#linear-regression"><i class="fa fa-check"></i><b>9.3</b> Linear regression</a></li>
<li class="chapter" data-level="9.4" data-path="regression2.html"><a href="regression2.html#linear-regression-in-r-using-caret"><i class="fa fa-check"></i><b>9.4</b> Linear regression in R using <code>caret</code></a></li>
<li class="chapter" data-level="9.5" data-path="regression2.html"><a href="regression2.html#comparing-linear-and-k-nn-regression"><i class="fa fa-check"></i><b>9.5</b> Comparing linear and k-nn regression</a></li>
<li class="chapter" data-level="9.6" data-path="regression2.html"><a href="regression2.html#multivariate-linear-regression"><i class="fa fa-check"></i><b>9.6</b> Multivariate linear regression</a></li>
<li class="chapter" data-level="9.7" data-path="regression2.html"><a href="regression2.html#the-other-side-of-regression"><i class="fa fa-check"></i><b>9.7</b> The other side of regression</a></li>
<li class="chapter" data-level="9.8" data-path="regression2.html"><a href="regression2.html#additional-readingsresources-2"><i class="fa fa-check"></i><b>9.8</b> Additional readings/resources</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>10</b> Clustering</a><ul>
<li class="chapter" data-level="10.1" data-path="clustering.html"><a href="clustering.html#overview-8"><i class="fa fa-check"></i><b>10.1</b> Overview</a></li>
<li class="chapter" data-level="10.2" data-path="clustering.html"><a href="clustering.html#chapter-learning-objectives-8"><i class="fa fa-check"></i><b>10.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="10.3" data-path="clustering.html"><a href="clustering.html#clustering-1"><i class="fa fa-check"></i><b>10.3</b> Clustering</a><ul>
<li class="chapter" data-level="10.3.1" data-path="clustering.html"><a href="clustering.html#a-toy-example"><i class="fa fa-check"></i><b>10.3.1</b> A toy example</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="clustering.html"><a href="clustering.html#k-means-clustering-algorithm"><i class="fa fa-check"></i><b>10.4</b> K-means clustering algorithm</a></li>
<li class="chapter" data-level="10.5" data-path="clustering.html"><a href="clustering.html#k-means-clustering-in-r"><i class="fa fa-check"></i><b>10.5</b> K-means clustering in R</a></li>
<li class="chapter" data-level="10.6" data-path="clustering.html"><a href="clustering.html#choosing-k-for-k-means-clustering"><i class="fa fa-check"></i><b>10.6</b> Choosing K for K-means clustering</a></li>
<li class="chapter" data-level="10.7" data-path="clustering.html"><a href="clustering.html#additional-readings"><i class="fa fa-check"></i><b>10.7</b> Additional readings:</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression1" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Introduction to regression through K-nearest neighbours</h1>
<div id="overview-6" class="section level2">
<h2><span class="header-section-number">8.1</span> Overview</h2>
<p>Introduction to regression using K-nearest neighbours (k-nn). We will first focus on prediction in cases where there is a response variable of interest and a predictor variable. We will work an example of multiple regression in the prediction context using k-nn.</p>
</div>
<div id="chapter-learning-objectives-6" class="section level2">
<h2><span class="header-section-number">8.2</span> Chapter learning objectives</h2>
<p>By the end of the chapter, students will be able to:</p>
<ul>
<li>Recognize situations where a simple regression analysis would be appropriate for making predictions.</li>
<li>Explain the k-nearest neighbour (k-nn) regression algorithm and describe how it differs from k-nn classification.</li>
<li>Interpret the output of a k-nn regression.</li>
<li>In a dataset with two variables, perform k-nearest neighbour regression in R using <code>caret::train()</code> to predict the values for a test dataset.</li>
<li>Using R, execute cross-validation in R to choose the number of neighbours.</li>
<li>Using R, evaluate k-nn regression prediction accuracy using a test data set and an appropriate metric (<em>e.g.</em>, root means square prediction error).</li>
<li>In a dataset with &gt; 2 variables, perform k-nn regression in R using <code>caret</code>’s <code>train</code> with <code>method = &quot;k-nn&quot;</code> to predict the values for a test dataset.</li>
<li>In the context of k-nn regression, compare and contrast goodness of fit and prediction properties (namely RMSE vs RMSPE).</li>
<li>Describe advantages and disadvantages of the k-nearest neighbour regression approach.</li>
</ul>
</div>
<div id="regression" class="section level2">
<h2><span class="header-section-number">8.3</span> Regression</h2>
<p>We can use regression as a method to answer a very similar question to classification (can we use past information to predict future observations?), but in the case of regression the goal is to predict numerical values instead of class labels. An example regression prediction question would be: can we use hours spent on exercise each week to predict marathon race time? And another example regression prediction question is: can we use house size (livable square feet) to predict house sale price? We will use regression to explore this question in the rest of this chapter, using a real estate data set from Sacremento, California that is available in the <code>caret</code> package. <em>Note: in addition to prediction, regression can also be used to model the relationship between two or more variables, but here we will focus only on prediction.</em></p>
</div>
<div id="sacremento-real-estate-example" class="section level2">
<h2><span class="header-section-number">8.4</span> Sacremento real estate example</h2>
<p>Let’s start by loading the libraries we need and previewing the data set. The data set comes with the <code>caret</code> package, so as soon as we load the <code>caret</code> library and type <code>data(Sacramento)</code> we are able to access it as a data frame named <code>Sacramento</code>.</p>
<div class="sourceCode" id="cb233"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb233-1" data-line-number="1"><span class="kw">library</span>(tidyverse)</a>
<a class="sourceLine" id="cb233-2" data-line-number="2"><span class="kw">library</span>(scales)</a>
<a class="sourceLine" id="cb233-3" data-line-number="3"><span class="kw">library</span>(caret)</a>
<a class="sourceLine" id="cb233-4" data-line-number="4"><span class="kw">library</span>(gridExtra)</a>
<a class="sourceLine" id="cb233-5" data-line-number="5"></a>
<a class="sourceLine" id="cb233-6" data-line-number="6"><span class="kw">data</span>(Sacramento)</a>
<a class="sourceLine" id="cb233-7" data-line-number="7"><span class="kw">head</span>(Sacramento)</a></code></pre></div>
<pre><code>##         city    zip beds baths sqft        type price latitude longitude
## 1 SACRAMENTO z95838    2     1  836 Residential 59222 38.63191 -121.4349
## 2 SACRAMENTO z95823    3     1 1167 Residential 68212 38.47890 -121.4310
## 3 SACRAMENTO z95815    2     1  796 Residential 68880 38.61830 -121.4438
## 4 SACRAMENTO z95815    2     1  852 Residential 69307 38.61684 -121.4391
## 5 SACRAMENTO z95824    2     1  797 Residential 81900 38.51947 -121.4358
## 6 SACRAMENTO z95841    3     1 1122       Condo 89921 38.66260 -121.3278</code></pre>
<p>Given that we are interested in answering the question of if we can we use house size (livable square feet) to predict house sale price in the Sacremento, CA area; the columns in this data frame that we are interested in are <code>sqft</code> (which is the house size in livable square feet) and <code>price</code>, which is the house size in US dollars (USD). Let’s next visualize the data as a scatter plot where we place the predictor/explanatory variable, house size, on the x-axis and the target/response variable, price, on the y-axis (this is what we would like to predict):</p>
<div class="sourceCode" id="cb235"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb235-1" data-line-number="1">eda &lt;-<span class="st"> </span><span class="kw">ggplot</span>(Sacramento, <span class="kw">aes</span>(<span class="dt">x =</span> sqft, <span class="dt">y =</span> price)) <span class="op">+</span></a>
<a class="sourceLine" id="cb235-2" data-line-number="2"><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">0.4</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb235-3" data-line-number="3"><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;House size (square footage)&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb235-4" data-line-number="4"><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Price (USD)&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb235-5" data-line-number="5"><span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">labels =</span> <span class="kw">dollar_format</span>()) </a>
<a class="sourceLine" id="cb235-6" data-line-number="6">eda</a></code></pre></div>
<p><img src="_main_files/figure-html/07-edaRegr-1.png" width="480" /></p>
<p>From looking at the visualization above, we see that in Sacramento, CA, as house size (square footage) increases, so does house price. Thus, we can reason that house size might be a useful predictor of house price and perhaps we can use the size of the house to predict the price a house will be sold at in this area (for a home that has not yet sold and thus consequently we do not know the house price).</p>
</div>
<div id="k-nearest-neighbours-regression" class="section level2">
<h2><span class="header-section-number">8.5</span> K-nearest neighbours regression</h2>
<p>Let’s take a small sample of the data above and walk through how K-nearest neighbours (knn) regression works before we dive in to creating our model and assessing how well it predicts house price. This subsample is taken to allow us to illustrate the mechanics of k-nn regression with a few data points, later in this chapter we will use all the data.</p>
<p>To take a small random sample of size 30 , we’ll use the function <code>sample_n</code>. This function takes two arguments:</p>
<ol style="list-style-type: decimal">
<li><code>tbl</code> (a data frame-like object to sample from)</li>
<li><code>size</code> (the number of observations/rows to be randomly selected/sampled)</li>
</ol>
<div class="sourceCode" id="cb236"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb236-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1234</span>)</a>
<a class="sourceLine" id="cb236-2" data-line-number="2">small_sacramento &lt;-<span class="st"> </span><span class="kw">sample_n</span>(Sacramento, <span class="dt">size =</span> <span class="dv">30</span>)</a></code></pre></div>
<p>Next let’s say we come across a new house we are interested in purchasing, and it is 2000 square feet! Its advertised list price is $350,000 should we offer to pay the asking price for this house? Or is that overpriced and we should offer less? Perhaps we cannot directly answer that, but we can get close by using the data we have to predict the sale price given the sale prices we have already observed.</p>
<p>Given the data in the plot below, we have no observations of a house that has sold that is 2000 square feet, so how can we predict the price?</p>
<div class="sourceCode" id="cb237"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb237-1" data-line-number="1">small_plot &lt;-<span class="st"> </span><span class="kw">ggplot</span>(small_sacramento, <span class="kw">aes</span>(<span class="dt">x =</span> sqft, <span class="dt">y =</span> price)) <span class="op">+</span></a>
<a class="sourceLine" id="cb237-2" data-line-number="2"><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb237-3" data-line-number="3"><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;House size (square footage)&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb237-4" data-line-number="4"><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Price (USD)&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb237-5" data-line-number="5"><span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">labels=</span><span class="kw">dollar_format</span>()) <span class="op">+</span></a>
<a class="sourceLine" id="cb237-6" data-line-number="6"><span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="dv">2000</span>, <span class="dt">linetype =</span> <span class="st">&quot;dotted&quot;</span>) </a>
<a class="sourceLine" id="cb237-7" data-line-number="7">small_plot</a></code></pre></div>
<p><img src="_main_files/figure-html/07-small-eda-regr-1.png" width="480" /></p>
<p>What we can do is use the neighbouring points to suggest/predict what the price should be. For the example above, below we find and label the 5 nearest neighbours to our observation of a house that is 2000 square feet:</p>
<div class="sourceCode" id="cb238"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb238-1" data-line-number="1">nearest_neighbours &lt;-<span class="st"> </span>small_sacramento <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb238-2" data-line-number="2"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">diff =</span> <span class="kw">abs</span>(<span class="dv">2000</span> <span class="op">-</span><span class="st"> </span>sqft)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb238-3" data-line-number="3"><span class="st">  </span><span class="kw">arrange</span>(diff) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb238-4" data-line-number="4"><span class="st">  </span><span class="kw">head</span>(<span class="dv">5</span>)</a>
<a class="sourceLine" id="cb238-5" data-line-number="5">nearest_neighbours</a></code></pre></div>
<pre><code>##             city    zip beds baths sqft        type  price latitude
## 1     GOLD_RIVER z95670    3     2 1981 Residential 305000 38.62873
## 2      ELK_GROVE z95758    4     2 2056 Residential 275000 38.41152
## 3      ELK_GROVE z95624    5     3 2136 Residential 223058 38.43544
## 4 RANCHO_CORDOVA z95742    4     2 1713 Residential 263500 38.55387
## 5      RIO_LINDA z95673    2     2 1690 Residential 136500 38.69110
##   longitude diff
## 1 -121.2611   19
## 2 -121.4814   56
## 3 -121.3945  136
## 4 -121.2191  287
## 5 -121.4518  310</code></pre>
<p><img src="_main_files/figure-html/07-knn3-example-1.png" width="480" /></p>
<p>Now that we have the 5 nearest neighbours to our new observation that we would like to predict the price for, we can use their values to predict a selling price for the home we are interested in buying that is 2000 square feet. Specifically, we can take the mean (or average) of these 5 values as our predicted value.</p>
<div class="sourceCode" id="cb240"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb240-1" data-line-number="1">prediction &lt;-<span class="st"> </span>nearest_neighbours <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb240-2" data-line-number="2"><span class="st">  </span><span class="kw">summarise</span>(<span class="dt">predicted =</span> <span class="kw">mean</span>(price))</a>
<a class="sourceLine" id="cb240-3" data-line-number="3">prediction</a></code></pre></div>
<pre><code>##   predicted
## 1  240611.6</code></pre>
<p><img src="_main_files/figure-html/07-predictedViz-knn-1.png" width="480" /></p>
<p>Our predicted price is $240612 (shown as a red point above), which is much less than $350,000, and so perhaps we might want to offer less than the list price that the house is advertised at. Simple right? Not quite. We have all the same unanswered questions here with k-nn regression that we had with k-nn classification. Which <span class="math inline">\(k\)</span> do we choose? And, is our model any good at making predictions? We’ll shortly address how to answer these questions in the context of k-nn regression.</p>
</div>
<div id="assessing-a-knn-regression-model" class="section level2">
<h2><span class="header-section-number">8.6</span> Assessing a knn regression model</h2>
<p>As usual, we should start by putting some test data away in a lock box that we can come back to after we choose our final model, so let’s take care of that business now. Note: for the remainder of the chapter we’ll be working with the entire Sacramento data set as opposed to the smaller sample of 30 points we worked with above.</p>
<div class="sourceCode" id="cb242"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb242-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1234</span>)</a>
<a class="sourceLine" id="cb242-2" data-line-number="2">training_rows &lt;-<span class="st"> </span>Sacramento <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb242-3" data-line-number="3"><span class="st">  </span><span class="kw">select</span>(price) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb242-4" data-line-number="4"><span class="st">  </span><span class="kw">unlist</span>() <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb242-5" data-line-number="5"><span class="st">  </span><span class="kw">createDataPartition</span>(<span class="dt">p =</span> <span class="fl">0.6</span>, <span class="dt">list =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb242-6" data-line-number="6"></a>
<a class="sourceLine" id="cb242-7" data-line-number="7">X_train &lt;-<span class="st"> </span>Sacramento <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb242-8" data-line-number="8"><span class="st">  </span><span class="kw">select</span>(sqft) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb242-9" data-line-number="9"><span class="st">  </span><span class="kw">slice</span>(training_rows) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb242-10" data-line-number="10"><span class="st">  </span><span class="kw">data.frame</span>()</a>
<a class="sourceLine" id="cb242-11" data-line-number="11"></a>
<a class="sourceLine" id="cb242-12" data-line-number="12">Y_train &lt;-<span class="st"> </span>Sacramento <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb242-13" data-line-number="13"><span class="st">  </span><span class="kw">select</span>(price) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb242-14" data-line-number="14"><span class="st">  </span><span class="kw">slice</span>(training_rows) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb242-15" data-line-number="15"><span class="st">  </span><span class="kw">unlist</span>()</a>
<a class="sourceLine" id="cb242-16" data-line-number="16"></a>
<a class="sourceLine" id="cb242-17" data-line-number="17">X_test &lt;-<span class="st"> </span>Sacramento <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb242-18" data-line-number="18"><span class="st">  </span><span class="kw">select</span>(sqft) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb242-19" data-line-number="19"><span class="st">  </span><span class="kw">slice</span>(<span class="op">-</span>training_rows) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb242-20" data-line-number="20"><span class="st">  </span><span class="kw">data.frame</span>()</a>
<a class="sourceLine" id="cb242-21" data-line-number="21"></a>
<a class="sourceLine" id="cb242-22" data-line-number="22">Y_test &lt;-<span class="st"> </span>Sacramento <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb242-23" data-line-number="23"><span class="st">  </span><span class="kw">select</span>(price) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb242-24" data-line-number="24"><span class="st">  </span><span class="kw">slice</span>(<span class="op">-</span>training_rows) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb242-25" data-line-number="25"><span class="st">  </span><span class="kw">unlist</span>()</a></code></pre></div>
<p>Next, we’ll use cross-validation to choose <span class="math inline">\(k\)</span>. In k-nn classification, we used accuracy to see how well our predictions matched the true labels. Here in the context of k-nn regression we will use root mean square prediction error (<span class="math inline">\(RMSPE\)</span>) instead. If the predictions are very close to the true values, then <span class="math inline">\(RMSPE\)</span> will be small. If, on the other-hand, the predictions are very different to the true values, then <span class="math inline">\(RMSPE\)</span> will be quite large. Thus, when we are doing cross validation to choose <span class="math inline">\(k\)</span>, we want to choose the <span class="math inline">\(k\)</span> that gives us the smallest <span class="math inline">\(RMSPE\)</span>.</p>
<p>The mathematical formula for calculating <span class="math inline">\(RMSPE\)</span> is shown below:</p>
<p><span class="math display">\[RMSPE = \sqrt{\frac{1}{n}\sum\limits_{i=1}^{n}(y_i - \hat{y_i})^2}\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(n\)</span> is the number of observations</li>
<li><span class="math inline">\(y_i\)</span> is the observed value for the <span class="math inline">\(ith\)</span> observation</li>
<li><span class="math inline">\(\hat{y_i}\)</span> is the forcasted/predicted value for the <span class="math inline">\(ith\)</span> observation</li>
</ul>
<p>A key feature the formula for RMSE is the distance between the observed target/response variable value, <span class="math inline">\(y\)</span>, and the prediction target/response variable value, <span class="math inline">\(\hat{y_i}\)</span>, for each observation (from 1 to <span class="math inline">\(i\)</span>).</p>
<p>Now that we know how we can assess how well our model predicts a numerical value, let’s use R to perform cross-validation and to choose the optimal <span class="math inline">\(k\)</span>.</p>
<div class="sourceCode" id="cb243"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb243-1" data-line-number="1">train_control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">10</span>)</a>
<a class="sourceLine" id="cb243-2" data-line-number="2"><span class="co"># makes a column of k&#39;s, from 1 to 500 in increments of 5</span></a>
<a class="sourceLine" id="cb243-3" data-line-number="3">k_lots =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">k =</span> <span class="kw">seq</span>(<span class="dt">from =</span> <span class="dv">1</span>, <span class="dt">to =</span> <span class="dv">500</span>, <span class="dt">by =</span> <span class="dv">5</span>)) </a>
<a class="sourceLine" id="cb243-4" data-line-number="4"></a>
<a class="sourceLine" id="cb243-5" data-line-number="5"><span class="kw">set.seed</span>(<span class="dv">1234</span>)</a>
<a class="sourceLine" id="cb243-6" data-line-number="6">knn_reg_cv_<span class="dv">10</span> &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">x =</span> X_train, </a>
<a class="sourceLine" id="cb243-7" data-line-number="7">                       <span class="dt">y =</span> Y_train, </a>
<a class="sourceLine" id="cb243-8" data-line-number="8">                       <span class="dt">method =</span> <span class="st">&quot;knn&quot;</span>, </a>
<a class="sourceLine" id="cb243-9" data-line-number="9">                       <span class="dt">tuneGrid =</span> k_lots, </a>
<a class="sourceLine" id="cb243-10" data-line-number="10">                       <span class="dt">trControl =</span> train_control) </a>
<a class="sourceLine" id="cb243-11" data-line-number="11"></a>
<a class="sourceLine" id="cb243-12" data-line-number="12"><span class="kw">ggplot</span>(knn_reg_cv_<span class="dv">10</span><span class="op">$</span>results, <span class="kw">aes</span>(<span class="dt">x =</span> k, <span class="dt">y =</span> RMSE)) <span class="op">+</span></a>
<a class="sourceLine" id="cb243-13" data-line-number="13"><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb243-14" data-line-number="14"><span class="st">  </span><span class="kw">geom_line</span>()</a></code></pre></div>
<p><img src="_main_files/figure-html/07-choose-k-knn-1.png" width="480" /></p>
<p>Here we see that the smallest <span class="math inline">\(RMSPE\)</span> is from the model where <span class="math inline">\(k\)</span> = 41. Thus the best <span class="math inline">\(k\)</span> for this model is 41.</p>
<div id="rmspe-versus-rmse" class="section level3">
<h3><span class="header-section-number">8.6.1</span> <span class="math inline">\(RMSPE\)</span> versus <span class="math inline">\(RMSE\)</span></h3>
<p>The error output we have been getting from <code>caret</code> to assess how well our k-nn regression models predict is labelled as <code>RMSE</code>, standing for root mean squared error. Why is this so? In statistics we try to be very precise with our language to indicate whether we are calculating the error on predicting on the training data (in sample prediction) versus predicting on the testing data (out of sample prediction). When we are predicting on the training data and we are interested in assessing model goodness of fit on the data used to fit the model, we say <span class="math inline">\(RMSE\)</span>. Whereas when we are predicting on the testing data and we are trying to assess how well our model will do at predicting on future, unseen data (e.g., the validation set(s), or the testing set) we say <span class="math inline">\(RMSPE\)</span>. <code>caret</code> doesn’t really know what you are doing (using training or testing data for prediction) and so it just uses the term <span class="math inline">\(RMSE\)</span> regardless. The equation for calculating <span class="math inline">\(RMSE\)</span> and <span class="math inline">\(RMSPE\)</span> is the same, all that changes is where the <span class="math inline">\(y\)</span>’s come from.</p>
</div>
</div>
<div id="how-do-different-ks-affect-k-nn-regression-predictions" class="section level2">
<h2><span class="header-section-number">8.7</span> How do different k’s affect k-nn regression predictions</h2>
<p>Below we plot the predicted values for house price from our k-nn regression models for 6 different values for <span class="math inline">\(k\)</span> where the only predictor is home size. For each model, we predict a price for every possible home size across the range of home sizes we observed in the data set (here 500 to 4250 square feet) and we plot the predicted prices as a blue line:</p>
<p><img src="_main_files/figure-html/07-howK-1.png" width="960" /></p>
<p>From the plots above, we see that when <span class="math inline">\(k\)</span> = 1, the blue line runs perfectly through almost all of our training observations. This happens because our predicted values for a given region, depend on just a single observation. A model like this has high variance and low bias. It has high variance because the flexible blue line follows the training observations very closely, and if we were to change any one of the training observation data points we would change the flexible blue line quite a lot. This means that the blue line matches the data we happen to have in this training data set, however, if we were to collect another training data set from the Sacramento real estate market it likely wouldn’t match those observations as well. However, it has low bias because the model/predicted values matches the actual observed values in this training data set very well. Another term that we use to collectively describe this phenomenon is overfitting.</p>
<p>What about the plot where <span class="math inline">\(k\)</span> is quite large, say <span class="math inline">\(k\)</span> = 450, for example? When <span class="math inline">\(k\)</span> = 450 for this data set, the blue line is extremely smooth, and almost flat. This happens because our predicted values for a given x value (here home size), depend on many many neighbouring observations, 450 to be exact! A model like this has low variance and high bias. It has low variance because the smooth, inflexible blue line does not follow the training observations very closely, and if we were to change any one of the training observation data points it really wouldn’t affect the shape of the smooth blue line at all. This means that although the blue line matches does not match the data we happen to have in this particular training data set perfectly, if we were to collect another training data set from the Sacramento real estate market it likely would match those observations equally as well as it matches those in this training data set. This model also has high bias because the model/predicted values does not match the actual observed values very well. Another term that we use to collectively describe this kind of model is underfitting.</p>
<p>Ideally, what we want is neither of the two examples discussed above. Instead, we would like a model with low variance (so that it will transer/generalize well to other data sets, meaning that it isn’t too dependent on the observations that happen to be in the training set we had) <strong>and</strong> low bias (one where the model/predicted values matches the actual observed values very well). If we explore the other values for <span class="math inline">\(k\)</span>, in particular <span class="math inline">\(k\)</span> = 41 (the optimal <span class="math inline">\(k\)</span> as suggested by cross-validation), we can see it has a lower bias than our model with a very high <span class="math inline">\(k\)</span> (e.g., 450), and thus the model/predicted values better match the actual observed values than the high <span class="math inline">\(k\)</span> model. Additionally, it has lower variance than our model with a very low <span class="math inline">\(k\)</span> (e.g., 1) and thus it should better transer/generalize to other data sets compared to the low <span class="math inline">\(k\)</span> model. All of this is similar to how the choice of <span class="math inline">\(k\)</span> affects k-nn classification (discussed in the previous chapter).</p>
</div>
<div id="assessing-how-well-the-model-predicts-on-unseen-data-with-the-test-set" class="section level2">
<h2><span class="header-section-number">8.8</span> Assessing how well the model predicts on unseen data with the test set</h2>
<p>To assess how well our model might do at predicting on unseen data, we will assess its RMSPE when predicting on the test data. Before we do that, we want to re-train our k-nn regression model on the entire training data set (not performing cross validation this time).</p>
<p>In the case of k-nn regression we use the function <code>defaultSummary</code> instead of <code>confusionMatrix</code> (which we used with knn classification). This is because our predictions are not class labels, but values, and as such the type of model prediction performance score is calculated differently. <code>defaultSummary</code> expects a data frame where one column is the observed target/response variable values from the test data, and a second column of the predicted values for the test data.</p>
<div class="sourceCode" id="cb244"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb244-1" data-line-number="1">k =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">k =</span> knn_reg_cv_<span class="dv">10</span><span class="op">$</span>bestTune<span class="op">$</span>k)</a>
<a class="sourceLine" id="cb244-2" data-line-number="2"></a>
<a class="sourceLine" id="cb244-3" data-line-number="3"><span class="kw">set.seed</span>(<span class="dv">1234</span>)</a>
<a class="sourceLine" id="cb244-4" data-line-number="4">knn_reg_final &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">x =</span> X_train, <span class="dt">y =</span> Y_train, <span class="dt">method =</span> <span class="st">&quot;knn&quot;</span>, <span class="dt">tuneGrid =</span> k)</a>
<a class="sourceLine" id="cb244-5" data-line-number="5"></a>
<a class="sourceLine" id="cb244-6" data-line-number="6">test_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(knn_reg_final, X_test)</a>
<a class="sourceLine" id="cb244-7" data-line-number="7">modelvalues &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">obs =</span> Y_test, <span class="dt">pred =</span> test_pred)</a>
<a class="sourceLine" id="cb244-8" data-line-number="8">test_results &lt;-<span class="st"> </span><span class="kw">defaultSummary</span>(modelvalues)</a>
<a class="sourceLine" id="cb244-9" data-line-number="9">test_results</a></code></pre></div>
<pre><code>##         RMSE     Rsquared          MAE 
## 87120.618645     0.580151 64185.103589</code></pre>
<p>Our final model’s test error as assessed by <span class="math inline">\(RMSPE\)</span> is 87120.62. But what does this <span class="math inline">\(RMSPE\)</span> score mean? When we calculated test set prediction accuracy when we performed k-nn classification the highest possible value was 1, and if we got an value close to that it was easy to assess how well our model did on at least one new data set that had never been used to choose our model (so we didn’t violate the golden rule of statistical/machine learning). So what about <span class="math inline">\(RMSPE\)</span>, what is it out of? Unfortunately there is no scale for <span class="math inline">\(RMSPE\)</span> (instead it is measured in the units of the target/response variable), and so it is a bit hard to interpret. For now, let’s consider this approach to thinking about <span class="math inline">\(RMSPE\)</span> from our testing data set: as long as its not WAY worse than the cross-validation <span class="math inline">\(RMSPE\)</span> of our best model then we can say that we’re not doing too much worse on the test data than we did on the training data, and so it appears to be generalizing OK to a new data set it has never seen before. In future courses on statistical/machine learning we will learn more about how to interpret <span class="math inline">\(RMSPE\)</span> from our testing data set and other ways to assess our model.</p>
<p>And what does our final model look like when we predict across all possible house sizes we might encounter in the Sacramento area? We plotted it above where we explored how <span class="math inline">\(k\)</span> affects k-nn regression, but we show it again now, along with the code for how we generated it:</p>
<div class="sourceCode" id="cb246"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb246-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1234</span>)</a>
<a class="sourceLine" id="cb246-2" data-line-number="2">predictions_all &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">sqft =</span> <span class="kw">seq</span>(<span class="dt">from =</span> <span class="dv">500</span>, <span class="dt">to =</span> <span class="dv">4250</span>, <span class="dt">by =</span> <span class="dv">1</span>))</a>
<a class="sourceLine" id="cb246-3" data-line-number="3">predictions_all<span class="op">$</span>price &lt;-<span class="st"> </span><span class="kw">predict</span>(knn_reg_final, </a>
<a class="sourceLine" id="cb246-4" data-line-number="4">                                 <span class="kw">data.frame</span>(<span class="dt">sqft =</span> <span class="kw">seq</span>(<span class="dt">from =</span> <span class="dv">500</span>, <span class="dt">to =</span> <span class="dv">4250</span>, <span class="dt">by =</span> <span class="dv">1</span>)))</a>
<a class="sourceLine" id="cb246-5" data-line-number="5">train_data &lt;-<span class="st"> </span><span class="kw">bind_cols</span>(X_train, <span class="kw">data.frame</span>(<span class="dt">price =</span> Y_train)) <span class="co">#combines X_train and Y_train to be on data set</span></a>
<a class="sourceLine" id="cb246-6" data-line-number="6">plot_final &lt;-<span class="st"> </span><span class="kw">ggplot</span>(train_data, <span class="kw">aes</span>(<span class="dt">x =</span> sqft, <span class="dt">y =</span> price)) <span class="op">+</span></a>
<a class="sourceLine" id="cb246-7" data-line-number="7"><span class="st">    </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">0.4</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb246-8" data-line-number="8"><span class="st">    </span><span class="kw">xlab</span>(<span class="st">&quot;House size (square footage)&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb246-9" data-line-number="9"><span class="st">    </span><span class="kw">ylab</span>(<span class="st">&quot;Price (USD)&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb246-10" data-line-number="10"><span class="st">    </span><span class="kw">scale_y_continuous</span>(<span class="dt">labels =</span> <span class="kw">dollar_format</span>())  <span class="op">+</span></a>
<a class="sourceLine" id="cb246-11" data-line-number="11"><span class="st">    </span><span class="kw">geom_line</span>(<span class="dt">data =</span> predictions_all, <span class="kw">aes</span>(<span class="dt">x =</span> sqft, <span class="dt">y =</span> price), <span class="dt">color =</span> <span class="st">&quot;blue&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb246-12" data-line-number="12"><span class="st">    </span><span class="kw">ggtitle</span>(<span class="st">&quot;k = 41&quot;</span>)</a>
<a class="sourceLine" id="cb246-13" data-line-number="13">plot_final</a></code></pre></div>
<p><img src="_main_files/figure-html/07-predict-all-1.png" width="480" /></p>
</div>
<div id="strengths-and-limitations-of-k-nn-regression" class="section level2">
<h2><span class="header-section-number">8.9</span> Strengths and limitations of k-nn regression</h2>
<p>As with k-nn classification (or any prediction algorithm for that manner), k-nn regression has both strengths and weaknesses. Some are listed here:</p>
<div id="strengths-of-k-nn-regression" class="section level3">
<h3><span class="header-section-number">8.9.1</span> Strengths of k-nn regression</h3>
<ol style="list-style-type: decimal">
<li>Simple and easy to understand</li>
<li>No assumptions about what the data must look like</li>
<li>Works well with non-linear relationships (i.e., if the relationship is not a straight line)</li>
</ol>
</div>
<div id="limitations-of-k-nn-regression" class="section level3">
<h3><span class="header-section-number">8.9.2</span> Limitations of k-nn regression</h3>
<ol style="list-style-type: decimal">
<li>As data gets bigger and bigger, k-nn gets slower and slower, quite quickly</li>
<li>Does not perform well with a large number of predictors unless the size of the training set is exponentially larger</li>
<li>Does not predict well beyond the range of values input in your training data</li>
</ol>
</div>
</div>
<div id="multivariate-k-nn-regression" class="section level2">
<h2><span class="header-section-number">8.10</span> Multivariate k-nn regression</h2>
<p>As in k-nn classification, in k-nn regression we can have multiple predictors. When we have multiple predictors in k-nn regression, we have the same concern regarding the scale of the predictors. This is because as in k-nn classification and regression, predictions are made by identifying the <span class="math inline">\(k\)</span> observations that are nearest to the new point we want to predict, and any variables that are on a large scale will have a much larger effect than variables on a small scale. Thus, once we start performing multivariate k-nn regression we need to use the <code>scale</code> function in R on our predictors to ensure this doesn’t happen.</p>
<p>We will now demonstrate a multi-variate k-nn regression analysis again using the <code>caret</code> package on the Sacramento real estate data. This time we will use house size (measured in square feet) as well as number of bathrooms as our predictors, and continue to use house sale price as our outcome/target variable that we are trying to predict.</p>
<p>It is always a good practice to do exploratory data analysis, such as visualizing the data, before we start modeling the data. Thus the first thing we will do is use ggpairs (from the <code>GGally</code> package) to plot all the variables we are interested in using in our analyses:</p>
<div class="sourceCode" id="cb247"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb247-1" data-line-number="1"><span class="kw">library</span>(GGally)</a></code></pre></div>
<pre><code>## Registered S3 method overwritten by &#39;GGally&#39;:
##   method from   
##   +.gg   ggplot2</code></pre>
<pre><code>## 
## Attaching package: &#39;GGally&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     nasa</code></pre>
<div class="sourceCode" id="cb251"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb251-1" data-line-number="1">plot_pairs &lt;-<span class="st"> </span>Sacramento <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb251-2" data-line-number="2"><span class="st">  </span><span class="kw">select</span>(price, sqft, baths) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb251-3" data-line-number="3"><span class="st">  </span><span class="kw">ggpairs</span>()</a>
<a class="sourceLine" id="cb251-4" data-line-number="4">plot_pairs</a></code></pre></div>
<p><img src="_main_files/figure-html/09-ggpairs-1.png" width="576" /></p>
<p>From this we can see that generally, as both house size and number of bathrooms increase, so does price. Does adding the number of baths to our model improve our ability to predict house price? To answer that question, we will have to come up with the test error for a k-nn regression model using house size and number of baths, and then we can compare it to the test error for the model we previously came up with that only used house size to see if it is smaller (decreased test error indicates increased prediction quality). Let’s do that now!</p>
<p>Looking at the data above, we can see that <code>sqft</code> and <code>beds</code> (number of bedrooms) are on vastly different scales. Thus we need to apply the <code>scale</code> function to these columns before we start our analysis:</p>
<div class="sourceCode" id="cb252"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb252-1" data-line-number="1">scaled_Sacramento &lt;-<span class="st"> </span>Sacramento <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb252-2" data-line-number="2"><span class="st">  </span><span class="kw">select</span>(price, sqft, baths) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb252-3" data-line-number="3"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">sqft =</span> <span class="kw">scale</span>(sqft, <span class="dt">center =</span> <span class="ot">FALSE</span>),</a>
<a class="sourceLine" id="cb252-4" data-line-number="4">         <span class="dt">baths =</span> <span class="kw">scale</span>(baths, <span class="dt">center =</span> <span class="ot">FALSE</span>))</a>
<a class="sourceLine" id="cb252-5" data-line-number="5"><span class="kw">head</span>(scaled_Sacramento)</a></code></pre></div>
<pre><code>##   price      sqft    baths
## 1 59222 0.4564854 0.459221
## 2 68212 0.6372231 0.459221
## 3 68880 0.4346440 0.459221
## 4 69307 0.4652220 0.459221
## 5 81900 0.4351901 0.459221
## 6 89921 0.6126515 0.459221</code></pre>
<p>Now we can split our data into a trained and test set as we did before:</p>
<div class="sourceCode" id="cb254"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb254-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2019</span>) <span class="co"># makes the random selection of rows reproducible</span></a>
<a class="sourceLine" id="cb254-2" data-line-number="2">training_rows &lt;-<span class="st"> </span>scaled_Sacramento <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb254-3" data-line-number="3"><span class="st">  </span><span class="kw">select</span>(price) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb254-4" data-line-number="4"><span class="st">  </span><span class="kw">unlist</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># converts Class from a tibble to a vector</span></a>
<a class="sourceLine" id="cb254-5" data-line-number="5"><span class="st">  </span><span class="kw">createDataPartition</span>(<span class="dt">p =</span> <span class="fl">0.6</span>, <span class="dt">list =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb254-6" data-line-number="6"></a>
<a class="sourceLine" id="cb254-7" data-line-number="7">X_train &lt;-<span class="st"> </span>scaled_Sacramento <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb254-8" data-line-number="8"><span class="st">  </span><span class="kw">select</span>(sqft, baths) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb254-9" data-line-number="9"><span class="st">  </span><span class="kw">slice</span>(training_rows) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb254-10" data-line-number="10"><span class="st">  </span><span class="kw">data.frame</span>()</a>
<a class="sourceLine" id="cb254-11" data-line-number="11"></a>
<a class="sourceLine" id="cb254-12" data-line-number="12">Y_train &lt;-<span class="st"> </span>scaled_Sacramento <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb254-13" data-line-number="13"><span class="st">  </span><span class="kw">select</span>(price) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb254-14" data-line-number="14"><span class="st">  </span><span class="kw">slice</span>(training_rows) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb254-15" data-line-number="15"><span class="st">  </span><span class="kw">unlist</span>()</a>
<a class="sourceLine" id="cb254-16" data-line-number="16"></a>
<a class="sourceLine" id="cb254-17" data-line-number="17">X_test &lt;-<span class="st"> </span>scaled_Sacramento <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb254-18" data-line-number="18"><span class="st">  </span><span class="kw">select</span>(sqft, baths) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb254-19" data-line-number="19"><span class="st">  </span><span class="kw">slice</span>(<span class="op">-</span>training_rows) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb254-20" data-line-number="20"><span class="st">  </span><span class="kw">data.frame</span>()</a>
<a class="sourceLine" id="cb254-21" data-line-number="21"></a>
<a class="sourceLine" id="cb254-22" data-line-number="22">Y_test &lt;-<span class="st"> </span>scaled_Sacramento <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb254-23" data-line-number="23"><span class="st">  </span><span class="kw">select</span>(price) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb254-24" data-line-number="24"><span class="st">  </span><span class="kw">slice</span>(<span class="op">-</span>training_rows) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb254-25" data-line-number="25"><span class="st">  </span><span class="kw">unlist</span>()</a></code></pre></div>
<p>Next, we’ll use 10-fold cross-validation to choose <span class="math inline">\(k\)</span>:</p>
<div class="sourceCode" id="cb255"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb255-1" data-line-number="1">train_control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">10</span>)</a>
<a class="sourceLine" id="cb255-2" data-line-number="2"><span class="co"># makes a column of k&#39;s, from 1 to 500 in increments of 5</span></a>
<a class="sourceLine" id="cb255-3" data-line-number="3">k_lots =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">k =</span> <span class="kw">seq</span>(<span class="dt">from =</span> <span class="dv">1</span>, <span class="dt">to =</span> <span class="dv">500</span>, <span class="dt">by =</span> <span class="dv">5</span>)) </a>
<a class="sourceLine" id="cb255-4" data-line-number="4"></a>
<a class="sourceLine" id="cb255-5" data-line-number="5"><span class="kw">set.seed</span>(<span class="dv">1234</span>)</a>
<a class="sourceLine" id="cb255-6" data-line-number="6">knn_reg_cv_<span class="dv">10</span> &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">x =</span> X_train, </a>
<a class="sourceLine" id="cb255-7" data-line-number="7">                       <span class="dt">y =</span> Y_train, </a>
<a class="sourceLine" id="cb255-8" data-line-number="8">                       <span class="dt">method =</span> <span class="st">&quot;knn&quot;</span>, </a>
<a class="sourceLine" id="cb255-9" data-line-number="9">                       <span class="dt">tuneGrid =</span> k_lots, </a>
<a class="sourceLine" id="cb255-10" data-line-number="10">                       <span class="dt">trControl =</span> train_control) </a>
<a class="sourceLine" id="cb255-11" data-line-number="11"></a>
<a class="sourceLine" id="cb255-12" data-line-number="12"><span class="kw">ggplot</span>(knn_reg_cv_<span class="dv">10</span><span class="op">$</span>results, <span class="kw">aes</span>(<span class="dt">x =</span> k, <span class="dt">y =</span> RMSE)) <span class="op">+</span></a>
<a class="sourceLine" id="cb255-13" data-line-number="13"><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb255-14" data-line-number="14"><span class="st">  </span><span class="kw">geom_line</span>()</a></code></pre></div>
<p><img src="_main_files/figure-html/09-mult-choose-k-1.png" width="480" />
Here we see that the smallest <span class="math inline">\(RMSPE\)</span> is from the model where <span class="math inline">\(k\)</span> = 36. Thus the best <span class="math inline">\(k\)</span> for this model, with two predictors, is 36.</p>
<p>Now that we have chosen <span class="math inline">\(k\)</span>, we need to re-train the model on the entire training data set with <span class="math inline">\(k\)</span> = 36, and after that we can use that model to predict on the test data to get our test error. At that point we will also visualize the model predictions overlaid on top of the data. This time the predictions will be a plane in 3-D space, instead of a line in 2-D space, as we have 2 predictors instead of 3.</p>
<div class="sourceCode" id="cb256"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb256-1" data-line-number="1">k =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">k =</span> <span class="dv">36</span>)</a>
<a class="sourceLine" id="cb256-2" data-line-number="2"></a>
<a class="sourceLine" id="cb256-3" data-line-number="3"><span class="kw">set.seed</span>(<span class="dv">1234</span>)</a>
<a class="sourceLine" id="cb256-4" data-line-number="4">knn_mult_reg_final &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">x =</span> X_train, <span class="dt">y =</span> Y_train, <span class="dt">method =</span> <span class="st">&quot;knn&quot;</span>, <span class="dt">tuneGrid =</span> k)</a>
<a class="sourceLine" id="cb256-5" data-line-number="5"></a>
<a class="sourceLine" id="cb256-6" data-line-number="6">test_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(knn_mult_reg_final, X_test)</a>
<a class="sourceLine" id="cb256-7" data-line-number="7">modelvalues &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">obs =</span> Y_test, <span class="dt">pred =</span> test_pred)</a>
<a class="sourceLine" id="cb256-8" data-line-number="8">knn_mult_test_results &lt;-<span class="st"> </span><span class="kw">defaultSummary</span>(modelvalues)</a>
<a class="sourceLine" id="cb256-9" data-line-number="9">knn_mult_test_results[[<span class="dv">1</span>]]</a></code></pre></div>
<pre><code>## [1] 88906.27</code></pre>
<p>This time when we performed k-nn regression on the same data set, but also included number of bathrooms as a predictor we obtained a <span class="math inline">\(RMSPE\)</span> test error of 88906.27. This compares to a RMSPE test error of 87120.62 when we used only house size as the single predictor. Thus in this case, we did not improve the model by adding this additional predictor.</p>
<p>What do the predictions from this model look like overlaid on the data?</p>
<div id="htmlwidget-affe4e87362c786fb942" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-affe4e87362c786fb942">{"x":{"visdat":{"165a939c2d6a2":["function () ","plotlyVisDat"]},"cur_data":"165a939c2d6a2","attrs":{"165a939c2d6a2":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":{},"y":{},"z":{},"type":"scatter3d","mode":"markers","marker":{"size":5,"opacity":0.4,"color":"red"},"inherit":true},"165a939c2d6a2.1":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":{},"type":"surface","x":{},"y":{},"colorbar":{"title":"Price (USD)"},"inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"Scaled house size (square feet)"},"zaxis":{"title":"Price (USD)"},"yaxis":{"title":"Scaled number of bathrooms"}},"hovermode":"closest","showlegend":false,"legend":{"yanchor":"top","y":0.5}},"source":"A","config":{"showSendToCloud":false},"data":[{"x":[0.456485432103372,0.434644023868761,0.465221995397216,0.435190059074626,0.612651500980841,0.602822867275266,0.642683437303432,0.625756345921608,0.496346002131537,0.556955909982583,0.558047980394314,0.460853713750294,0.434097988662896,0.321068701048783,0.61046736015738,0.677083655272944,0.874202364590309,0.491977720484615,0.594086303981422,0.567330578894023,0.6290325571568,0.60937528974565,0.803763823033689,0.625756345921608,0.712029908448322,0.65906449347939,0.866557871708196,1.06749882746662,0.9042343009129,0.806493999063015,0.64868982456795,0.798849506180901,0.935904342853086,0.647051718950354,0.639953261274105,0.6290325571568,0.663432775126312,0.617019782627764,0.87529443500204,0.743699950388508,1.18052811508073,0.691280570625441,0.5110889526899,0.909148617765687,0.868195977325791,1.16633119972823,0.882392892678289,0.697286957889959,0.790658978092922,1.2203886851089,1.14285168587603,0.651420000597276,1.1810741502866,0.804309858239554,0.523101727218936,1.36945629631012,0.868742012531657,0.724042682977358,1.00634288440971,1.0489336304672,1.52343822436412,0.841440252238393,0.555863839570853,0.93535830764722,1.29519550831244,0.789566907681191,1.0882481652895,1.01398737729182,0.614835641804303,1.00634288440971,1.52889857642278,1.29246533228311,1.91985978382232,1.04511138402614,0.922799497912319,1.48794593598288,1.07077503870181,1.71127433518178,1.04565741923201,1.02272394058566,0.894405667207325,1.49286025283567,1.72765539135774,1.96518070590913,1.12155631284728,0.888399279942807,1.87836110817655,1.55401619589258,1.28809705063619,1.8745388617355,1.48739990077702,1.87836110817655,1.91549150217539,1.5835020970093,2.02306043773085,1.92586617108683,0.434097988662896,0.458669572926833,0.436828164692222,0.582619564658251,0.333081475577819,0.827789372091761,0.666162951155639,0.39423741863473,0.897135843236651,0.39423741863473,0.5897180223345,0.573883001364407,0.528016044071723,0.599546656040075,0.484879262808366,0.522555692013071,0.690188500213711,0.691280570625441,0.656334317450064,0.567330578894023,0.790658978092922,0.645959648538623,0.720766471742166,0.609921324951515,0.744792020800239,0.715306119683514,0.549311417100469,0.602822867275266,0.442288516750875,0.493615826102211,0.631216697980261,0.721312506948032,0.785744661240134,0.912424829000879,0.950101258205583,0.690734535419576,0.549857452306335,0.936996413264816,0.998698391527593,0.849084745120507,0.611559430569111,0.620842029068821,0.641045331685836,1.16087084766958,0.982317335351634,0.714214049271783,1.06640675705489,0.394783453840596,0.751344443270622,0.517641375160283,0.719128366124571,0.742607879976777,0.742607879976777,0.831065583326952,0.805401928651284,0.663978810332178,1.19417899522736,0.704931450772073,0.793389154122248,1.36017369781041,0.692918676243037,0.938088483676547,0.642137402097566,0.859459414031947,0.870926153355118,0.85563716759089,1.60698161086151,1.08114970761325,0.902596195295304,1.12264838325901,1.361811803428,0.791751048504652,0.786290696446,0.833795759356279,0.770455675475907,1.63264526553718,0.998698391527593,0.909148617765687,1.67742015241813,1.20400762893294,0.791205013298787,0.686912288978519,1.40604065510309,1.20564573455053,0.605007008098728,0.870926153355118,1.00361270838038,1.03746689114403,1.19581710084496,2.04544788117133,2.16721373207929,1.74294437712197,1.54473359739287,0.680905901714001,1.12920080572939,2.17977254181419,1.85488159432435,2.11916263396314,1.05330191211412,1.6763280820064,1.24059198772591,1.52398425956999,1.38911356372127,2.17540426016727,1.21329022743265,1.67032169474188,1.55401619589258,0.886761174325211,0.264281039638794,0.529654149689319,0.340179933254068,0.508904811866439,0.434644023868761,0.682544007331597,0.553133663541526,0.434097988662896,0.76663342903485,0.341272003665798,0.611559430569111,0.726772859006684,0.553679698747391,0.790658978092922,0.527470008865858,0.425361425369051,0.600638726451805,0.641045331685836,1.08934023570123,0.439012305515683,0.745884091211969,0.491977720484615,0.5897180223345,0.602822867275266,0.53074622010105,0.758988936152736,0.739331668741586,0.425907460574917,0.922799497912319,0.709845767624861,0.768271534652446,0.577159212599598,0.74806823203543,0.73769356312399,0.6290325571568,0.792843118916383,0.634492909215453,0.645413613332758,0.607191148922189,0.657426387861794,0.784106555622539,0.740423739153316,0.624664275509878,0.507812741454708,1.05930829937864,0.686912288978519,1.01344134208596,0.959929891911158,0.949555222999718,0.827789372091761,0.849084745120507,1.02163187017393,0.710937838036591,0.41280261563415,1.10626732708305,0.750798408064757,0.577705247805464,0.648143789362084,0.722950612565628,0.779192238769751,0.916247075441936,0.981771300145769,1.44808536595472,0.713121978860052,1.33614814875233,0.645413613332758,0.633400838803722,0.999244426733458,0.941364694911739,1.18762657275698,0.987231652204422,0.93535830764722,0.795573294945709,0.801579682210227,1.05275587690826,1.04947966567306,0.824513160856569,0.524193797630666,0.851268885943968,1.63373733594891,1.15158824916987,0.832157653738683,0.681451936919866,1.02436204620326,1.35908162739868,1.01071116605663,0.998698391527593,1.21110608660919,0.761173076976197,0.769909640270041,1.28099859295994,1.28154462816581,0.905872406530495,1.41423118319107,1.17670586863967,0.988323722616152,0.876932540619636,1.02163187017393,0.9828633705575,0.918977251471262,0.871472188560983,0.643775507715162,0.89495170241319,1.79154151044398,0.926621744353376,1.0882481652895,1.1384834042291,1.39129770454473,0.952285399029044,1.49067611201221,1.2973796491359,2.34958949083829,2.31846548410397,1.24168405813764,1.8193893059431,1.04019706717335,1.72383314491668,1.65830892021285,2.042717705142,1.45245364760164,1.83304018608974,1.58131795618584,1.10572129187719,0.636677050038914,0.546035205865278,0.457577502515103,0.493615826102211,0.493615826102211,0.470136312250004,0.494707896513941,0.454301291279911,0.436828164692222,0.461945784162025,0.720766471742166,0.608829254539785,0.638315155656509,0.635584979627183,0.732233211065337,0.665616915949773,0.615381677010168,0.694556781860633,0.61046736015738,0.68800435939025,0.690188500213711,0.578797318217194,0.618111853039494,0.596270444804883,0.524193797630666,0.586987846305173,0.779738273975616,0.622480134686416,0.845808533885315,0.769909640270041,0.677083655272944,0.934812272441355,0.862735625267139,0.911332758589148,0.561870226835371,1.17998207987486,0.900958089677708,0.720766471742166,0.638861190862375,0.654696211832468,0.925529673941646,0.631762733186126,0.769909640270041,0.596816480010748,0.966482314381541,0.784106555622539,0.613743571392572,0.621934099480551,0.725134753389089,0.695102817066498,0.59081009274623,0.861643554855408,0.434644023868761,0.792843118916383,0.826151266474165,0.861643554855408,0.947917117382122,0.817960738386186,0.804309858239554,0.53074622010105,0.807586069474746,0.567876614099889,0.780830344387347,0.9828633705575,1.06640675705489,0.672715373626022,0.537298642571433,0.798303470975036,0.73769356312399,0.701109204331016,0.751344443270622,0.709845767624861,0.855091132385025,0.774823957122829,0.893859632001459,0.969758525616733,0.730595105447741,1.23076335402034,0.786836731651865,1.08715609487777,1.2329474948438,1.1526803195816,0.920615357088858,1.29246533228311,1.07132107390767,1.02436204620326,0.915155005030205,1.03692085593816,0.893313596795594,0.998152356321727,0.967028349587407,0.785198626034269,0.829973512915222,0.822329020033108,1.42242171127905,0.653058106214872,0.885123068707615,0.988869757822018,1.50159681612951,1.04292724320268,1.0079809900273,0.842532322650123,0.900958089677708,1.20892194578572,0.787928802063596,0.863827695678869,0.947371082176257,1.14448979149362,0.939180554088277,1.179436044669,0.939726589294143,0.725134753389089,0.886215139119345,1.39511995098578,1.4153232536028,1.11828010161209,0.724588718183223,0.9828633705575,1.18435036152179,1.34160850081099,0.742607879976777,1.61735627977295,1.03091446867364,1.17670586863967,0.84526249867945,0.721858542153897,0.882392892678289,1.42296774648491,1.72710935615187,1.5709432872744,1.61626420936122,0.930443990794433,1.18598846713938,1.14667393231708,1.05057173608479,1.40713272551482,1.42842809854357,0.810862280709937,0.903688265707034,1.73584591944572,1.8996564812053,1.11882613681795,1.56985121686267,1.94716154411558,1.20073141769775,1.22421093154995,1.27444617048956,1.0614924402021,1.72055693368149,1.12101027764141,1.2990177547535,1.35416731054589,1.32031312778224,1.79208754564984,1.84559899582464,0.683636077743327,1.28099859295994,1.28645894501859,1.75823336288619,2.1213467747866,2.40255490580722,0.478326840337983,0.509450847072304,0.4717744178676,0.552041593129796,0.632308768391991,0.596270444804883,0.522009656807205,0.621934099480551,0.577705247805464,0.567876614099889,0.573883001364407,0.633946874009587,0.548219346688739,0.671077268008426,0.633946874009587,0.548765381894604,0.692918676243037,0.64868982456795,0.857275273208486,0.596816480010748,0.624664275509878,0.546035205865278,0.581527494246521,0.658518458273525,0.701655239536882,0.842532322650123,0.760081006564466,0.710937838036591,0.749706337653026,0.754074619299948,1.04674948964374,0.878024611031366,0.733871316682933,0.602822867275266,0.577159212599598,0.894405667207325,0.863827695678869,0.493615826102211,0.731687175859472,0.657426387861794,0.806493999063015,0.817414703180321,0.524193797630666,0.651966035803141,0.567330578894023,0.706569556389669,0.91570104023607,0.672169338420157,0.641591366891701,0.773185851505233,0.878570646237232,0.594632339187287,0.7076616268014,0.434097988662896,0.715306119683514,0.950101258205583,1.02763825743845,0.710937838036591,0.774277921916964,0.798303470975036,0.720220436536301,0.966482314381541,0.888399279942807,0.909148617765687,0.567876614099889,0.524193797630666,0.795027259739844,0.791751048504652,0.741515809565047,0.936450378058951,0.806493999063015,0.920069321882993,0.743699950388508],"y":[0.459221045374271,0.459221045374271,0.459221045374271,0.459221045374271,0.459221045374271,0.918442090748542,0.459221045374271,0.918442090748542,0.918442090748542,0.459221045374271,0.918442090748542,0.459221045374271,0.459221045374271,0.459221045374271,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.459221045374271,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.459221045374271,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,1.83688418149708,0.918442090748542,0.459221045374271,1.37766313612281,0.918442090748542,1.37766313612281,0.918442090748542,0.918442090748542,0.918442090748542,1.37766313612281,0.918442090748542,0.918442090748542,0.918442090748542,1.37766313612281,0.459221045374271,1.37766313612281,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,1.37766313612281,0.918442090748542,0.459221045374271,0.918442090748542,1.37766313612281,0.918442090748542,0.918442090748542,0.918442090748542,0.459221045374271,0.918442090748542,1.37766313612281,1.37766313612281,1.83688418149708,1.37766313612281,0.459221045374271,1.37766313612281,1.37766313612281,0.918442090748542,1.37766313612281,1.37766313612281,1.37766313612281,1.37766313612281,1.37766313612281,1.37766313612281,0.918442090748542,1.37766313612281,1.37766313612281,0.918442090748542,1.37766313612281,1.37766313612281,1.37766313612281,1.37766313612281,1.83688418149708,1.37766313612281,1.83688418149708,1.37766313612281,0.459221045374271,0.459221045374271,0.459221045374271,0.459221045374271,0.459221045374271,0.918442090748542,0.459221045374271,0.459221045374271,0.459221045374271,0.459221045374271,0.459221045374271,0.459221045374271,0.918442090748542,0.459221045374271,0.459221045374271,0.459221045374271,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.459221045374271,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.459221045374271,0.459221045374271,0.918442090748542,0.459221045374271,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.459221045374271,0.918442090748542,0.918442090748542,0.918442090748542,1.37766313612281,0.459221045374271,0.688831568061407,0.459221045374271,0.918442090748542,1.37766313612281,0.459221045374271,0.918442090748542,0.918442090748542,0.459221045374271,0.918442090748542,0.459221045374271,1.14805261343568,1.37766313612281,0.459221045374271,1.14805261343568,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,1.37766313612281,1.14805261343568,1.37766313612281,0.918442090748542,1.37766313612281,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,1.83688418149708,1.14805261343568,0.918442090748542,1.37766313612281,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,1.37766313612281,0.459221045374271,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,1.83688418149708,1.83688418149708,0.918442090748542,1.37766313612281,0.459221045374271,0.459221045374271,1.37766313612281,1.83688418149708,1.37766313612281,1.37766313612281,1.37766313612281,1.14805261343568,1.37766313612281,0.918442090748542,1.37766313612281,0.918442090748542,1.60727365880995,2.29610522687136,1.83688418149708,0.459221045374271,0.459221045374271,0.459221045374271,0.459221045374271,0.459221045374271,0.459221045374271,0.459221045374271,0.459221045374271,0.918442090748542,0.459221045374271,0.918442090748542,0.459221045374271,0.459221045374271,0.918442090748542,0.459221045374271,0.459221045374271,0.918442090748542,0.459221045374271,1.83688418149708,0.459221045374271,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.459221045374271,0.918442090748542,0.918442090748542,0.459221045374271,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.459221045374271,0.918442090748542,0.459221045374271,1.37766313612281,0.459221045374271,0.918442090748542,0.918442090748542,1.37766313612281,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.459221045374271,1.37766313612281,0.918442090748542,0.459221045374271,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,1.37766313612281,0.918442090748542,1.37766313612281,0.918442090748542,0.459221045374271,0.918442090748542,0.918442090748542,1.37766313612281,0.918442090748542,0.459221045374271,0.918442090748542,0.918442090748542,0.918442090748542,1.37766313612281,0.918442090748542,0.918442090748542,0.918442090748542,1.37766313612281,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,1.37766313612281,1.37766313612281,1.14805261343568,0.918442090748542,0.918442090748542,0.918442090748542,1.37766313612281,1.37766313612281,0.918442090748542,1.37766313612281,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.459221045374271,0.918442090748542,0.918442090748542,0.459221045374271,1.37766313612281,0.918442090748542,1.14805261343568,1.37766313612281,0.918442090748542,0.918442090748542,1.37766313612281,0.918442090748542,1.83688418149708,1.37766313612281,0.918442090748542,1.60727365880995,0.918442090748542,1.83688418149708,0.918442090748542,0.918442090748542,1.37766313612281,1.37766313612281,1.37766313612281,0.918442090748542,0.918442090748542,0.918442090748542,0.459221045374271,0.459221045374271,0.459221045374271,0.459221045374271,0.459221045374271,0.459221045374271,0.459221045374271,0.459221045374271,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.459221045374271,0.459221045374271,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.459221045374271,0.918442090748542,0.918442090748542,0.459221045374271,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.459221045374271,0.918442090748542,0.459221045374271,0.918442090748542,0.918442090748542,0.918442090748542,1.37766313612281,1.14805261343568,0.459221045374271,0.459221045374271,0.918442090748542,0.459221045374271,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.459221045374271,1.14805261343568,0.918442090748542,0.918442090748542,0.459221045374271,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,1.37766313612281,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,1.37766313612281,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.459221045374271,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,1.14805261343568,1.14805261343568,0.918442090748542,0.918442090748542,1.37766313612281,0.918442090748542,0.918442090748542,0.918442090748542,1.37766313612281,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,1.37766313612281,1.37766313612281,0.918442090748542,0.918442090748542,1.37766313612281,0.918442090748542,1.37766313612281,0.918442090748542,1.37766313612281,0.918442090748542,1.37766313612281,0.918442090748542,0.918442090748542,0.918442090748542,1.37766313612281,1.37766313612281,1.37766313612281,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,1.14805261343568,1.37766313612281,1.37766313612281,0.918442090748542,0.918442090748542,2.06649470418422,1.37766313612281,0.918442090748542,1.37766313612281,1.83688418149708,0.918442090748542,1.37766313612281,0.918442090748542,0.918442090748542,1.37766313612281,0.918442090748542,1.37766313612281,1.14805261343568,0.918442090748542,1.14805261343568,1.37766313612281,0.459221045374271,1.37766313612281,1.37766313612281,1.14805261343568,1.60727365880995,1.37766313612281,0.459221045374271,0.459221045374271,0.459221045374271,0.459221045374271,0.918442090748542,0.918442090748542,0.459221045374271,0.918442090748542,0.459221045374271,0.918442090748542,0.459221045374271,0.459221045374271,0.459221045374271,0.459221045374271,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.459221045374271,0.688831568061407,0.918442090748542,0.459221045374271,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.459221045374271,0.918442090748542,0.918442090748542,0.459221045374271,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.459221045374271,0.688831568061407,0.459221045374271,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.459221045374271,0.459221045374271,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,1.14805261343568,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.459221045374271,0.459221045374271,0.459221045374271,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542,0.918442090748542],"z":[59222,68880,69307,81900,89921,90895,91002,98937,100309,107502,108750,113263,116250,120000,122000,123000,124100,125000,126640,133000,134555,138750,148750,149593,152000,154000,161500,166357,166357,168000,170000,174250,174313,178480,181000,181872,182716,182750,183200,194818,195000,198000,200000,206000,212864,223058,227887,235000,236000,236685,237800,240122,242638,244500,244960,245918,250000,250134,254200,254200,258000,260000,260014,263500,273750,275086,287417,291000,292024,298000,304037,315537,320000,328360,334150,335750,347029,347650,351300,352000,370000,370500,375000,381300,381942,391000,394470,395000,400186,425000,461000,489332,510000,585000,600000,606238,69000,71000,78000,80000,93675,97750,98000,98000,99000,100000,106716,111000,114800,120108,125000,129000,135500,140000,143500,145000,145000,146000,148500,149000,150000,150000,152000,156000,156000,157788,161653,161829,168000,175000,176250,179000,180000,180400,184500,185000,189000,194000,195000,202500,205000,205000,205000,207000,210000,211500,215000,215500,222381,225000,229665,230000,235000,236250,242000,245000,245000,250000,250000,255000,260000,261000,261800,270000,275000,275000,280000,286013,292000,293993,296769,297500,306500,312500,315000,319789,330000,330000,331000,336000,339000,339000,356000,361745,380000,420000,425000,425000,433500,445000,460000,460000,465000,471750,485000,495000,500500,504000,560000,572500,582000,614000,680000,839000,48000,61500,62050,65000,65000,84000,85000,90000,100000,100000,102750,112500,113000,114000,114000,114750,115000,116100,120000,120000,120108,121500,122000,123000,125000,125573,126714,127000,136500,140800,149600,150000,155500,158000,158000,160000,164000,164000,165000,174000,180000,195000,200345,203000,208000,212000,215000,217500,221000,222900,225500,230000,230000,230522,231200,232000,233641,234000,234500,236073,238000,238861,239700,245000,246000,247480,249862,254172,258000,261000,262500,266000,270000,275336,280000,284686,285000,285000,285000,295000,296000,296056,297359,299940,304000,305000,307000,311328,313138,316630,320000,325000,328578,331500,340000,345746,353767,360552,362305,365000,370000,378000,388000,395100,400000,400000,408431,413000,416767,420000,423000,450000,452000,470000,488750,500000,520000,528000,579093,636000,668365,676200,677048,30000,30000,55422,63000,65000,75000,77000,96140,104250,115000,115500,116000,122000,122500,123000,124000,124000,124413,131750,137760,140000,145000,145000,150000,155000,155800,156142,162000,165000,165000,167293,168000,168000,168750,168750,170250,173000,175000,176250,178000,179000,180000,180000,182000,182587,185074,185833,186785,188335,190000,190000,190000,190000,193000,193500,195000,195000,195000,195000,201000,205000,205000,205878,207000,207744,210000,210944,212500,215000,215000,215000,216033,220000,220000,220702,221250,222000,222750,225000,225000,232500,233500,239000,240000,240971,243500,247000,249000,250000,250000,250000,252000,255000,255000,257200,260000,260000,263500,270000,271000,272700,275000,276500,278000,280000,280000,288000,289000,290000,290000,294173,295000,300000,300000,303000,306000,310000,310000,311518,312000,315000,322000,325000,325500,328370,330000,335000,346375,347225,349000,350000,350000,350000,350000,356200,367463,375000,380578,386222,389000,390000,395500,397000,400000,400000,412500,415000,425000,438000,450000,455000,460000,490000,493000,533000,575000,600000,600000,600000,680000,884790,61000,62000,68566,70000,80000,85500,92000,93600,95000,97750,105000,109000,110000,110000,114800,119000,122000,123675,126854,127059,134000,134000,138000,142000,143012,145846,150000,157500,160000,161250,165000,165750,166000,170000,170000,170725,172000,173056,174000,174250,176850,179500,185000,188000,189000,191250,195500,200000,200000,200000,204750,205000,205900,207973,208318,211500,219000,219794,220000,220000,220000,220000,223000,224000,224000,224252,225000,228000,229027,230000,234000,235301,235738],"type":"scatter3d","mode":"markers","marker":{"color":"red","size":5,"opacity":0.4,"line":{"color":"rgba(31,119,180,1)"}},"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null},{"colorbar":{"title":"Price (USD)","ticklen":2,"len":0.5,"lenmode":"fraction","y":1,"yanchor":"top"},"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666667","rgba(70,19,97,1)"],["0.0833333333333333","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333334","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":true,"z":[[103863.72972973,103863.72972973,103863.72972973,105415.5,106284.25,113107,141356.972222222,159852.888888889,174180.888888889,181545.972222222,190233.194444444,201094.555555556,212653.472222222,212653.472222222,209162.722222222,215781.222222222,215781.222222222,215781.222222222,215781.222222222,217717.324324324,251201.777777778,265910.138888889,308173.527777778,316952.916666667,321625.138888889,326408.472222222,326408.472222222,335211.944444444,344267.5,341174.444444444,341174.444444444,349871.472222222,349871.472222222,363904.805555556,363904.805555556,363904.805555556,363904.805555556,363904.805555556,368460.361111111,381836.416666667,434854.583333333,436718.472222222,447660.083333333,453639.25,449611.472222222,450305.916666667,450305.916666667,450305.916666667,446972.583333333,459194.805555556],[103863.72972973,103863.72972973,103863.72972973,105415.5,106284.25,113107,141356.972222222,159852.888888889,174180.888888889,181545.972222222,190233.194444444,201094.555555556,212653.472222222,210375.027777778,209944.055555556,215781.222222222,215781.222222222,215781.222222222,215781.222222222,254408.388888889,269123.777777778,315410.972222222,308173.527777778,322119.583333333,322180.694444444,326408.472222222,339132.777777778,341174.444444444,341174.444444444,341424.444444444,341424.444444444,349871.472222222,363904.805555556,363904.805555556,363904.805555556,363904.805555556,363904.805555556,368460.361111111,368460.361111111,417973.138888889,435274.027777778,439249.25,447048.972222222,449611.472222222,450305.916666667,450305.916666667,450305.916666667,446972.583333333,459194.805555556,456389.25],[103863.72972973,103863.72972973,103863.72972973,105415.5,106284.25,113107,141356.972222222,159852.888888889,174180.888888889,181545.972222222,190233.194444444,204177.888888889,210375.027777778,210375.027777778,209944.055555556,215781.222222222,215781.222222222,215781.222222222,250353.083333333,265205.055555556,298051.361111111,315410.972222222,321409.638888889,325645.75,322180.694444444,332992.277777778,337960.333333333,341424.444444444,341424.444444444,341424.444444444,341424.444444444,349871.472222222,363904.805555556,363904.805555556,363904.805555556,363904.805555556,368460.361111111,368460.361111111,389993.166666667,423676.805555556,435341.694444444,447048.972222222,449611.472222222,450305.916666667,450305.916666667,450305.916666667,446972.583333333,459194.805555556,456389.25,456389.25],[103863.72972973,103863.72972973,103863.72972973,105415.5,106284.25,113107,141356.972222222,158634.222222222,161375.333333333,180179.864864865,195038.75,198622.333333333,210545.25,210545.25,208225.666666667,217975.666666667,217975.666666667,241132.27027027,254141.5,299001.083333333,300449.361111111,322422.555555556,321409.638888889,339361.722222222,334585.333333333,338168.666666667,343071.444444444,341424.444444444,341424.444444444,341424.444444444,341424.444444444,349871.472222222,363904.805555556,363904.805555556,363904.805555556,372061.555555556,372061.555555556,372061.555555556,422097.638888889,432912.916666667,438638.138888889,449611.472222222,450305.916666667,450305.916666667,450305.916666667,446972.583333333,449250.361111111,456389.25,456389.25,450050.361111111],[103863.72972973,103863.72972973,103863.72972973,105415.5,108395.361111111,112943.055555556,141283.611111111,158300.888888889,164875.333333333,185018.194444444,195907.75,197713.305555556,210545.25,210545.25,208225.666666667,224008.282051282,231352.486486486,262010.805555556,273949.25,303738.638888889,315921.583333333,322422.555555556,336180.194444444,346487.138888889,346003.333333333,338168.666666667,343071.444444444,341424.444444444,341424.444444444,341424.444444444,341424.444444444,361917.111111111,369006,369006,372061.555555556,372061.555555556,372061.555555556,388083.833333333,420989.305555556,438860.361111111,438783.972222222,450305.916666667,450305.916666667,450305.916666667,446972.583333333,449250.361111111,456389.25,456389.25,450050.361111111,450050.361111111],[103863.72972973,103863.72972973,104762.052631579,106296.162162162,108395.361111111,114971.621621622,141283.611111111,158259.222222222,164875.333333333,185018.194444444,195907.75,197713.305555556,207437.972222222,215100.421052632,231015.918918919,231893.055555556,251705.111111111,273394.324324324,292387.972222222,304738.638888889,317073.666666667,327807,333640.333333333,346487.138888889,346003.333333333,338168.666666667,343071.444444444,341424.444444444,347412.916666667,347412.916666667,347412.916666667,369006,369006,372061.555555556,372061.555555556,372061.555555556,390724.472222222,420294.861111111,427710.833333333,432415.916666667,438783.972222222,450305.916666667,446972.583333333,446972.583333333,449250.361111111,444772.583333333,450050.361111111,450050.361111111,450050.361111111,450050.361111111],[104762.052631579,104762.052631579,104762.052631579,106296.162162162,111812.027777778,114971.621621622,143140.810810811,158259.222222222,155296.125,190655.783783784,189696.153846154,200470.694444444,206075.722222222,223745.472222222,238479.138888889,246699.194444444,255519.416666667,275731.444444444,294937.722222222,303199.055555556,320388.805555556,327807,333640.333333333,348116.111111111,349366.111111111,352354.527777778,349436.527777778,347412.916666667,347412.916666667,347412.916666667,353470.083333333,362833.777777778,369006,369006,369006,369006,402152.305555556,425717.777777778,432415.916666667,439006.194444444,433922.861111111,446972.583333333,446972.583333333,444772.583333333,444772.583333333,450050.361111111,450050.361111111,450050.361111111,450050.361111111,450050.361111111],[118226.722222222,118221.166666667,123279.305555556,130296.944444444,140661.361111111,140661.361111111,140661.361111111,140146.583333333,149885.27027027,177456.25,200055.972972973,199071.25,217236.555555556,222290.222222222,232832.027777778,245717.027777778,251772.916666667,273210.416666667,300648.416666667,302429.083333333,325073.333333333,322052.638888889,343269.305555556,342031.972222222,342838.333333333,349298.972222222,345158.75,347135.138888889,347135.138888889,347135.138888889,359525.638888889,369006,369006,369006,369006,393713.361111111,425717.777777778,428223.638888889,439006.194444444,430311.75,433922.861111111,446972.583333333,444772.583333333,444772.583333333,444772.583333333,450050.361111111,450050.361111111,450050.361111111,450050.361111111,450050.361111111],[122984.189189189,128528.026315789,134660.666666667,140661.361111111,140661.361111111,140661.361111111,140661.361111111,142575.054054054,149885.27027027,177456.25,200055.972972973,203481.5,217236.555555556,221908.277777778,232832.027777778,248613.513513514,251772.916666667,273210.416666667,300648.416666667,302429.083333333,325073.333333333,322052.638888889,343269.305555556,342031.972222222,342838.333333333,349298.972222222,345158.75,347135.138888889,347135.138888889,347412.916666667,359525.638888889,369006,369006,369006,369093,414258.388888889,428223.638888889,424707.583333333,434145.083333333,430311.75,449250.361111111,444772.583333333,444772.583333333,447411.472222222,450050.361111111,450050.361111111,450050.361111111,450050.361111111,450050.361111111,450050.361111111],[131410.666666667,138636.72972973,138636.72972973,138636.72972973,138636.72972973,138636.72972973,138636.72972973,141313.25,148215.416666667,177279.944444444,201779.75,202425.944444444,216169.888888889,220561.055555556,236120.083333333,248613.513513514,251772.916666667,273210.416666667,300648.416666667,302429.083333333,325073.333333333,322052.638888889,343269.305555556,342031.972222222,342838.333333333,349298.972222222,345158.75,347135.138888889,347412.916666667,353470.083333333,359525.638888889,369006,369006,369006,382153.916666667,426515,420547.861111111,421681.194444444,430311.75,431945.083333333,444772.583333333,444772.583333333,447411.472222222,447411.472222222,450050.361111111,450050.361111111,450050.361111111,450050.361111111,458055.916666667,452848.972222222],[138636.72972973,138636.72972973,138636.72972973,138654.416666667,138654.416666667,138654.416666667,138654.416666667,140924.361111111,144899.861111111,175314.405405405,202113.083333333,202148.166666667,216031,220561.055555556,236120.083333333,248613.513513514,251772.916666667,273210.416666667,300648.416666667,302429.083333333,325073.333333333,322052.638888889,343269.305555556,342031.972222222,342838.333333333,349298.972222222,345158.75,347412.916666667,347412.916666667,359525.638888889,359525.638888889,369006,369006,379928.916666667,417116.27027027,420547.861111111,424827.027777778,421681.194444444,426056.194444444,436445.083333333,447411.472222222,447411.472222222,447411.472222222,450050.361111111,450050.361111111,453697.648648649,458055.916666667,458055.916666667,462437.861111111,462437.861111111],[138654.416666667,138654.416666667,138654.416666667,138654.416666667,138654.416666667,138654.416666667,138654.416666667,140924.361111111,144899.861111111,175314.405405405,202113.083333333,202148.166666667,216031,220561.055555556,236120.083333333,248613.513513514,251772.916666667,273210.416666667,300648.416666667,302429.083333333,325073.333333333,322052.638888889,343269.305555556,342031.972222222,342838.333333333,349298.972222222,349436.527777778,347412.916666667,347412.916666667,359525.638888889,366822.861111111,373186.555555556,380978.222222222,397970.416666667,416638.472222222,424827.027777778,424827.027777778,441645.083333333,442672.861111111,441617.305555556,447411.472222222,447411.472222222,447411.472222222,450744.805555556,458055.916666667,458055.916666667,462437.861111111,462437.861111111,464987.861111111,467793.416666667],[138654.416666667,138654.416666667,138654.416666667,138654.416666667,138654.416666667,138654.416666667,138654.416666667,140924.361111111,144899.861111111,175314.405405405,202113.083333333,202148.166666667,216031,220561.055555556,236120.083333333,248613.513513514,251772.916666667,273210.416666667,300648.416666667,302429.083333333,325073.333333333,322052.638888889,343269.305555556,342031.972222222,342838.333333333,352354.527777778,349436.527777778,350706.194444444,359733.972222222,366822.861111111,366822.861111111,373186.555555556,396717.638888889,425121.75,425873.194444444,432082.583333333,435110.361111111,442672.861111111,438756.194444444,441617.305555556,447411.472222222,447411.472222222,458055.916666667,458055.916666667,458555.916666667,458555.916666667,464987.861111111,464987.861111111,464987.861111111,467793.416666667],[138654.416666667,138654.416666667,138654.416666667,138654.416666667,138654.416666667,138654.416666667,138654.416666667,140924.361111111,144899.861111111,175314.405405405,202113.083333333,202148.166666667,216031,220561.055555556,236120.083333333,248613.513513514,251772.916666667,273210.416666667,300648.416666667,302429.083333333,325073.333333333,322052.638888889,343269.305555556,342031.972222222,355285.083333333,347450.416666667,352353.194444444,350706.194444444,359733.972222222,366822.861111111,366822.861111111,404050.777777778,412440.166666667,426614.805555556,431185.361111111,434388.138888889,434714.527777778,438756.194444444,438756.194444444,441617.305555556,448861.472222222,450694.805555556,458555.916666667,462439.25,462439.25,464987.861111111,464987.861111111,464987.861111111,467793.416666667,460815.638888889],[138654.416666667,138654.416666667,138654.416666667,138654.416666667,138654.416666667,138654.416666667,138654.416666667,140924.361111111,144899.861111111,175314.405405405,202113.083333333,202148.166666667,216031,220561.055555556,236120.083333333,248613.513513514,251772.916666667,273210.416666667,300648.416666667,302429.083333333,325073.333333333,322052.638888889,345699.861111111,355768.888888889,355285.083333333,347450.416666667,352353.194444444,350706.194444444,366822.861111111,366822.861111111,400257.555555556,406343.388888889,411050.138888889,426581.527777778,435838.138888889,432818.694444444,433145.083333333,438756.194444444,434228.416666667,435311.75,450694.805555556,462439.25,462439.25,462439.25,464987.861111111,464987.861111111,464987.861111111,464987.861111111,467793.416666667,460815.638888889],[138654.416666667,138654.416666667,138654.416666667,138654.416666667,138654.416666667,138654.416666667,138654.416666667,140924.361111111,144899.861111111,175314.405405405,202113.083333333,202148.166666667,216031,220561.055555556,236120.083333333,248613.513513514,251772.916666667,273210.416666667,300648.416666667,305811.054054054,321601.111111111,335144.305555556,342227.638888889,355768.888888889,355285.083333333,347450.416666667,352353.194444444,350706.194444444,366822.861111111,395889,407191.305555556,402218.388888889,408425.138888889,430610.333333333,432818.694444444,428486.388888889,433145.083333333,434797.861111111,434797.861111111,438158.972222222,455592.027777778,462439.25,462439.25,464987.861111111,464987.861111111,464987.861111111,464987.861111111,467793.416666667,472098.972222222,464982.305555556],[138654.416666667,138654.416666667,138654.416666667,138654.416666667,138654.416666667,138654.416666667,138654.416666667,140924.361111111,144899.861111111,175314.405405405,203799.216216216,202704.459459459,218210.947368421,225109.666666667,231726.324324324,242979.621621622,258561.777777778,277771.055555556,297503.75,308882.194444444,326698.513513513,329010.972222222,347157.777777778,347589.305555556,347589.305555556,347450.416666667,352353.194444444,362213.833333333,397866.777777778,409571.861111111,407120.472222222,403496.166666667,414620.194444444,424165.888888889,428486.388888889,422625.277777778,426535,440904.444444444,439460,455592.027777778,462439.25,462439.25,464987.861111111,468515.638888889,468515.638888889,468515.638888889,468515.638888889,472098.972222222,464982.305555556,465398.972222222],[138654.416666667,138654.416666667,138654.416666667,138654.416666667,138654.416666667,138654.416666667,138654.416666667,140924.361111111,144899.861111111,181673.648648649,201451,202704.459459459,221199.055555556,231456.888888889,238502.055555556,251120.027777778,256390.333333333,277771.055555556,297503.75,315673.861111111,327995.694444444,335746.666666667,348026.75,349367.083333333,346589.305555556,338854.805555556,346895.25,376166.222222222,404815.972222222,407033.666666667,409380.888888889,406768.388888889,415814.638888889,413843.861111111,423722.5,423722.5,435293.333333333,440904.444444444,439460,455592.027777778,462439.25,468515.638888889,468515.638888889,468515.638888889,468515.638888889,468515.638888889,472098.972222222,472098.972222222,465398.972222222,470676.75],[138654.416666667,138654.416666667,138654.416666667,138654.416666667,138654.416666667,138654.416666667,138654.416666667,140924.361111111,152618.194444444,181673.648648649,201451,203949.078947368,225701.736842105,231828.111111111,247515.888888889,251120.027777778,256390.333333333,279213.083333333,304114.861111111,315673.861111111,319745.694444444,335746.666666667,337761.527777778,352915.166666667,368457.666666667,364755.75,371899.416666667,364428.972222222,387678.972222222,399213.194444444,405414.916666667,399778.805555556,415722.972222222,423699.083333333,421485.194444444,428244.722222222,435293.333333333,440571.111111111,445960,455979.805555556,468515.638888889,468515.638888889,468515.638888889,468515.638888889,468515.638888889,469348.972222222,472793.416666667,470676.75,470676.75,465537.861111111],[138654.416666667,138654.416666667,138654.416666667,138654.416666667,138654.416666667,138654.416666667,138654.416666667,144248.444444444,154113.944444444,185826.833333333,201374.631578947,208768.513513514,236644.888888889,248314.305555556,263436.611111111,270112.416666667,281266.083333333,297877.944444444,292949,311072.611111111,321554.138888889,336850.611111111,336247.638888889,337214.666666667,351949.222222222,370218.444444444,373053.222222222,377558.777777778,364762.305555556,377276.194444444,396825,401064.916666667,420350.805555556,423699.083333333,426790.75,430071.111111111,438085,440571.111111111,445536.388888889,469585.083333333,469585.083333333,469585.083333333,469585.083333333,469585.083333333,469348.972222222,470821.194444444,470676.75,470676.75,465537.861111111,464704.527777778],[138654.416666667,138654.416666667,138654.416666667,138654.416666667,138654.416666667,144314.138888889,150529.416666667,148922.055555556,156556.810810811,195674.055555556,213161.361111111,220754.081081081,250461.432432432,273012.361111111,275764.263157895,281123.868421053,286762.888888889,295137.888888889,300031.916666667,309309.694444444,333009.305555556,336822.833333333,329227.611111111,339058.416666667,345961.194444444,357996.222222222,368325.444444444,385442.111111111,369986.944444444,375971.333333333,385140.777777778,413042.083333333,420944.555555556,429551.861111111,433485.194444444,428922.694444444,439890.555555556,443175.277777778,448564.166666667,469585.083333333,469585.083333333,469585.083333333,469585.083333333,474598.972222222,469348.972222222,479737.861111111,472065.638888889,464704.527777778,471598.972222222,471598.972222222],[138654.416666667,138654.416666667,140731.324324324,140731.324324324,144314.138888889,152737.75,152737.75,148922.055555556,175406.567567568,213686.75,218573.611111111,237307.083333333,268103.139534884,276031.72972973,277558.487179487,286762.888888889,286762.888888889,290568.444444444,306073.583333333,309309.694444444,330663.111111111,336392.277777778,332194.277777778,334716.75,349482.027777778,357996.222222222,368325.444444444,378811.555555556,367200.444444444,375971.333333333,385140.777777778,403262.027777778,414731.027777778,422747,430421.305555556,432776.861111111,433985.194444444,443744.722222222,450230.833333333,468308.333333333,476918.416666667,476918.416666667,476918.416666667,476682.305555556,479737.861111111,472065.638888889,464204.527777778,464204.527777778,464204.527777778,469848.972222222],[140731.324324324,140731.324324324,140731.324324324,146966.916666667,151904.416666667,152737.75,152737.75,169045.666666667,180618.105263158,217270.083333333,242230.9,266120.702702703,269831.468085106,286762.888888889,286762.888888889,286762.888888889,286762.888888889,292920.805555556,298629.138888889,309309.694444444,333864.5,333178.388888889,332438.333333333,343412,349482.027777778,357996.222222222,360997.25,378811.555555556,367200.444444444,371943.555555556,374168.555555556,397289.805555556,414731.027777778,422747,435340.75,438028.25,442383.805555556,448143.333333333,457629.444444444,470697.222222222,470697.222222222,470697.222222222,473682.305555556,479321.194444444,474454.527777778,464204.527777778,464204.527777778,464204.527777778,469848.972222222,469848.972222222],[140731.324324324,144383.583333333,146966.916666667,151904.416666667,152737.75,164725.236842105,172240.666666667,173524.833333333,192296.162162162,245657,262773.277777778,272152.425,286762.888888889,286762.888888889,286762.888888889,286762.888888889,287290.666666667,298962.472222222,298629.138888889,305129.138888889,323942.277777778,331283.944444444,327294.527777778,343412,349482.027777778,352538.916666667,360997.25,374978.222222222,371089.333333333,366769.944444444,374168.555555556,397289.805555556,414731.027777778,431385.888888889,435340.75,438028.25,442383.805555556,448467.138888889,456115.555555556,470401.388888889,475040.277777778,480928.138888889,480692.027777778,480664.25,471603.138888889,471603.138888889,477247.583333333,477247.583333333,477247.583333333,477247.583333333],[144383.583333333,146966.916666667,151904.416666667,152737.75,167552.794871795,172240.666666667,175720.648648649,182813,221567.473684211,255909.810810811,269670.236842105,286762.888888889,286762.888888889,286762.888888889,286762.888888889,282901.777777778,292332.333333333,298962.472222222,308666.189189189,322408.944444444,322020.055555556,321712.222222222,322600.083333333,328203.666666667,344759.805555556,348802.805555556,367002.805555556,374978.222222222,371089.333333333,366769.944444444,376067.583333333,400650.916666667,421579.277777778,435340.75,435340.75,438028.25,442383.805555556,448467.138888889,451696.305555556,470401.388888889,477568.055555556,479997.583333333,482330.916666667,472714.25,472714.25,472714.25,472714.25,472714.25,472714.25,477247.583333333],[151904.416666667,151904.416666667,159177.27027027,172240.666666667,172240.666666667,175720.648648649,182502.837837838,204666.459459459,238484.820512821,260357.432432432,286762.888888889,286762.888888889,286762.888888889,282901.777777778,282901.777777778,284735.111111111,292795.805555556,294309.694444444,316561.722222222,322408.944444444,318658.944444444,321712.222222222,322600.083333333,328203.666666667,344759.805555556,348802.805555556,367002.805555556,374978.222222222,371089.333333333,366769.944444444,374557.444444444,395789.805555556,415001.861111111,435340.75,435340.75,438028.25,442383.805555556,448467.138888889,451696.305555556,467148.805555556,475161.666666667,482330.916666667,472714.25,472714.25,472714.25,472714.25,472714.25,472714.25,472714.25,472714.25],[151904.416666667,164725.236842105,172240.666666667,172240.666666667,175720.648648649,180627.916666667,196985.138888889,257558.783783784,252953.853658537,277683.666666667,298957.333333333,298957.333333333,282901.777777778,284735.111111111,284735.111111111,286345.805555556,292795.805555556,303012.472222222,316561.722222222,322408.944444444,318658.944444444,321712.222222222,322600.083333333,328203.666666667,344759.805555556,348802.805555556,367002.805555556,374978.222222222,371089.333333333,376221.333333333,374557.444444444,395789.805555556,415001.861111111,431267.833333333,435340.75,438028.25,448467.138888889,448467.138888889,452793.527777778,463500.833333333,479161.666666667,479161.666666667,472878.333333333,472714.25,472714.25,472714.25,472714.25,472714.25,479886.472222222,479886.472222222],[167552.794871795,172240.666666667,174462.888888889,175720.648648649,180627.916666667,216901.805555556,247737.567567568,266099.305555556,262062.25,298957.333333333,298707.333333333,298707.333333333,298707.333333333,298146.222222222,281609.694444444,284890.513513513,300929.138888889,314943.666666667,316561.722222222,322408.944444444,318658.944444444,321712.222222222,322600.083333333,328203.666666667,344759.805555556,348802.805555556,367002.805555556,374978.222222222,371089.333333333,376221.333333333,374557.444444444,397560.638888889,415001.861111111,423017.833333333,435340.75,438028.25,448467.138888889,453154.638888889,462986.944444444,463500.833333333,479161.666666667,472878.333333333,472878.333333333,471072.777777778,472714.25,479886.472222222,479886.472222222,479886.472222222,479886.472222222,479886.472222222],[172240.666666667,175720.648648649,195322.361111111,198421.756756757,209947.361111111,240331.378378378,247737.567567568,278123.777777778,294790.666666667,294790.666666667,298707.333333333,298707.333333333,298340.666666667,298340.666666667,298215.25,302012.472222222,329484.305555556,314943.666666667,316561.722222222,322408.944444444,318658.944444444,321712.222222222,322600.083333333,328203.666666667,344759.805555556,348802.805555556,367002.805555556,374978.222222222,371478.222222222,376221.333333333,381300.5,397560.638888889,415001.861111111,423017.833333333,432340.75,439543.527777778,449265.75,460764.722222222,465389.722222222,469028.611111111,474822.777777778,471072.777777778,471072.777777778,471072.777777778,471072.777777778,479886.472222222,479886.472222222,479886.472222222,479886.472222222,479886.472222222],[195322.361111111,195322.361111111,198421.756756757,209947.361111111,241229.472222222,251598.333333333,268196.527777778,280597.736842105,294790.666666667,292146.222222222,292146.222222222,293951.777777778,294099,295090.25,294826.361111111,308368.027777778,310451.361111111,321701.361111111,311249.243243243,322408.944444444,318658.944444444,321712.222222222,322600.083333333,328203.666666667,344759.805555556,348802.805555556,367002.805555556,383089.333333333,371478.222222222,376221.333333333,381300.5,399018.972222222,418294.555555556,427687.972222222,428918.527777778,436786.583333333,444599.083333333,455334.166666667,455098.055555556,459389.722222222,470656.111111111,471072.777777778,471072.777777778,471072.777777778,471072.777777778,471072.777777778,479886.472222222,479886.472222222,479886.472222222,479886.472222222],[195322.361111111,198421.756756757,209947.361111111,226135.432432432,251598.333333333,252131.666666667,278375.166666667,294790.666666667,292146.222222222,290035.111111111,290035.111111111,291793.444444444,289048.583333333,293562.472222222,292128.351351351,310451.361111111,310451.361111111,310451.361111111,325456.527777778,307658.944444444,312158.944444444,315020.055555556,318808.416666667,327842.555555556,344759.805555556,348802.805555556,365275.027777778,383089.333333333,371478.222222222,371846.333333333,384367.166666667,405685.638888889,422794.555555556,433762.972222222,427224.083333333,443161.583333333,449099.083333333,455098.055555556,455098.055555556,456625.833333333,465225.555555556,462670,462670,462670,472064.444444444,472064.444444444,472064.444444444,479886.472222222,479205.916666667,479205.916666667],[198421.756756757,209947.361111111,224509.027777778,251598.333333333,252131.666666667,273190.861111111,290312.888888889,290035.111111111,290035.111111111,290035.111111111,291793.444444444,289048.583333333,289048.583333333,293298.583333333,310451.361111111,310451.361111111,310451.361111111,310451.361111111,325456.527777778,323360.333333333,312158.944444444,315020.055555556,315225.083333333,325481.444444444,339884.805555556,345177.805555556,365275.027777778,384081,368625.833333333,379805.361111111,384367.166666667,411771.25,425395.25,429437.972222222,434564.361111111,443161.583333333,449099.083333333,455098.055555556,455098.055555556,459431.388888889,465225.555555556,465225.555555556,462670,462670,462670,466203.333333333,473925.555555556,473925.555555556,473925.555555556,481817.027777778],[209947.361111111,224509.027777778,241368.361111111,249931.666666667,252131.666666667,277684.324324324,290035.111111111,290035.111111111,290035.111111111,291793.444444444,289048.583333333,289048.583333333,293562.472222222,293298.583333333,310451.361111111,310451.361111111,310451.361111111,310451.361111111,325456.527777778,323360.333333333,326797.833333333,315020.055555556,315225.083333333,325481.444444444,331551.472222222,343121.222222222,360922.666666667,378970.702702703,379131.75,379805.361111111,388839.388888889,411687.916666667,426089.694444444,429437.972222222,443161.583333333,443161.583333333,446904.638888889,455098.055555556,454875.833333333,451453.611111111,459781.111111111,459781.111111111,459781.111111111,459781.111111111,459781.111111111,466203.333333333,466203.333333333,466203.333333333,473925.555555556,473925.555555556],[213290.694444444,236817.864864865,247987.222222222,252131.666666667,269593.638888889,290035.111111111,290035.111111111,290035.111111111,291793.444444444,291420.648648649,289048.583333333,293562.472222222,294604.138888889,293298.583333333,310451.361111111,310451.361111111,310451.361111111,310451.361111111,325456.527777778,323360.333333333,326797.833333333,330547.833333333,328323.333333333,325481.444444444,330488.972222222,344218.444444444,357978.222222222,378386.555555556,376944.25,385888.694444444,397311.611111111,404233.333333333,425087.27027027,429604.972222222,446800.472222222,444606.027777778,449264.722222222,454875.833333333,457681.388888889,451453.611111111,459781.111111111,459781.111111111,459781.111111111,459781.111111111,459781.111111111,459781.111111111,466203.333333333,466203.333333333,466203.333333333,473925.555555556],[237618.361111111,247987.222222222,254242.777777778,264754.75,280590.666666667,290035.111111111,290035.111111111,291793.444444444,291420.648648649,289048.583333333,289048.583333333,293562.472222222,294604.138888889,310451.361111111,310451.361111111,310451.361111111,310451.361111111,310451.361111111,325456.527777778,323360.333333333,326797.833333333,330547.833333333,328323.333333333,329744.527777778,334738.972222222,351412.888888889,357978.222222222,372692.111111111,376110.916666667,381277.583333333,404063.888888889,404831.583333333,428872.972222222,429604.972222222,446800.472222222,444606.027777778,449264.722222222,454875.833333333,457681.388888889,451453.611111111,459781.111111111,459781.111111111,459781.111111111,459781.111111111,459781.111111111,459781.111111111,459781.111111111,466203.333333333,466203.333333333,466203.333333333],[247987.222222222,248403.888888889,254242.777777778,269593.638888889,290035.111111111,290035.111111111,291793.444444444,291420.648648649,289048.583333333,289048.583333333,293562.472222222,291912.135135135,294604.138888889,310451.361111111,310451.361111111,310451.361111111,310451.361111111,310451.361111111,325456.527777778,323360.333333333,326797.833333333,330547.833333333,327466.111111111,329744.527777778,337838.111111111,351412.888888889,357978.222222222,373442.861111111,370777.583333333,392416.472222222,399273.25,415539.916666667,428872.972222222,436372.405405405,444606.027777778,444606.027777778,449264.722222222,457681.388888889,457681.388888889,451453.611111111,459781.111111111,459781.111111111,459781.111111111,459781.111111111,459781.111111111,459781.111111111,459781.111111111,459781.111111111,466203.333333333,466203.333333333],[248403.888888889,254242.777777778,269593.638888889,282057.333333333,290035.111111111,291793.444444444,291420.648648649,289048.583333333,289048.583333333,293562.472222222,293562.472222222,294604.138888889,310451.361111111,310451.361111111,310451.361111111,310451.361111111,310451.361111111,310451.361111111,325456.527777778,323360.333333333,326797.833333333,332006.166666667,327466.111111111,348163.972222222,353115.888888889,351412.888888889,362325.444444444,373442.861111111,376972.027777778,388918.75,406731.583333333,411335.055555556,428872.972222222,438341.083333333,444606.027777778,444606.027777778,449723.055555556,457681.388888889,457681.388888889,451453.611111111,459781.111111111,459781.111111111,459781.111111111,459781.111111111,459781.111111111,459781.111111111,459781.111111111,459781.111111111,459781.111111111,466203.333333333],[250587.567567568,264754.75,268335.918918919,290035.111111111,291793.444444444,291420.648648649,289048.583333333,289048.583333333,293562.472222222,293562.472222222,293562.472222222,294604.138888889,310451.361111111,310451.361111111,310451.361111111,310451.361111111,310451.361111111,310451.361111111,325456.527777778,323360.333333333,328228.388888889,332006.166666667,332152.222222222,348163.972222222,357725.388888889,368468.444444444,373220.25,378894.25,389583.138888889,399057.638888889,412142,419807.277777778,428853.527777778,446396.638888889,455049.444444444,455049.444444444,455507.777777778,463792.5,457681.388888889,451453.611111111,459781.111111111,459781.111111111,459781.111111111,459781.111111111,459781.111111111,459781.111111111,459781.111111111,459781.111111111,459781.111111111,459781.111111111],[254242.777777778,269593.638888889,272609.138888889,291793.444444444,291420.648648649,289048.583333333,289048.583333333,289048.583333333,293562.472222222,293562.472222222,294604.138888889,294604.138888889,310451.361111111,310451.361111111,310451.361111111,310451.361111111,310451.361111111,310451.361111111,325456.527777778,322661.405405405,328228.388888889,332006.166666667,346716.5,357820.222222222,358947.611111111,378315.666666667,392619.702702703,371222.027777778,389583.138888889,405307.638888889,412142,412165.611111111,425853.527777778,444646.638888889,455049.444444444,455507.777777778,455507.777777778,463792.5,457786.944444444,454092.5,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,459781.111111111],[269593.638888889,272609.138888889,281488.216216216,291420.648648649,289048.583333333,289048.583333333,289048.583333333,293562.472222222,293562.472222222,293562.472222222,294604.138888889,310451.361111111,310451.361111111,310451.361111111,310451.361111111,310451.361111111,310451.361111111,310451.361111111,322604.138888889,322661.405405405,341700.611111111,348422.833333333,349188.722222222,357820.222222222,365205.944444444,378315.666666667,388942.472222222,371222.027777778,399444.25,405307.638888889,412142,417312.833333333,431936.861111111,444646.638888889,449874.444444444,455507.777777778,455507.777777778,463792.5,457786.944444444,454092.5,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778],[269593.638888889,277072.777777778,291420.648648649,289048.583333333,289048.583333333,289048.583333333,293562.472222222,293562.472222222,293562.472222222,294604.138888889,294604.138888889,310451.361111111,310451.361111111,310451.361111111,310451.361111111,310451.361111111,310451.361111111,309687.472222222,324826.361111111,331470.416666667,350006.166666667,350561.722222222,349188.722222222,357820.222222222,365205.944444444,386241.083333333,388942.472222222,390472.027777778,403333.138888889,405307.638888889,412142,417312.833333333,425943.805555556,444646.638888889,450332.777777778,455507.777777778,455507.777777778,463792.5,457786.944444444,454092.5,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778],[272609.138888889,281501.72972973,289048.583333333,289048.583333333,289048.583333333,289048.583333333,293562.472222222,293562.472222222,293562.472222222,294604.138888889,310451.361111111,310451.361111111,310451.361111111,310451.361111111,310451.361111111,320312.472222222,319695.918918919,319695.918918919,324826.361111111,331159.694444444,348172.833333333,357839.5,349188.722222222,362320.222222222,371969.833333333,385893.861111111,388942.472222222,395305.361111111,419366.472222222,405307.638888889,414326.027777778,418535.055555556,425943.805555556,433979.972222222,450332.777777778,449896.666666667,455507.777777778,463792.5,457786.944444444,454092.5,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778],[277072.777777778,291420.648648649,289048.583333333,289048.583333333,289048.583333333,293562.472222222,293562.472222222,293562.472222222,294604.138888889,294604.138888889,310451.361111111,310451.361111111,320312.472222222,320312.472222222,320312.472222222,319695.918918919,319695.918918919,319695.918918919,326604.138888889,328937.472222222,348172.833333333,357839.5,356327.611111111,362320.222222222,376619.138888889,385893.861111111,388942.472222222,395305.361111111,419366.472222222,406905.888888889,414326.027777778,418535.055555556,425943.805555556,438034.5,450332.777777778,449896.666666667,449896.666666667,463792.5,457786.944444444,454092.5,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778],[281501.72972973,289048.583333333,289048.583333333,289048.583333333,293562.472222222,293562.472222222,293562.472222222,293562.472222222,294604.138888889,320312.472222222,320312.472222222,320312.472222222,320312.472222222,320312.472222222,319695.918918919,319695.918918919,319695.918918919,313826.361111111,331831.054054054,328937.472222222,357624.222222222,357839.5,356327.611111111,365960.138888889,376619.138888889,385893.861111111,389492.861111111,395305.361111111,419366.472222222,421003.111111111,414326.027777778,418535.055555556,425943.805555556,438034.5,442638.333333333,449896.666666667,449896.666666667,450425.833333333,457786.944444444,454092.5,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778],[289048.583333333,289048.583333333,289048.583333333,289048.583333333,293562.472222222,293562.472222222,293562.472222222,294604.138888889,305020.243243243,320312.472222222,320312.472222222,320312.472222222,320312.472222222,319695.918918919,319695.918918919,319695.918918919,313826.361111111,322284.694444444,331831.054054054,328937.472222222,357624.222222222,363922.833333333,364049.027777778,365725.944444444,376619.138888889,396102.194444444,389492.861111111,395305.361111111,424340.972222222,421003.111111111,433937.138888889,418535.055555556,419878.111111111,438034.5,442638.333333333,443535.555555556,449896.666666667,450425.833333333,446953.611111111,454092.5,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778],[289048.583333333,289048.583333333,289048.583333333,293562.472222222,293562.472222222,293562.472222222,307034.694444444,305020.243243243,305020.243243243,320312.472222222,320312.472222222,320312.472222222,319695.918918919,319695.918918919,313826.361111111,313826.361111111,313826.361111111,322284.694444444,331831.054054054,338354.138888889,361775.972222222,363922.833333333,364049.027777778,370969,382656.459459459,390102.194444444,389492.861111111,395305.361111111,424340.972222222,421003.111111111,433937.138888889,418535.055555556,419878.111111111,438034.5,442638.333333333,443535.555555556,443535.555555556,450425.833333333,446953.611111111,452236.944444444,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778],[289048.583333333,289048.583333333,293562.472222222,293562.472222222,300993.027777778,300993.027777778,307034.694444444,305020.243243243,320312.472222222,320312.472222222,320312.472222222,319695.918918919,319695.918918919,313826.361111111,313826.361111111,313826.361111111,321812.472222222,322284.694444444,331831.054054054,338354.138888889,361775.972222222,370940.611111111,368329.083333333,370969,387202.472222222,390102.194444444,389492.861111111,395069.25,424340.972222222,421003.111111111,433937.138888889,432208.666666667,419878.111111111,438034.5,442638.333333333,443535.555555556,443535.555555556,450425.833333333,446953.611111111,452236.944444444,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778],[289048.583333333,289048.583333333,300993.027777778,300993.027777778,300993.027777778,307034.694444444,305020.243243243,305020.243243243,320312.472222222,319695.918918919,319695.918918919,319695.918918919,313826.361111111,313826.361111111,313826.361111111,321812.472222222,321812.472222222,322284.694444444,332548.583333333,342243.027777778,361775.972222222,370940.611111111,368329.083333333,379607.888888889,387202.472222222,390102.194444444,389492.861111111,395069.25,424340.972222222,421003.111111111,433937.138888889,432208.666666667,419878.111111111,438034.5,442638.333333333,443535.555555556,443535.555555556,437856.388888889,446953.611111111,452236.944444444,467870,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778],[301362.888888889,300993.027777778,300993.027777778,300993.027777778,300993.027777778,307034.694444444,305020.243243243,320312.472222222,319695.918918919,319695.918918919,313826.361111111,313826.361111111,313826.361111111,313826.361111111,321812.472222222,321812.472222222,321812.472222222,322284.694444444,332548.583333333,342243.027777778,365104.138888889,372293.388888889,374127.694444444,380699.567567568,387202.472222222,390102.194444444,392533.5,395069.25,424340.972222222,431475.333333333,439778.805555556,432208.666666667,431517,438034.5,442638.333333333,443535.555555556,443535.555555556,437856.388888889,443014.722222222,452236.944444444,467870,467870,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778],[300993.027777778,300993.027777778,300993.027777778,300993.027777778,307034.694444444,305020.243243243,305020.243243243,319695.918918919,319695.918918919,313826.361111111,313826.361111111,313826.361111111,313826.361111111,313826.361111111,321812.472222222,321812.472222222,321812.472222222,322284.694444444,339006.916666667,342243.027777778,365104.138888889,372293.388888889,383988.805555556,380699.567567568,387202.472222222,390102.194444444,392533.5,395069.25,424340.972222222,431475.333333333,439778.805555556,432208.666666667,431517,438034.5,442638.333333333,443535.555555556,443535.555555556,437856.388888889,443014.722222222,452236.944444444,467870,467870,467870,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778,471397.777777778]],"type":"surface","x":[0.264,0.30765306122449,0.35130612244898,0.394959183673469,0.438612244897959,0.482265306122449,0.525918367346939,0.569571428571429,0.613224489795918,0.656877551020408,0.700530612244898,0.744183673469388,0.787836734693878,0.831489795918367,0.875142857142857,0.918795918367347,0.962448979591837,1.00610204081633,1.04975510204082,1.09340816326531,1.1370612244898,1.18071428571429,1.22436734693878,1.26802040816327,1.31167346938776,1.35532653061225,1.39897959183673,1.44263265306122,1.48628571428571,1.5299387755102,1.57359183673469,1.61724489795918,1.66089795918367,1.70455102040816,1.74820408163265,1.79185714285714,1.83551020408163,1.87916326530612,1.92281632653061,1.9664693877551,2.01012244897959,2.05377551020408,2.09742857142857,2.14108163265306,2.18473469387755,2.22838775510204,2.27204081632653,2.31569387755102,2.35934693877551,2.403],"y":[0.459,0.496489795918367,0.533979591836735,0.571469387755102,0.608959183673469,0.646448979591837,0.683938775510204,0.721428571428571,0.758918367346939,0.796408163265306,0.833897959183673,0.871387755102041,0.908877551020408,0.946367346938775,0.983857142857143,1.02134693877551,1.05883673469388,1.09632653061224,1.13381632653061,1.17130612244898,1.20879591836735,1.24628571428571,1.28377551020408,1.32126530612245,1.35875510204082,1.39624489795918,1.43373469387755,1.47122448979592,1.50871428571429,1.54620408163265,1.58369387755102,1.62118367346939,1.65867346938775,1.69616326530612,1.73365306122449,1.77114285714286,1.80863265306122,1.84612244897959,1.88361224489796,1.92110204081633,1.95859183673469,1.99608163265306,2.03357142857143,2.0710612244898,2.10855102040816,2.14604081632653,2.1835306122449,2.22102040816326,2.25851020408163,2.296],"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>We can see that the predictions in this case, where we have 2 predictors, form a plane instead of a line. Because the newly added predictor, number of bathrooms, is correlated with price (USD) (meaning as price changes, so does number of bathrooms) we get additional and useful information for making our predictions. For example, in this model we would predict that the cost of a house with a scaled house size of ~ 0.52 and a scaled number bathrooms of ~ 1.13 would cost less than the same sized house with a higher scaled number bathrooms (e.g., ~ 2.11). Without having the additional predictor of number of bathrooms, we would predict the same price for these two houses.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="classification-continued.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="regression2.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/07-regression1.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
