<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Classification II: evaluation &amp; tuning | Introduction to Data Science</title>
  <meta name="description" content="This is an open source textbook for teaching introductory data science." />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Classification II: evaluation &amp; tuning | Introduction to Data Science" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is an open source textbook for teaching introductory data science." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Classification II: evaluation &amp; tuning | Introduction to Data Science" />
  
  <meta name="twitter:description" content="This is an open source textbook for teaching introductory data science." />
  

<meta name="author" content="Tiffany-Anne Timbers" />
<meta name="author" content="Trevor Campbell" />
<meta name="author" content="Melissa Lee" />


<meta name="date" content="2020-12-16" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="classification.html"/>
<link rel="next" href="regression1.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.9.2.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.52.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.52.2/plotly-latest.min.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Data Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> R, Jupyter, and the tidyverse</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#chapter-learning-objectives"><i class="fa fa-check"></i><b>1.1</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#jupyter-notebooks"><i class="fa fa-check"></i><b>1.2</b> Jupyter notebooks</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#loading-a-spreadsheet-like-dataset"><i class="fa fa-check"></i><b>1.3</b> Loading a spreadsheet-like dataset</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#assigning-value-to-a-data-frame"><i class="fa fa-check"></i><b>1.4</b> Assigning value to a data frame</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#creating-subsets-of-data-frames-with-select-filter"><i class="fa fa-check"></i><b>1.5</b> Creating subsets of data frames with <code>select</code> &amp; <code>filter</code></a><ul>
<li class="chapter" data-level="1.5.1" data-path="index.html"><a href="index.html#using-select-to-extract-multiple-columns"><i class="fa fa-check"></i><b>1.5.1</b> Using <code>select</code> to extract multiple columns</a></li>
<li class="chapter" data-level="1.5.2" data-path="index.html"><a href="index.html#using-select-to-extract-a-range-of-columns"><i class="fa fa-check"></i><b>1.5.2</b> Using <code>select</code> to extract a range of columns</a></li>
<li class="chapter" data-level="1.5.3" data-path="index.html"><a href="index.html#using-filter-to-extract-a-single-row"><i class="fa fa-check"></i><b>1.5.3</b> Using <code>filter</code> to extract a single row</a></li>
<li class="chapter" data-level="1.5.4" data-path="index.html"><a href="index.html#using-filter-to-extract-rows-with-values-above-a-threshold"><i class="fa fa-check"></i><b>1.5.4</b> Using <code>filter</code> to extract rows with values above a threshold</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#exploring-data-with-visualizations"><i class="fa fa-check"></i><b>1.6</b> Exploring data with visualizations</a><ul>
<li class="chapter" data-level="1.6.1" data-path="index.html"><a href="index.html#using-ggplot-to-create-a-scatter-plot"><i class="fa fa-check"></i><b>1.6.1</b> Using <code>ggplot</code> to create a scatter plot</a></li>
<li class="chapter" data-level="1.6.2" data-path="index.html"><a href="index.html#formatting-ggplot-objects"><i class="fa fa-check"></i><b>1.6.2</b> Formatting ggplot objects</a></li>
<li class="chapter" data-level="1.6.3" data-path="index.html"><a href="index.html#changing-the-units"><i class="fa fa-check"></i><b>1.6.3</b> Changing the units</a></li>
<li class="chapter" data-level="1.6.4" data-path="index.html"><a href="index.html#coloring-points-by-group"><i class="fa fa-check"></i><b>1.6.4</b> Coloring points by group</a></li>
<li class="chapter" data-level="1.6.5" data-path="index.html"><a href="index.html#putting-it-all-together"><i class="fa fa-check"></i><b>1.6.5</b> Putting it all together</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="reading.html"><a href="reading.html"><i class="fa fa-check"></i><b>2</b> Reading in data locally and from the web</a><ul>
<li class="chapter" data-level="2.1" data-path="reading.html"><a href="reading.html#overview"><i class="fa fa-check"></i><b>2.1</b> Overview</a></li>
<li class="chapter" data-level="2.2" data-path="reading.html"><a href="reading.html#chapter-learning-objectives-1"><i class="fa fa-check"></i><b>2.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="2.3" data-path="reading.html"><a href="reading.html#absolute-and-relative-file-paths"><i class="fa fa-check"></i><b>2.3</b> Absolute and relative file paths</a></li>
<li class="chapter" data-level="2.4" data-path="reading.html"><a href="reading.html#reading-tabular-data-from-a-plain-text-file-into-r"><i class="fa fa-check"></i><b>2.4</b> Reading tabular data from a plain text file into R</a><ul>
<li class="chapter" data-level="2.4.1" data-path="reading.html"><a href="reading.html#skipping-rows-when-reading-in-data"><i class="fa fa-check"></i><b>2.4.1</b> Skipping rows when reading in data</a></li>
<li class="chapter" data-level="2.4.2" data-path="reading.html"><a href="reading.html#read_delim-as-a-more-flexible-method-to-get-tabular-data-into-r"><i class="fa fa-check"></i><b>2.4.2</b> <code>read_delim</code> as a more flexible method to get tabular data into R</a></li>
<li class="chapter" data-level="2.4.3" data-path="reading.html"><a href="reading.html#reading-tabular-data-directly-from-a-url"><i class="fa fa-check"></i><b>2.4.3</b> Reading tabular data directly from a URL</a></li>
<li class="chapter" data-level="2.4.4" data-path="reading.html"><a href="reading.html#previewing-a-data-file-before-reading-it-into-r"><i class="fa fa-check"></i><b>2.4.4</b> Previewing a data file before reading it into R</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="reading.html"><a href="reading.html#reading-data-from-an-microsoft-excel-file"><i class="fa fa-check"></i><b>2.5</b> Reading data from an Microsoft Excel file</a></li>
<li class="chapter" data-level="2.6" data-path="reading.html"><a href="reading.html#reading-data-from-a-database"><i class="fa fa-check"></i><b>2.6</b> Reading data from a database</a><ul>
<li class="chapter" data-level="2.6.1" data-path="reading.html"><a href="reading.html#connecting-to-a-database"><i class="fa fa-check"></i><b>2.6.1</b> Connecting to a database</a></li>
<li class="chapter" data-level="2.6.2" data-path="reading.html"><a href="reading.html#interacting-with-a-database"><i class="fa fa-check"></i><b>2.6.2</b> Interacting with a database</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="reading.html"><a href="reading.html#writing-data-from-r-to-a-.csv-file"><i class="fa fa-check"></i><b>2.7</b> Writing data from R to a <code>.csv</code> file</a></li>
<li class="chapter" data-level="2.8" data-path="reading.html"><a href="reading.html#scraping-data-off-the-web-using-r"><i class="fa fa-check"></i><b>2.8</b> Scraping data off the web using R</a><ul>
<li class="chapter" data-level="2.8.1" data-path="reading.html"><a href="reading.html#html-and-css-selectors"><i class="fa fa-check"></i><b>2.8.1</b> HTML and CSS selectors</a></li>
<li class="chapter" data-level="2.8.2" data-path="reading.html"><a href="reading.html#are-you-allowed-to-scrape-that-website"><i class="fa fa-check"></i><b>2.8.2</b> Are you allowed to scrape that website?</a></li>
<li class="chapter" data-level="2.8.3" data-path="reading.html"><a href="reading.html#using-rvest"><i class="fa fa-check"></i><b>2.8.3</b> Using <code>rvest</code></a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="reading.html"><a href="reading.html#additional-resources"><i class="fa fa-check"></i><b>2.9</b> Additional resources</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="wrangling.html"><a href="wrangling.html"><i class="fa fa-check"></i><b>3</b> Cleaning and wrangling data</a><ul>
<li class="chapter" data-level="3.1" data-path="wrangling.html"><a href="wrangling.html#overview-1"><i class="fa fa-check"></i><b>3.1</b> Overview</a></li>
<li class="chapter" data-level="3.2" data-path="wrangling.html"><a href="wrangling.html#chapter-learning-objectives-2"><i class="fa fa-check"></i><b>3.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="3.3" data-path="wrangling.html"><a href="wrangling.html#vectors-and-data-frames"><i class="fa fa-check"></i><b>3.3</b> Vectors and Data frames</a><ul>
<li class="chapter" data-level="3.3.1" data-path="wrangling.html"><a href="wrangling.html#what-is-a-data-frame"><i class="fa fa-check"></i><b>3.3.1</b> What is a data frame?</a></li>
<li class="chapter" data-level="3.3.2" data-path="wrangling.html"><a href="wrangling.html#what-is-a-vector"><i class="fa fa-check"></i><b>3.3.2</b> What is a vector?</a></li>
<li class="chapter" data-level="3.3.3" data-path="wrangling.html"><a href="wrangling.html#how-are-vectors-different-from-a-list"><i class="fa fa-check"></i><b>3.3.3</b> How are vectors different from a list?</a></li>
<li class="chapter" data-level="3.3.4" data-path="wrangling.html"><a href="wrangling.html#what-does-this-have-to-do-with-data-frames"><i class="fa fa-check"></i><b>3.3.4</b> What does this have to do with data frames?</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="wrangling.html"><a href="wrangling.html#tidy-data"><i class="fa fa-check"></i><b>3.4</b> Tidy Data</a><ul>
<li class="chapter" data-level="3.4.1" data-path="wrangling.html"><a href="wrangling.html#what-is-tidy-data"><i class="fa fa-check"></i><b>3.4.1</b> What is tidy data?</a></li>
<li class="chapter" data-level="3.4.2" data-path="wrangling.html"><a href="wrangling.html#why-is-tidy-data-important-in-r"><i class="fa fa-check"></i><b>3.4.2</b> Why is tidy data important in R?</a></li>
<li class="chapter" data-level="3.4.3" data-path="wrangling.html"><a href="wrangling.html#going-from-wide-to-long-or-tidy-using-pivot_longer"><i class="fa fa-check"></i><b>3.4.3</b> Going from wide to long (or tidy!) using <code>pivot_longer</code></a></li>
<li class="chapter" data-level="3.4.4" data-path="wrangling.html"><a href="wrangling.html#going-from-long-to-wide-using-pivot_wider"><i class="fa fa-check"></i><b>3.4.4</b> Going from long to wide using <code>pivot_wider</code></a></li>
<li class="chapter" data-level="3.4.5" data-path="wrangling.html"><a href="wrangling.html#using-separate-to-deal-with-multiple-delimiters"><i class="fa fa-check"></i><b>3.4.5</b> Using <code>separate</code> to deal with multiple delimiters</a></li>
<li class="chapter" data-level="3.4.6" data-path="wrangling.html"><a href="wrangling.html#notes-on-defining-tidy-data"><i class="fa fa-check"></i><b>3.4.6</b> Notes on defining tidy data</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="wrangling.html"><a href="wrangling.html#combining-functions-using-the-pipe-operator"><i class="fa fa-check"></i><b>3.5</b> Combining functions using the pipe operator, <code>%&gt;%</code>:</a><ul>
<li class="chapter" data-level="3.5.1" data-path="wrangling.html"><a href="wrangling.html#using-to-combine-filter-and-select"><i class="fa fa-check"></i><b>3.5.1</b> Using <code>%&gt;%</code> to combine <code>filter</code> and <code>select</code></a></li>
<li class="chapter" data-level="3.5.2" data-path="wrangling.html"><a href="wrangling.html#using-with-more-than-two-functions"><i class="fa fa-check"></i><b>3.5.2</b> Using <code>%&gt;%</code> with more than two functions</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="wrangling.html"><a href="wrangling.html#iterating-over-data-with-group_by-summarize"><i class="fa fa-check"></i><b>3.6</b> Iterating over data with <code>group_by</code> + <code>summarize</code></a><ul>
<li class="chapter" data-level="3.6.1" data-path="wrangling.html"><a href="wrangling.html#calculating-summary-statistics"><i class="fa fa-check"></i><b>3.6.1</b> Calculating summary statistics:</a></li>
<li class="chapter" data-level="3.6.2" data-path="wrangling.html"><a href="wrangling.html#calculating-group-summary-statistics"><i class="fa fa-check"></i><b>3.6.2</b> Calculating group summary statistics:</a></li>
<li class="chapter" data-level="3.6.3" data-path="wrangling.html"><a href="wrangling.html#additional-reading-on-the-dplyr-functions"><i class="fa fa-check"></i><b>3.6.3</b> Additional reading on the <code>dplyr</code> functions</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="wrangling.html"><a href="wrangling.html#using-purrrs-map-functions-to-iterate"><i class="fa fa-check"></i><b>3.7</b> Using <code>purrr</code>’s <code>map*</code> functions to iterate</a></li>
<li class="chapter" data-level="3.8" data-path="wrangling.html"><a href="wrangling.html#additional-resources-1"><i class="fa fa-check"></i><b>3.8</b> Additional resources</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="viz.html"><a href="viz.html"><i class="fa fa-check"></i><b>4</b> Effective data visualization</a><ul>
<li class="chapter" data-level="4.1" data-path="viz.html"><a href="viz.html#overview-2"><i class="fa fa-check"></i><b>4.1</b> Overview</a></li>
<li class="chapter" data-level="4.2" data-path="viz.html"><a href="viz.html#chapter-learning-objectives-3"><i class="fa fa-check"></i><b>4.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="4.3" data-path="viz.html"><a href="viz.html#choosing-the-visualization"><i class="fa fa-check"></i><b>4.3</b> Choosing the visualization</a></li>
<li class="chapter" data-level="4.4" data-path="viz.html"><a href="viz.html#refining-the-visualization"><i class="fa fa-check"></i><b>4.4</b> Refining the visualization</a></li>
<li class="chapter" data-level="4.5" data-path="viz.html"><a href="viz.html#creating-visualizations-with-ggplot2"><i class="fa fa-check"></i><b>4.5</b> Creating visualizations with <code>ggplot2</code></a><ul>
<li class="chapter" data-level="4.5.1" data-path="viz.html"><a href="viz.html#the-mauna-loa-co2-data-set"><i class="fa fa-check"></i><b>4.5.1</b> The Mauna Loa CO2 data set</a></li>
<li class="chapter" data-level="4.5.2" data-path="viz.html"><a href="viz.html#the-island-landmass-data-set"><i class="fa fa-check"></i><b>4.5.2</b> The island landmass data set</a></li>
<li class="chapter" data-level="4.5.3" data-path="viz.html"><a href="viz.html#the-old-faithful-eruptionwaiting-time-data-set"><i class="fa fa-check"></i><b>4.5.3</b> The Old Faithful eruption/waiting time data set</a></li>
<li class="chapter" data-level="4.5.4" data-path="viz.html"><a href="viz.html#the-michelson-speed-of-light-data-set"><i class="fa fa-check"></i><b>4.5.4</b> The Michelson speed of light data set</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="viz.html"><a href="viz.html#explaining-the-visualization"><i class="fa fa-check"></i><b>4.6</b> Explaining the visualization</a></li>
<li class="chapter" data-level="4.7" data-path="viz.html"><a href="viz.html#saving-the-visualization"><i class="fa fa-check"></i><b>4.7</b> Saving the visualization</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="version-control.html"><a href="version-control.html"><i class="fa fa-check"></i><b>5</b> Collaboration with version control</a><ul>
<li class="chapter" data-level="5.1" data-path="version-control.html"><a href="version-control.html#overview-3"><i class="fa fa-check"></i><b>5.1</b> Overview</a></li>
<li class="chapter" data-level="5.2" data-path="version-control.html"><a href="version-control.html#chapter-learning-objectives-4"><i class="fa fa-check"></i><b>5.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="5.3" data-path="version-control.html"><a href="version-control.html#what-is-version-control-and-why-should-i-use-it"><i class="fa fa-check"></i><b>5.3</b> What is version control, and why should I use it?</a></li>
<li class="chapter" data-level="5.4" data-path="version-control.html"><a href="version-control.html#creating-a-space-for-your-project-online"><i class="fa fa-check"></i><b>5.4</b> Creating a space for your project online</a></li>
<li class="chapter" data-level="5.5" data-path="version-control.html"><a href="version-control.html#creating-and-editing-files-on-github"><i class="fa fa-check"></i><b>5.5</b> Creating and editing files on GitHub</a><ul>
<li class="chapter" data-level="5.5.1" data-path="version-control.html"><a href="version-control.html#the-pen-tool"><i class="fa fa-check"></i><b>5.5.1</b> The pen tool</a></li>
<li class="chapter" data-level="5.5.2" data-path="version-control.html"><a href="version-control.html#the-add-file-menu"><i class="fa fa-check"></i><b>5.5.2</b> The “Add file” menu</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="version-control.html"><a href="version-control.html#cloning-your-repository-on-jupyterhub"><i class="fa fa-check"></i><b>5.6</b> Cloning your repository on JupyterHub</a></li>
<li class="chapter" data-level="5.7" data-path="version-control.html"><a href="version-control.html#working-in-a-cloned-repository-on-jupyterhub"><i class="fa fa-check"></i><b>5.7</b> Working in a cloned repository on JupyterHub</a><ul>
<li class="chapter" data-level="5.7.1" data-path="version-control.html"><a href="version-control.html#specifying-files-to-commit"><i class="fa fa-check"></i><b>5.7.1</b> Specifying files to commit</a></li>
<li class="chapter" data-level="5.7.2" data-path="version-control.html"><a href="version-control.html#making-the-commit"><i class="fa fa-check"></i><b>5.7.2</b> Making the commit</a></li>
<li class="chapter" data-level="5.7.3" data-path="version-control.html"><a href="version-control.html#pushing-the-commits-to-github"><i class="fa fa-check"></i><b>5.7.3</b> Pushing the commits to GitHub</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="version-control.html"><a href="version-control.html#collaboration"><i class="fa fa-check"></i><b>5.8</b> Collaboration</a><ul>
<li class="chapter" data-level="5.8.1" data-path="version-control.html"><a href="version-control.html#giving-collaborators-access-to-your-project"><i class="fa fa-check"></i><b>5.8.1</b> Giving collaborators access to your project</a></li>
<li class="chapter" data-level="5.8.2" data-path="version-control.html"><a href="version-control.html#pulling-changes-from-github"><i class="fa fa-check"></i><b>5.8.2</b> Pulling changes from GitHub</a></li>
<li class="chapter" data-level="5.8.3" data-path="version-control.html"><a href="version-control.html#handling-merge-conflicts"><i class="fa fa-check"></i><b>5.8.3</b> Handling merge conflicts</a></li>
<li class="chapter" data-level="5.8.4" data-path="version-control.html"><a href="version-control.html#communicating-using-github-issues"><i class="fa fa-check"></i><b>5.8.4</b> Communicating using GitHub issues</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="version-control.html"><a href="version-control.html#additional-resources-2"><i class="fa fa-check"></i><b>5.9</b> Additional resources</a><ul>
<li class="chapter" data-level="5.9.1" data-path="version-control.html"><a href="version-control.html#best-practices-and-workflows"><i class="fa fa-check"></i><b>5.9.1</b> Best practices and workflows</a></li>
<li class="chapter" data-level="5.9.2" data-path="version-control.html"><a href="version-control.html#technical-references"><i class="fa fa-check"></i><b>5.9.2</b> Technical references</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>6</b> Classification I: training &amp; predicting</a><ul>
<li class="chapter" data-level="6.1" data-path="classification.html"><a href="classification.html#overview-4"><i class="fa fa-check"></i><b>6.1</b> Overview</a></li>
<li class="chapter" data-level="6.2" data-path="classification.html"><a href="classification.html#chapter-learning-objectives-5"><i class="fa fa-check"></i><b>6.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="6.3" data-path="classification.html"><a href="classification.html#the-classification-problem"><i class="fa fa-check"></i><b>6.3</b> The classification problem</a></li>
<li class="chapter" data-level="6.4" data-path="classification.html"><a href="classification.html#exploring-a-labelled-data-set"><i class="fa fa-check"></i><b>6.4</b> Exploring a labelled data set</a></li>
<li class="chapter" data-level="6.5" data-path="classification.html"><a href="classification.html#classification-with-k-nearest-neighbours"><i class="fa fa-check"></i><b>6.5</b> Classification with K-nearest neighbours</a></li>
<li class="chapter" data-level="6.6" data-path="classification.html"><a href="classification.html#k-nearest-neighbours-with-tidymodels"><i class="fa fa-check"></i><b>6.6</b> K-nearest neighbours with <code>tidymodels</code></a></li>
<li class="chapter" data-level="6.7" data-path="classification.html"><a href="classification.html#data-preprocessing-with-tidymodels"><i class="fa fa-check"></i><b>6.7</b> Data preprocessing with <code>tidymodels</code></a><ul>
<li class="chapter" data-level="6.7.1" data-path="classification.html"><a href="classification.html#centering-and-scaling"><i class="fa fa-check"></i><b>6.7.1</b> Centering and scaling</a></li>
<li class="chapter" data-level="6.7.2" data-path="classification.html"><a href="classification.html#balancing"><i class="fa fa-check"></i><b>6.7.2</b> Balancing</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="classification.html"><a href="classification.html#putting-it-together-in-a-workflow"><i class="fa fa-check"></i><b>6.8</b> Putting it together in a <code>workflow</code></a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="classification-continued.html"><a href="classification-continued.html"><i class="fa fa-check"></i><b>7</b> Classification II: evaluation &amp; tuning</a><ul>
<li class="chapter" data-level="7.1" data-path="classification-continued.html"><a href="classification-continued.html#overview-5"><i class="fa fa-check"></i><b>7.1</b> Overview</a></li>
<li class="chapter" data-level="7.2" data-path="classification-continued.html"><a href="classification-continued.html#chapter-learning-objectives-6"><i class="fa fa-check"></i><b>7.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="7.3" data-path="classification-continued.html"><a href="classification-continued.html#evaluating-accuracy"><i class="fa fa-check"></i><b>7.3</b> Evaluating accuracy</a></li>
<li class="chapter" data-level="7.4" data-path="classification-continued.html"><a href="classification-continued.html#tuning-the-classifier"><i class="fa fa-check"></i><b>7.4</b> Tuning the classifier</a><ul>
<li class="chapter" data-level="7.4.1" data-path="classification-continued.html"><a href="classification-continued.html#cross-validation"><i class="fa fa-check"></i><b>7.4.1</b> Cross-validation</a></li>
<li class="chapter" data-level="7.4.2" data-path="classification-continued.html"><a href="classification-continued.html#parameter-value-selection"><i class="fa fa-check"></i><b>7.4.2</b> Parameter value selection</a></li>
<li class="chapter" data-level="7.4.3" data-path="classification-continued.html"><a href="classification-continued.html#underoverfitting"><i class="fa fa-check"></i><b>7.4.3</b> Under/overfitting</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="classification-continued.html"><a href="classification-continued.html#splitting-data"><i class="fa fa-check"></i><b>7.5</b> Splitting data</a></li>
<li class="chapter" data-level="7.6" data-path="classification-continued.html"><a href="classification-continued.html#summary"><i class="fa fa-check"></i><b>7.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="regression1.html"><a href="regression1.html"><i class="fa fa-check"></i><b>8</b> Regression I: K-nearest neighbours</a><ul>
<li class="chapter" data-level="8.1" data-path="regression1.html"><a href="regression1.html#overview-6"><i class="fa fa-check"></i><b>8.1</b> Overview</a></li>
<li class="chapter" data-level="8.2" data-path="regression1.html"><a href="regression1.html#chapter-learning-objectives-7"><i class="fa fa-check"></i><b>8.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="8.3" data-path="regression1.html"><a href="regression1.html#regression"><i class="fa fa-check"></i><b>8.3</b> Regression</a></li>
<li class="chapter" data-level="8.4" data-path="regression1.html"><a href="regression1.html#sacremento-real-estate-example"><i class="fa fa-check"></i><b>8.4</b> Sacremento real estate example</a></li>
<li class="chapter" data-level="8.5" data-path="regression1.html"><a href="regression1.html#k-nearest-neighbours-regression"><i class="fa fa-check"></i><b>8.5</b> K-nearest neighbours regression</a></li>
<li class="chapter" data-level="8.6" data-path="regression1.html"><a href="regression1.html#training-evaluating-and-tuning-the-model"><i class="fa fa-check"></i><b>8.6</b> Training, evaluating, and tuning the model</a></li>
<li class="chapter" data-level="8.7" data-path="regression1.html"><a href="regression1.html#underfitting-and-overfitting"><i class="fa fa-check"></i><b>8.7</b> Underfitting and overfitting</a></li>
<li class="chapter" data-level="8.8" data-path="regression1.html"><a href="regression1.html#evaluating-on-the-test-set"><i class="fa fa-check"></i><b>8.8</b> Evaluating on the test set</a></li>
<li class="chapter" data-level="8.9" data-path="regression1.html"><a href="regression1.html#strengths-and-limitations-of-k-nn-regression"><i class="fa fa-check"></i><b>8.9</b> Strengths and limitations of K-NN regression</a></li>
<li class="chapter" data-level="8.10" data-path="regression1.html"><a href="regression1.html#multivariate-k-nn-regression"><i class="fa fa-check"></i><b>8.10</b> Multivariate K-NN regression</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="regression2.html"><a href="regression2.html"><i class="fa fa-check"></i><b>9</b> Regression II: linear regression</a><ul>
<li class="chapter" data-level="9.1" data-path="regression2.html"><a href="regression2.html#overview-7"><i class="fa fa-check"></i><b>9.1</b> Overview</a></li>
<li class="chapter" data-level="9.2" data-path="regression2.html"><a href="regression2.html#chapter-learning-objectives-8"><i class="fa fa-check"></i><b>9.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="9.3" data-path="regression2.html"><a href="regression2.html#simple-linear-regression"><i class="fa fa-check"></i><b>9.3</b> Simple linear regression</a></li>
<li class="chapter" data-level="9.4" data-path="regression2.html"><a href="regression2.html#linear-regression-in-r"><i class="fa fa-check"></i><b>9.4</b> Linear regression in R</a></li>
<li class="chapter" data-level="9.5" data-path="regression2.html"><a href="regression2.html#comparing-simple-linear-and-k-nn-regression"><i class="fa fa-check"></i><b>9.5</b> Comparing simple linear and K-NN regression</a></li>
<li class="chapter" data-level="9.6" data-path="regression2.html"><a href="regression2.html#multivariate-linear-regression"><i class="fa fa-check"></i><b>9.6</b> Multivariate linear regression</a></li>
<li class="chapter" data-level="9.7" data-path="regression2.html"><a href="regression2.html#the-other-side-of-regression"><i class="fa fa-check"></i><b>9.7</b> The other side of regression</a></li>
<li class="chapter" data-level="9.8" data-path="regression2.html"><a href="regression2.html#additional-resources-3"><i class="fa fa-check"></i><b>9.8</b> Additional resources</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>10</b> Clustering</a><ul>
<li class="chapter" data-level="10.1" data-path="clustering.html"><a href="clustering.html#overview-8"><i class="fa fa-check"></i><b>10.1</b> Overview</a></li>
<li class="chapter" data-level="10.2" data-path="clustering.html"><a href="clustering.html#chapter-learning-objectives-9"><i class="fa fa-check"></i><b>10.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="10.3" data-path="clustering.html"><a href="clustering.html#clustering-1"><i class="fa fa-check"></i><b>10.3</b> Clustering</a></li>
<li class="chapter" data-level="10.4" data-path="clustering.html"><a href="clustering.html#k-means"><i class="fa fa-check"></i><b>10.4</b> K-means</a><ul>
<li class="chapter" data-level="10.4.1" data-path="clustering.html"><a href="clustering.html#measuring-cluster-quality"><i class="fa fa-check"></i><b>10.4.1</b> Measuring cluster quality</a></li>
<li class="chapter" data-level="10.4.2" data-path="clustering.html"><a href="clustering.html#the-clustering-algorithm"><i class="fa fa-check"></i><b>10.4.2</b> The clustering algorithm</a></li>
<li class="chapter" data-level="10.4.3" data-path="clustering.html"><a href="clustering.html#random-restarts"><i class="fa fa-check"></i><b>10.4.3</b> Random restarts</a></li>
<li class="chapter" data-level="10.4.4" data-path="clustering.html"><a href="clustering.html#choosing-k"><i class="fa fa-check"></i><b>10.4.4</b> Choosing K</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="clustering.html"><a href="clustering.html#data-pre-processing-for-k-means"><i class="fa fa-check"></i><b>10.5</b> Data pre-processing for K-means</a></li>
<li class="chapter" data-level="10.6" data-path="clustering.html"><a href="clustering.html#k-means-in-r"><i class="fa fa-check"></i><b>10.6</b> K-means in R</a></li>
<li class="chapter" data-level="10.7" data-path="clustering.html"><a href="clustering.html#additional-resources-4"><i class="fa fa-check"></i><b>10.7</b> Additional resources</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>11</b> Introduction to Statistical Inference</a><ul>
<li class="chapter" data-level="11.1" data-path="inference.html"><a href="inference.html#overview-9"><i class="fa fa-check"></i><b>11.1</b> Overview</a></li>
<li class="chapter" data-level="11.2" data-path="inference.html"><a href="inference.html#chapter-learning-objectives-10"><i class="fa fa-check"></i><b>11.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="11.3" data-path="inference.html"><a href="inference.html#why-do-we-need-sampling"><i class="fa fa-check"></i><b>11.3</b> Why do we need sampling?</a></li>
<li class="chapter" data-level="11.4" data-path="inference.html"><a href="inference.html#sampling-distributions"><i class="fa fa-check"></i><b>11.4</b> Sampling distributions</a><ul>
<li class="chapter" data-level="11.4.1" data-path="inference.html"><a href="inference.html#sampling-distributions-for-proportions"><i class="fa fa-check"></i><b>11.4.1</b> Sampling distributions for proportions</a></li>
<li class="chapter" data-level="11.4.2" data-path="inference.html"><a href="inference.html#sampling-distributions-for-means"><i class="fa fa-check"></i><b>11.4.2</b> Sampling distributions for means</a></li>
<li class="chapter" data-level="11.4.3" data-path="inference.html"><a href="inference.html#summary-1"><i class="fa fa-check"></i><b>11.4.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="inference.html"><a href="inference.html#bootstrapping"><i class="fa fa-check"></i><b>11.5</b> Bootstrapping</a><ul>
<li class="chapter" data-level="11.5.1" data-path="inference.html"><a href="inference.html#overview-10"><i class="fa fa-check"></i><b>11.5.1</b> Overview</a></li>
<li class="chapter" data-level="11.5.2" data-path="inference.html"><a href="inference.html#bootstrapping-in-r"><i class="fa fa-check"></i><b>11.5.2</b> Bootstrapping in R</a></li>
<li class="chapter" data-level="11.5.3" data-path="inference.html"><a href="inference.html#using-the-bootstrap-to-calculate-a-plausible-range"><i class="fa fa-check"></i><b>11.5.3</b> Using the bootstrap to calculate a plausible range</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="inference.html"><a href="inference.html#additional-resources-5"><i class="fa fa-check"></i><b>11.6</b> Additional resources</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="move-to-your-own-machine.html"><a href="move-to-your-own-machine.html"><i class="fa fa-check"></i><b>12</b> Moving to your own machine</a><ul>
<li class="chapter" data-level="12.1" data-path="move-to-your-own-machine.html"><a href="move-to-your-own-machine.html#overview-11"><i class="fa fa-check"></i><b>12.1</b> Overview</a></li>
<li class="chapter" data-level="12.2" data-path="move-to-your-own-machine.html"><a href="move-to-your-own-machine.html#chapter-learning-objectives-11"><i class="fa fa-check"></i><b>12.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="12.3" data-path="move-to-your-own-machine.html"><a href="move-to-your-own-machine.html#installing-software-on-your-own-computer"><i class="fa fa-check"></i><b>12.3</b> Installing software on your own computer</a><ul>
<li class="chapter" data-level="12.3.1" data-path="move-to-your-own-machine.html"><a href="move-to-your-own-machine.html#git"><i class="fa fa-check"></i><b>12.3.1</b> Git</a></li>
<li class="chapter" data-level="12.3.2" data-path="move-to-your-own-machine.html"><a href="move-to-your-own-machine.html#miniconda"><i class="fa fa-check"></i><b>12.3.2</b> Miniconda</a></li>
<li class="chapter" data-level="12.3.3" data-path="move-to-your-own-machine.html"><a href="move-to-your-own-machine.html#jupyterlab"><i class="fa fa-check"></i><b>12.3.3</b> JupyterLab</a></li>
<li class="chapter" data-level="12.3.4" data-path="move-to-your-own-machine.html"><a href="move-to-your-own-machine.html#r-and-the-irkernel"><i class="fa fa-check"></i><b>12.3.4</b> R and the IRkernel</a></li>
<li class="chapter" data-level="12.3.5" data-path="move-to-your-own-machine.html"><a href="move-to-your-own-machine.html#r-packages"><i class="fa fa-check"></i><b>12.3.5</b> R packages</a></li>
<li class="chapter" data-level="12.3.6" data-path="move-to-your-own-machine.html"><a href="move-to-your-own-machine.html#latex"><i class="fa fa-check"></i><b>12.3.6</b> LaTeX</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="move-to-your-own-machine.html"><a href="move-to-your-own-machine.html#moving-files-to-your-computer"><i class="fa fa-check"></i><b>12.4</b> Moving files to your computer</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="classification_continued" class="section level1">
<h1><span class="header-section-number">Chapter 7</span> Classification II: evaluation &amp; tuning</h1>
<div id="overview-5" class="section level2">
<h2><span class="header-section-number">7.1</span> Overview</h2>
<p>This chapter continues the introduction to predictive modelling through
classification. While the previous chapter covered training and data
preprocessing, this chapter focuses on how to split data, how to evaluate
prediction accuracy, and how to choose model parameters to maximize
performance.</p>
</div>
<div id="chapter-learning-objectives-6" class="section level2">
<h2><span class="header-section-number">7.2</span> Chapter learning objectives</h2>
<p>By the end of the chapter, students will be able to:</p>
<ul>
<li>Describe what training, validation, and test data sets are and how they are used in classification</li>
<li>Split data into training, validation, and test data sets</li>
<li>Evaluate classification accuracy in R using a validation data set and appropriate metrics</li>
<li>Execute cross-validation in R to choose the number of neighbours in a K-nearest neighbour classifier</li>
<li>Describe advantages and disadvantages of the K-nearest neighbour classification algorithm</li>
</ul>
</div>
<div id="evaluating-accuracy" class="section level2">
<h2><span class="header-section-number">7.3</span> Evaluating accuracy</h2>
<p>Sometimes our classifier might make the wrong prediction. A classifier does not
need to be right 100% of the time to be useful, though we don’t want the
classifier to make too many wrong predictions. How do we measure how “good” our
classifier is? Let’s revisit the
<a href="http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29">breast cancer images example</a>
and think about how our classifier will be used in practice. A biopsy will be
performed on a <em>new</em> patient’s tumour, the resulting image will be analyzed,
and the classifier will be asked to decide whether the tumour is benign or
malignant. The key word here is <em>new</em>: our classifier is “good” if it provides
accurate predictions on data <em>not seen during training</em>. But then how can we
evaluate our classifier without having to visit the hospital to collect more
tumour images?</p>
<p>The trick is to split up the data set into a <strong>training set</strong> and <strong>test set</strong>,
and only show the classifier the <strong>training set</strong> when building the classifier.
Then to evaluate the accuracy of the classifier, we can use it to predict the
labels (which we know) in the <strong>test set</strong>. If our predictions match the true
labels for the observations in the <strong>test set</strong> very well, then we have some
confidence that our classifier might also do a good job of predicting the class
labels for new observations that we do not have the class labels for.</p>
<blockquote>
<p>Note: if there were a golden rule of machine learning, it might be this: <em>you cannot use the test data to build the model!</em>
If you do, the model gets to “see” the test data in advance, making it look more accurate than it really is. Imagine
how bad it would be to overestimate your classifier’s accuracy when predicting whether a patient’s tumour is malignant or benign!</p>
</blockquote>
<div class="figure"><span id="fig:06-training-test"></span>
<img src="img/training_test.jpeg" alt="Splitting the data into training and testing sets" width="600" />
<p class="caption">
Figure 7.1: Splitting the data into training and testing sets
</p>
</div>
<p>How exactly can we assess how well our predictions match the true labels for
the observations in the test set? One way we can do this is to calculate the
<strong>prediction accuracy</strong>. This is the fraction of examples for which the
classifier made the correct prediction. To calculate this we divide the number
of correct predictions by the number of predictions made. Other measures for
how well our classifier performed include <em>precision</em> and <em>recall</em>; these will
not be discussed here, but you will encounter them in other more advanced
courses on this topic. This process is illustrated below:</p>
<div class="figure"><span id="fig:06-ML-paradigm-test"></span>
<img src="img/ML-paradigm-test.png" alt="Process for splitting the data and finding the prediction accuracy" width="800" />
<p class="caption">
Figure 7.2: Process for splitting the data and finding the prediction accuracy
</p>
</div>
<p>In R, we can use the <code>tidymodels</code> library collection not only to perform K-nearest neighbour
classification, but also to assess how well our classification worked. Let’s
start by loading the necessary libraries, reading in the breast cancer data
from the previous chapter, and making a quick scatter plot visualization of
tumour cell concavity versus smoothness coloured by diagnosis.</p>
<div class="sourceCode" id="cb199"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb199-1"><a href="classification-continued.html#cb199-1"></a><span class="co"># load libraries</span></span>
<span id="cb199-2"><a href="classification-continued.html#cb199-2"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb199-3"><a href="classification-continued.html#cb199-3"></a><span class="kw">library</span>(tidymodels)</span>
<span id="cb199-4"><a href="classification-continued.html#cb199-4"></a></span>
<span id="cb199-5"><a href="classification-continued.html#cb199-5"></a><span class="co"># load data</span></span>
<span id="cb199-6"><a href="classification-continued.html#cb199-6"></a>cancer &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;data/unscaled_wdbc.csv&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb199-7"><a href="classification-continued.html#cb199-7"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">Class =</span> <span class="kw">as_factor</span>(Class)) <span class="co"># convert the character Class variable to the factor datatype</span></span>
<span id="cb199-8"><a href="classification-continued.html#cb199-8"></a></span>
<span id="cb199-9"><a href="classification-continued.html#cb199-9"></a><span class="co"># colour palette</span></span>
<span id="cb199-10"><a href="classification-continued.html#cb199-10"></a>cbPalette &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;#E69F00&quot;</span>, <span class="st">&quot;#56B4E9&quot;</span>, <span class="st">&quot;#009E73&quot;</span>, <span class="st">&quot;#F0E442&quot;</span>, <span class="st">&quot;#0072B2&quot;</span>, <span class="st">&quot;#D55E00&quot;</span>, <span class="st">&quot;#CC79A7&quot;</span>, <span class="st">&quot;#999999&quot;</span>)</span>
<span id="cb199-11"><a href="classification-continued.html#cb199-11"></a></span>
<span id="cb199-12"><a href="classification-continued.html#cb199-12"></a><span class="co"># create scatter plot of tumour cell concavity versus smoothness,</span></span>
<span id="cb199-13"><a href="classification-continued.html#cb199-13"></a><span class="co"># labelling the points be diagnosis class</span></span>
<span id="cb199-14"><a href="classification-continued.html#cb199-14"></a>perim_concav &lt;-<span class="st"> </span>cancer <span class="op">%&gt;%</span></span>
<span id="cb199-15"><a href="classification-continued.html#cb199-15"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> Smoothness, <span class="dt">y =</span> Concavity, <span class="dt">color =</span> Class)) <span class="op">+</span></span>
<span id="cb199-16"><a href="classification-continued.html#cb199-16"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">0.5</span>) <span class="op">+</span></span>
<span id="cb199-17"><a href="classification-continued.html#cb199-17"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">color =</span> <span class="st">&quot;Diagnosis&quot;</span>) <span class="op">+</span></span>
<span id="cb199-18"><a href="classification-continued.html#cb199-18"></a><span class="st">  </span><span class="kw">scale_color_manual</span>(<span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;Malignant&quot;</span>, <span class="st">&quot;Benign&quot;</span>), <span class="dt">values =</span> cbPalette)</span>
<span id="cb199-19"><a href="classification-continued.html#cb199-19"></a></span>
<span id="cb199-20"><a href="classification-continued.html#cb199-20"></a>perim_concav</span></code></pre></div>
<div class="figure"><span id="fig:06-precode"></span>
<img src="_main_files/figure-html/06-precode-1.png" alt="Scatterplot of tumour cell concavity versus smoothness coloured by diagnosis label" width="480" />
<p class="caption">
Figure 7.3: Scatterplot of tumour cell concavity versus smoothness coloured by diagnosis label
</p>
</div>
<p><strong>1. Create the train / test split</strong></p>
<p>Once we have decided on a predictive question to answer and done some
preliminary exploration, the very next thing to do is to split the data into
the training and test sets. Typically, the training set is between 50 - 100% of
the data, while the test set is the remaining 0 - 50%; the intuition is that
you want to trade off between training an accurate model (by using a larger
training data set) and getting an accurate evaluation of its performance (by
using a larger test data set). Here, we will use 75% of the data for training,
and 25% for testing. To do this we will use the <code>initial_split</code> function,
specifying that <code>prop = 0.75</code> and the target variable is <code>Class</code>:</p>
<div class="sourceCode" id="cb200"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb200-1"><a href="classification-continued.html#cb200-1"></a><span class="kw">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb200-2"><a href="classification-continued.html#cb200-2"></a>cancer_split &lt;-<span class="st"> </span><span class="kw">initial_split</span>(cancer, <span class="dt">prop =</span> <span class="fl">0.75</span>, <span class="dt">strata =</span> Class)</span>
<span id="cb200-3"><a href="classification-continued.html#cb200-3"></a>cancer_train &lt;-<span class="st"> </span><span class="kw">training</span>(cancer_split)</span>
<span id="cb200-4"><a href="classification-continued.html#cb200-4"></a>cancer_test &lt;-<span class="st"> </span><span class="kw">testing</span>(cancer_split)</span></code></pre></div>
<blockquote>
<p>Note: You will see in the code above that we use the <code>set.seed</code> function again, as discussed in the previous chapter. In this case it is because
<code>initial_split</code> uses random sampling to choose which rows will be in the training set. Since we want our code to be reproducible
and generate the same train/test split each time it is run, we use <code>set.seed</code>.</p>
</blockquote>
<div class="sourceCode" id="cb201"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb201-1"><a href="classification-continued.html#cb201-1"></a><span class="kw">glimpse</span>(cancer_train)</span></code></pre></div>
<pre><code>## Rows: 427
## Columns: 12
## $ ID                &lt;dbl&gt; 842302, 842517, 84300903, 84348301, 84358402, 84378…
## $ Class             &lt;fct&gt; M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, B, B, …
## $ Radius            &lt;dbl&gt; 17.990, 20.570, 19.690, 11.420, 20.290, 12.450, 18.…
## $ Texture           &lt;dbl&gt; 10.38, 17.77, 21.25, 20.38, 14.34, 15.70, 19.98, 20…
## $ Perimeter         &lt;dbl&gt; 122.80, 132.90, 130.00, 77.58, 135.10, 82.57, 119.6…
## $ Area              &lt;dbl&gt; 1001.0, 1326.0, 1203.0, 386.1, 1297.0, 477.1, 1040.…
## $ Smoothness        &lt;dbl&gt; 0.11840, 0.08474, 0.10960, 0.14250, 0.10030, 0.1278…
## $ Compactness       &lt;dbl&gt; 0.27760, 0.07864, 0.15990, 0.28390, 0.13280, 0.1700…
## $ Concavity         &lt;dbl&gt; 0.30010, 0.08690, 0.19740, 0.24140, 0.19800, 0.1578…
## $ Concave_Points    &lt;dbl&gt; 0.14710, 0.07017, 0.12790, 0.10520, 0.10430, 0.0808…
## $ Symmetry          &lt;dbl&gt; 0.2419, 0.1812, 0.2069, 0.2597, 0.1809, 0.2087, 0.1…
## $ Fractal_Dimension &lt;dbl&gt; 0.07871, 0.05667, 0.05999, 0.09744, 0.05883, 0.0761…</code></pre>
<div class="sourceCode" id="cb203"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb203-1"><a href="classification-continued.html#cb203-1"></a><span class="kw">glimpse</span>(cancer_test)</span></code></pre></div>
<pre><code>## Rows: 142
## Columns: 12
## $ ID                &lt;dbl&gt; 844981, 84799002, 848406, 849014, 8510426, 8511133,…
## $ Class             &lt;fct&gt; M, M, M, M, B, M, M, M, M, M, M, B, B, M, M, M, B, …
## $ Radius            &lt;dbl&gt; 13.000, 14.540, 14.680, 19.810, 13.540, 15.340, 18.…
## $ Texture           &lt;dbl&gt; 21.82, 27.54, 20.13, 22.15, 14.36, 14.26, 25.11, 26…
## $ Perimeter         &lt;dbl&gt; 87.50, 96.73, 94.74, 130.00, 87.46, 102.50, 124.80,…
## $ Area              &lt;dbl&gt; 519.8, 658.8, 684.5, 1260.0, 566.3, 704.4, 1088.0, …
## $ Smoothness        &lt;dbl&gt; 0.12730, 0.11390, 0.09867, 0.09831, 0.09779, 0.1073…
## $ Compactness       &lt;dbl&gt; 0.19320, 0.15950, 0.07200, 0.10270, 0.08129, 0.2135…
## $ Concavity         &lt;dbl&gt; 0.18590, 0.16390, 0.07395, 0.14790, 0.06664, 0.2077…
## $ Concave_Points    &lt;dbl&gt; 0.093530, 0.073640, 0.052590, 0.094980, 0.047810, 0…
## $ Symmetry          &lt;dbl&gt; 0.2350, 0.2303, 0.1586, 0.1582, 0.1885, 0.2521, 0.2…
## $ Fractal_Dimension &lt;dbl&gt; 0.07389, 0.07077, 0.05922, 0.05395, 0.05766, 0.0703…</code></pre>
<p>We can see from <code>glimpse</code> in the code above that the training set contains 427
observations, while the test set contains 142 observations. This corresponds to
a train / test split of 75% / 25%, as desired.</p>
<p><strong>2. Pre-process the data</strong></p>
<p>As we mentioned last chapter, K-NN is sensitive to the scale of the predictors,
and so we should perform some preprocessing to standardize them. An
additional consideration we need to take when doing this is that we should
create the standardization preprocessor using <strong>only the training data</strong>. This ensures that
our test data does not influence any aspect of our model training. Once we have
created the standardization preprocessor, we can then apply it separately to both the
training and test data sets.</p>
<p>Fortunately, the <code>recipe</code> framework from <code>tidymodels</code> makes it simple to handle
this properly. Below we construct and prepare the recipe using only the training
data (due to <code>data = cancer_train</code> in the first line).</p>
<div class="sourceCode" id="cb205"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb205-1"><a href="classification-continued.html#cb205-1"></a>cancer_recipe &lt;-<span class="st"> </span><span class="kw">recipe</span>(Class <span class="op">~</span><span class="st"> </span>Smoothness <span class="op">+</span><span class="st"> </span>Concavity, <span class="dt">data =</span> cancer_train) <span class="op">%&gt;%</span></span>
<span id="cb205-2"><a href="classification-continued.html#cb205-2"></a><span class="st">  </span><span class="kw">step_scale</span>(<span class="kw">all_predictors</span>()) <span class="op">%&gt;%</span></span>
<span id="cb205-3"><a href="classification-continued.html#cb205-3"></a><span class="st">  </span><span class="kw">step_center</span>(<span class="kw">all_predictors</span>())</span></code></pre></div>
<p><strong>3. Train the classifier</strong></p>
<p>Now that we have split our original data set into training and test sets, we
can create our K-nearest neighbour classifier with only the training set using
the technique we learned in the previous chapter. For now, we will just choose
the number <span class="math inline">\(K\)</span> of neighbours to be 3, and use concavity and smoothness as the
predictors.</p>
<div class="sourceCode" id="cb206"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb206-1"><a href="classification-continued.html#cb206-1"></a><span class="kw">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb206-2"><a href="classification-continued.html#cb206-2"></a>knn_spec &lt;-<span class="st"> </span><span class="kw">nearest_neighbor</span>(<span class="dt">weight_func =</span> <span class="st">&quot;rectangular&quot;</span>, <span class="dt">neighbors =</span> <span class="dv">3</span>) <span class="op">%&gt;%</span></span>
<span id="cb206-3"><a href="classification-continued.html#cb206-3"></a><span class="st">  </span><span class="kw">set_engine</span>(<span class="st">&quot;kknn&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb206-4"><a href="classification-continued.html#cb206-4"></a><span class="st">  </span><span class="kw">set_mode</span>(<span class="st">&quot;classification&quot;</span>)</span>
<span id="cb206-5"><a href="classification-continued.html#cb206-5"></a></span>
<span id="cb206-6"><a href="classification-continued.html#cb206-6"></a>knn_fit &lt;-<span class="st"> </span><span class="kw">workflow</span>() <span class="op">%&gt;%</span></span>
<span id="cb206-7"><a href="classification-continued.html#cb206-7"></a><span class="st">  </span><span class="kw">add_recipe</span>(cancer_recipe) <span class="op">%&gt;%</span></span>
<span id="cb206-8"><a href="classification-continued.html#cb206-8"></a><span class="st">  </span><span class="kw">add_model</span>(knn_spec) <span class="op">%&gt;%</span></span>
<span id="cb206-9"><a href="classification-continued.html#cb206-9"></a><span class="st">  </span><span class="kw">fit</span>(<span class="dt">data =</span> cancer_train)</span>
<span id="cb206-10"><a href="classification-continued.html#cb206-10"></a></span>
<span id="cb206-11"><a href="classification-continued.html#cb206-11"></a>knn_fit</span></code></pre></div>
<pre><code>## ══ Workflow [trained] ══════════════════════════════════════════════════════════
## Preprocessor: Recipe
## Model: nearest_neighbor()
## 
## ── Preprocessor ────────────────────────────────────────────────────────────────
## 2 Recipe Steps
## 
## ● step_scale()
## ● step_center()
## 
## ── Model ───────────────────────────────────────────────────────────────────────
## 
## Call:
## kknn::train.kknn(formula = formula, data = data, ks = ~3, kernel = ~&quot;rectangular&quot;)
## 
## Type of response variable: nominal
## Minimal misclassification: 0.1264637
## Best kernel: rectangular
## Best k: 3</code></pre>
<blockquote>
<p>Note: Here again you see the <code>set.seed</code> function. In the K-nearest neighbour algorithm,
there is a tie for the majority neighbour class, the winner is randomly selected. Although there is no chance
of a tie when <span class="math inline">\(K\)</span> is odd (here <span class="math inline">\(K=3\)</span>), it is possible that the code may be changed in the future to have an even value of <span class="math inline">\(K\)</span>.
Thus, to prevent potential issues with reproducibility, we have set the seed. Note that in your own code,
you only have to set the seed once at the beginning of your analysis.</p>
</blockquote>
<p><strong>4. Predict the labels in the test set</strong></p>
<p>Now that we have a K-nearest neighbour classifier object, we can use it to
predict the class labels for our test set. We use the <code>bind_cols</code> to add the
column of predictions to the original test data, creating the
<code>cancer_test_predictions</code> data frame. The <code>Class</code> variable contains the true
diagnoses, while the <code>.pred_class</code> contains the predicted diagnoses from the
model.</p>
<div class="sourceCode" id="cb208"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb208-1"><a href="classification-continued.html#cb208-1"></a>cancer_test_predictions &lt;-<span class="st"> </span><span class="kw">predict</span>(knn_fit, cancer_test) <span class="op">%&gt;%</span></span>
<span id="cb208-2"><a href="classification-continued.html#cb208-2"></a><span class="st">  </span><span class="kw">bind_cols</span>(cancer_test)</span>
<span id="cb208-3"><a href="classification-continued.html#cb208-3"></a>cancer_test_predictions</span></code></pre></div>
<pre><code>## # A tibble: 142 x 13
##    .pred_class     ID Class Radius Texture Perimeter  Area Smoothness
##    &lt;fct&gt;        &lt;dbl&gt; &lt;fct&gt;  &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;
##  1 M           8.45e5 M       13      21.8      87.5  520.     0.127 
##  2 M           8.48e7 M       14.5    27.5      96.7  659.     0.114 
##  3 B           8.48e5 M       14.7    20.1      94.7  684.     0.0987
##  4 M           8.49e5 M       19.8    22.2     130   1260      0.0983
##  5 B           8.51e6 B       13.5    14.4      87.5  566.     0.0978
##  6 M           8.51e6 M       15.3    14.3     102.   704.     0.107 
##  7 M           8.53e5 M       18.6    25.1     125.  1088      0.106 
##  8 M           8.54e5 M       19.3    26.5     128.  1162      0.0940
##  9 B           8.55e5 M       13.4    21.6      86.2  563      0.0816
## 10 M           8.56e5 M       13.3    20.3      87.3  545.     0.104 
## # … with 132 more rows, and 5 more variables: Compactness &lt;dbl&gt;,
## #   Concavity &lt;dbl&gt;, Concave_Points &lt;dbl&gt;, Symmetry &lt;dbl&gt;,
## #   Fractal_Dimension &lt;dbl&gt;</code></pre>
<p><strong>5. Compute the accuracy</strong></p>
<p>Finally we can assess our classifier’s accuracy. To do this we use the <code>metrics</code> function
from <code>tidymodels</code> to get the statistics about the quality of our model, specifying
the <code>truth</code> and <code>estimate</code> arguments:</p>
<div class="sourceCode" id="cb210"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb210-1"><a href="classification-continued.html#cb210-1"></a>cancer_test_predictions <span class="op">%&gt;%</span></span>
<span id="cb210-2"><a href="classification-continued.html#cb210-2"></a><span class="st">  </span><span class="kw">metrics</span>(<span class="dt">truth =</span> Class, <span class="dt">estimate =</span> .pred_class)</span></code></pre></div>
<pre><code>## # A tibble: 2 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.880
## 2 kap      binary         0.741</code></pre>
<p>This shows that the accuracy of the classifier on the test data was 88%.
We can also look at the <em>confusion matrix</em> for the classifier, which shows
the table of predicted labels and correct labels, using the <code>conf_mat</code> function:</p>
<div class="sourceCode" id="cb212"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb212-1"><a href="classification-continued.html#cb212-1"></a>cancer_test_predictions <span class="op">%&gt;%</span></span>
<span id="cb212-2"><a href="classification-continued.html#cb212-2"></a><span class="st">  </span><span class="kw">conf_mat</span>(<span class="dt">truth =</span> Class, <span class="dt">estimate =</span> .pred_class)</span></code></pre></div>
<pre><code>##           Truth
## Prediction  M  B
##          M 43  7
##          B 10 82</code></pre>
<p>This says that the classifier labelled 43+82 = 125 observations correctly,
10 observations as benign when they were truly malignant,
and 7 observations as malignant when they were truly benign.</p>
</div>
<div id="tuning-the-classifier" class="section level2">
<h2><span class="header-section-number">7.4</span> Tuning the classifier</h2>
<p>The vast majority of predictive models in statistics and machine learning have
<em>parameters</em> that you have to pick. For example, in the K-nearest neighbour
classification algorithm we have been using in the past two chapters, we have
had to pick the number of neighbours <span class="math inline">\(K\)</span> for the class vote. Is it possible to
make this selection, i.e., <em>tune</em> the model, in a principled way? Ideally what
we want is to somehow maximize the performance of our classifier on data <em>it
hasn’t seen yet</em>. So we will play the same trick we did before when evaluating
our classifier: we’ll split our <strong>overall training data set</strong> further into two
subsets, called the <strong>training set</strong> and <strong>validation set</strong>. We will use the
newly-named <strong>training set</strong> for building the classifier, and the <strong>validation
set</strong> for evaluating it! Then we will try different values of the parameter <span class="math inline">\(K\)</span>
and pick the one that yields the highest accuracy.</p>
<blockquote>
<p><strong>Remember:</strong> <em>don’t touch the test set during the tuning process. Tuning is a part of model training!</em></p>
</blockquote>
<div id="cross-validation" class="section level3">
<h3><span class="header-section-number">7.4.1</span> Cross-validation</h3>
<p>There is an important detail to mention about the process of tuning: we can, if
we want to, split our overall training data up in multiple different ways,
train and evaluate a classifier for each split, and then choose the parameter
based on <strong><em>all</em></strong> of the different results. If we just split our overall training
data <em>once</em>, our best parameter choice will depend strongly on whatever data
was lucky enough to end up in the validation set. Perhaps using multiple
different train / validation splits, we’ll get a better estimate of accuracy,
which will lead to a better choice of the number of neighbours <span class="math inline">\(K\)</span> for the
overall set of training data.</p>
<blockquote>
<p><strong>Note:</strong> you might be wondering why we can’t we use the multiple splits to test our final classifier after tuning is done. This is simply
because at the end of the day, we will produce a single classifier using our overall training data. If we do multiple train / test splits, we will
end up with multiple classifiers, each with their own accuracy evaluated on different test data.</p>
</blockquote>
<p>Let’s investigate this idea in R! In particular, we will use different seed
values in the <code>set.seed</code> function to generate five different train / validation
splits of our overall training data, train five different K-nearest neighbour
models, and evaluate their accuracy.</p>
<div class="sourceCode" id="cb214"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb214-1"><a href="classification-continued.html#cb214-1"></a>accuracies &lt;-<span class="st"> </span><span class="kw">c</span>()</span>
<span id="cb214-2"><a href="classification-continued.html#cb214-2"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>) {</span>
<span id="cb214-3"><a href="classification-continued.html#cb214-3"></a>  <span class="kw">set.seed</span>(i) <span class="co"># makes the random selection of rows reproducible</span></span>
<span id="cb214-4"><a href="classification-continued.html#cb214-4"></a></span>
<span id="cb214-5"><a href="classification-continued.html#cb214-5"></a>  <span class="co"># create the 25/75 split of the training data into training and validation</span></span>
<span id="cb214-6"><a href="classification-continued.html#cb214-6"></a>  cancer_split &lt;-<span class="st"> </span><span class="kw">initial_split</span>(cancer_train, <span class="dt">prop =</span> <span class="fl">0.75</span>, <span class="dt">strata =</span> Class)</span>
<span id="cb214-7"><a href="classification-continued.html#cb214-7"></a>  cancer_subtrain &lt;-<span class="st"> </span><span class="kw">training</span>(cancer_split)</span>
<span id="cb214-8"><a href="classification-continued.html#cb214-8"></a>  cancer_validation &lt;-<span class="st"> </span><span class="kw">testing</span>(cancer_split)</span>
<span id="cb214-9"><a href="classification-continued.html#cb214-9"></a></span>
<span id="cb214-10"><a href="classification-continued.html#cb214-10"></a>  <span class="co"># recreate the standardization recipe from before (since it must be based on the training data)</span></span>
<span id="cb214-11"><a href="classification-continued.html#cb214-11"></a>  cancer_recipe &lt;-<span class="st"> </span><span class="kw">recipe</span>(Class <span class="op">~</span><span class="st"> </span>Smoothness <span class="op">+</span><span class="st"> </span>Concavity, <span class="dt">data =</span> cancer_subtrain) <span class="op">%&gt;%</span></span>
<span id="cb214-12"><a href="classification-continued.html#cb214-12"></a><span class="st">    </span><span class="kw">step_scale</span>(<span class="kw">all_predictors</span>()) <span class="op">%&gt;%</span></span>
<span id="cb214-13"><a href="classification-continued.html#cb214-13"></a><span class="st">    </span><span class="kw">step_center</span>(<span class="kw">all_predictors</span>())</span>
<span id="cb214-14"><a href="classification-continued.html#cb214-14"></a></span>
<span id="cb214-15"><a href="classification-continued.html#cb214-15"></a>  <span class="co"># fit the knn model (we can reuse the old knn_spec model from before)</span></span>
<span id="cb214-16"><a href="classification-continued.html#cb214-16"></a>  knn_fit &lt;-<span class="st"> </span><span class="kw">workflow</span>() <span class="op">%&gt;%</span></span>
<span id="cb214-17"><a href="classification-continued.html#cb214-17"></a><span class="st">    </span><span class="kw">add_recipe</span>(cancer_recipe) <span class="op">%&gt;%</span></span>
<span id="cb214-18"><a href="classification-continued.html#cb214-18"></a><span class="st">    </span><span class="kw">add_model</span>(knn_spec) <span class="op">%&gt;%</span></span>
<span id="cb214-19"><a href="classification-continued.html#cb214-19"></a><span class="st">    </span><span class="kw">fit</span>(<span class="dt">data =</span> cancer_subtrain)</span>
<span id="cb214-20"><a href="classification-continued.html#cb214-20"></a></span>
<span id="cb214-21"><a href="classification-continued.html#cb214-21"></a>  <span class="co"># get predictions on the validation data</span></span>
<span id="cb214-22"><a href="classification-continued.html#cb214-22"></a>  validation_predicted &lt;-<span class="st"> </span><span class="kw">predict</span>(knn_fit, cancer_validation) <span class="op">%&gt;%</span></span>
<span id="cb214-23"><a href="classification-continued.html#cb214-23"></a><span class="st">    </span><span class="kw">bind_cols</span>(cancer_validation)</span>
<span id="cb214-24"><a href="classification-continued.html#cb214-24"></a></span>
<span id="cb214-25"><a href="classification-continued.html#cb214-25"></a>  <span class="co"># compute the accuracy</span></span>
<span id="cb214-26"><a href="classification-continued.html#cb214-26"></a>  acc &lt;-<span class="st"> </span>validation_predicted <span class="op">%&gt;%</span></span>
<span id="cb214-27"><a href="classification-continued.html#cb214-27"></a><span class="st">    </span><span class="kw">metrics</span>(<span class="dt">truth =</span> Class, <span class="dt">estimate =</span> .pred_class) <span class="op">%&gt;%</span></span>
<span id="cb214-28"><a href="classification-continued.html#cb214-28"></a><span class="st">    </span><span class="kw">filter</span>(.metric <span class="op">==</span><span class="st"> &quot;accuracy&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb214-29"><a href="classification-continued.html#cb214-29"></a><span class="st">    </span><span class="kw">select</span>(.estimate) <span class="op">%&gt;%</span></span>
<span id="cb214-30"><a href="classification-continued.html#cb214-30"></a><span class="st">    </span><span class="kw">pull</span>()</span>
<span id="cb214-31"><a href="classification-continued.html#cb214-31"></a>  accuracies &lt;-<span class="st"> </span><span class="kw">append</span>(accuracies, acc)</span>
<span id="cb214-32"><a href="classification-continued.html#cb214-32"></a>}</span>
<span id="cb214-33"><a href="classification-continued.html#cb214-33"></a>accuracies</span></code></pre></div>
<pre><code>## [1] 0.9150943 0.8679245 0.8490566 0.8962264 0.9150943</code></pre>
<p>With five different shuffles of the data, we get five different values for
accuracy. None of these is necessarily “more correct” than any other; they’re
just five estimates of the true, underlying accuracy of our classifier built
using our overall training data. We can combine the estimates by taking their
average (here 0.8886792) to try to get a single assessment of our
classifier’s accuracy; this has the effect of reducing the influence of any one
(un)lucky validation set on the estimate.</p>
<p>In practice, we don’t use random splits, but rather use a more structured
splitting procedure so that each observation in the data set is used in a
validation set only a single time. The name for this strategy is called
<strong>cross-validation</strong>. In <strong>cross-validation</strong>, we split our <strong>overall training
data</strong> into <span class="math inline">\(C\)</span> evenly-sized chunks, and then iteratively use <span class="math inline">\(1\)</span> chunk as the
<strong>validation set</strong> and combine the remaining <span class="math inline">\(C-1\)</span> chunks
as the <strong>training set</strong>:</p>
<div class="figure"><span id="fig:06-cv-image"></span>
<img src="img/cv.png" alt="5-fold cross validation" width="800" />
<p class="caption">
Figure 7.4: 5-fold cross validation
</p>
</div>
<p>In the picture above, <span class="math inline">\(C=5\)</span> different chunks of the data set are used,
resulting in 5 different choices for the <strong>validation set</strong>; we call this
<em>5-fold</em> cross-validation. To do 5-fold cross-validation in R with <code>tidymodels</code>, we
use another function: <code>vfold_cv</code>. This function splits our training data into
<code>v</code> folds automatically:</p>
<div class="sourceCode" id="cb216"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb216-1"><a href="classification-continued.html#cb216-1"></a>cancer_vfold &lt;-<span class="st"> </span><span class="kw">vfold_cv</span>(cancer_train, <span class="dt">v =</span> <span class="dv">5</span>, <span class="dt">strata =</span> Class)</span>
<span id="cb216-2"><a href="classification-continued.html#cb216-2"></a>cancer_vfold</span></code></pre></div>
<pre><code>## #  5-fold cross-validation using stratification 
## # A tibble: 5 x 2
##   splits           id   
##   &lt;list&gt;           &lt;chr&gt;
## 1 &lt;split [341/86]&gt; Fold1
## 2 &lt;split [341/86]&gt; Fold2
## 3 &lt;split [341/86]&gt; Fold3
## 4 &lt;split [342/85]&gt; Fold4
## 5 &lt;split [343/84]&gt; Fold5</code></pre>
<p>Then, when we create our data analysis workflow, we use the <code>fit_resamples</code> function
instead of the <code>fit</code> function for training. This runs cross-validation on each
train/validation split.</p>
<blockquote>
<p><strong>Note:</strong> we set the seed when we call <code>train</code> not only because of the potential for ties, but also because we are doing
cross-validation. Cross-validation uses a random process to select how to partition the training data.</p>
</blockquote>
<div class="sourceCode" id="cb218"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb218-1"><a href="classification-continued.html#cb218-1"></a><span class="kw">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb218-2"><a href="classification-continued.html#cb218-2"></a></span>
<span id="cb218-3"><a href="classification-continued.html#cb218-3"></a><span class="co"># recreate the standardization recipe from before (since it must be based on the training data)</span></span>
<span id="cb218-4"><a href="classification-continued.html#cb218-4"></a>cancer_recipe &lt;-<span class="st"> </span><span class="kw">recipe</span>(Class <span class="op">~</span><span class="st"> </span>Smoothness <span class="op">+</span><span class="st"> </span>Concavity, <span class="dt">data =</span> cancer_train) <span class="op">%&gt;%</span></span>
<span id="cb218-5"><a href="classification-continued.html#cb218-5"></a><span class="st">  </span><span class="kw">step_scale</span>(<span class="kw">all_predictors</span>()) <span class="op">%&gt;%</span></span>
<span id="cb218-6"><a href="classification-continued.html#cb218-6"></a><span class="st">  </span><span class="kw">step_center</span>(<span class="kw">all_predictors</span>())</span>
<span id="cb218-7"><a href="classification-continued.html#cb218-7"></a></span>
<span id="cb218-8"><a href="classification-continued.html#cb218-8"></a><span class="co"># fit the knn model (we can reuse the old knn_spec model from before)</span></span>
<span id="cb218-9"><a href="classification-continued.html#cb218-9"></a>knn_fit &lt;-<span class="st"> </span><span class="kw">workflow</span>() <span class="op">%&gt;%</span></span>
<span id="cb218-10"><a href="classification-continued.html#cb218-10"></a><span class="st">  </span><span class="kw">add_recipe</span>(cancer_recipe) <span class="op">%&gt;%</span></span>
<span id="cb218-11"><a href="classification-continued.html#cb218-11"></a><span class="st">  </span><span class="kw">add_model</span>(knn_spec) <span class="op">%&gt;%</span></span>
<span id="cb218-12"><a href="classification-continued.html#cb218-12"></a><span class="st">  </span><span class="kw">fit_resamples</span>(<span class="dt">resamples =</span> cancer_vfold)</span>
<span id="cb218-13"><a href="classification-continued.html#cb218-13"></a></span>
<span id="cb218-14"><a href="classification-continued.html#cb218-14"></a>knn_fit</span></code></pre></div>
<pre><code>## #  5-fold cross-validation using stratification 
## # A tibble: 5 x 4
##   splits           id    .metrics         .notes          
##   &lt;list&gt;           &lt;chr&gt; &lt;list&gt;           &lt;list&gt;          
## 1 &lt;split [341/86]&gt; Fold1 &lt;tibble [2 × 3]&gt; &lt;tibble [0 × 1]&gt;
## 2 &lt;split [341/86]&gt; Fold2 &lt;tibble [2 × 3]&gt; &lt;tibble [0 × 1]&gt;
## 3 &lt;split [341/86]&gt; Fold3 &lt;tibble [2 × 3]&gt; &lt;tibble [0 × 1]&gt;
## 4 &lt;split [342/85]&gt; Fold4 &lt;tibble [2 × 3]&gt; &lt;tibble [0 × 1]&gt;
## 5 &lt;split [343/84]&gt; Fold5 &lt;tibble [2 × 3]&gt; &lt;tibble [0 × 1]&gt;</code></pre>
<p>The <code>collect_metrics</code> function is used to aggregate the mean and <em>standard error</em>
of the classifier’s validation accuracy across the folds. The standard error is
a measure of how uncertain we are in the mean value. A detailed treatment of this
is beyond the scope of this chapter; but roughly, if your estimated mean (that
the <code>collect_metrics</code> function gives you) is 0.88 and standard
error is 0.02, you can expect the <em>true</em> average accuracy of the
classifier to be somewhere roughly between 0.86 and 0.90 (although it may
fall outside this range).</p>
<div class="sourceCode" id="cb220"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb220-1"><a href="classification-continued.html#cb220-1"></a>knn_fit <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">collect_metrics</span>()</span></code></pre></div>
<pre><code>## # A tibble: 2 x 5
##   .metric  .estimator  mean     n std_err
##   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
## 1 accuracy binary     0.883     5  0.0189
## 2 roc_auc  binary     0.923     5  0.0104</code></pre>
<p>We can choose any number of folds, and typically the more we use the better our
accuracy estimate will be (lower standard error). However, we are limited
by computational power: the
more folds we choose, the more computation it takes, and hence the more time
it takes to run the analysis. So when you do cross-validation, you need to
consider the size of the data, and the speed of the algorithm (e.g., K-nearest
neighbour) and the speed of your computer. In practice, this is a trial and
error process, but typically <span class="math inline">\(C\)</span> is chosen to be either 5 or 10. Here we show
how the standard error decreases when we use 10-fold cross validation rather
than 5-fold:</p>
<div class="sourceCode" id="cb222"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb222-1"><a href="classification-continued.html#cb222-1"></a>cancer_vfold &lt;-<span class="st"> </span><span class="kw">vfold_cv</span>(cancer_train, <span class="dt">v =</span> <span class="dv">10</span>, <span class="dt">strata =</span> Class)</span>
<span id="cb222-2"><a href="classification-continued.html#cb222-2"></a></span>
<span id="cb222-3"><a href="classification-continued.html#cb222-3"></a><span class="kw">workflow</span>() <span class="op">%&gt;%</span></span>
<span id="cb222-4"><a href="classification-continued.html#cb222-4"></a><span class="st">  </span><span class="kw">add_recipe</span>(cancer_recipe) <span class="op">%&gt;%</span></span>
<span id="cb222-5"><a href="classification-continued.html#cb222-5"></a><span class="st">  </span><span class="kw">add_model</span>(knn_spec) <span class="op">%&gt;%</span></span>
<span id="cb222-6"><a href="classification-continued.html#cb222-6"></a><span class="st">  </span><span class="kw">fit_resamples</span>(<span class="dt">resamples =</span> cancer_vfold) <span class="op">%&gt;%</span></span>
<span id="cb222-7"><a href="classification-continued.html#cb222-7"></a><span class="st">  </span><span class="kw">collect_metrics</span>()</span></code></pre></div>
<pre><code>## # A tibble: 2 x 5
##   .metric  .estimator  mean     n std_err
##   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
## 1 accuracy binary     0.885    10  0.0104
## 2 roc_auc  binary     0.927    10  0.0111</code></pre>
</div>
<div id="parameter-value-selection" class="section level3">
<h3><span class="header-section-number">7.4.2</span> Parameter value selection</h3>
<p>Using 5- and 10-fold cross-validation, we have estimated that the prediction
accuracy of our classifier is somewhere around 88%. Whether 88% is good or not
depends entirely on the downstream application of the data analysis. In the
present situation, we are trying to predict a tumour diagnosis, with expensive,
damaging chemo/radiation therapy or patient death as potential consequences of
misprediction. Hence, we’d like to do better than 88% for this application.</p>
<p>In order to improve our classifier, we have one choice of parameter: the number of
neighbours, <span class="math inline">\(K\)</span>. Since cross-validation helps us evaluate the accuracy of our
classifier, we can use cross-validation to calculate an accuracy for each value
of <span class="math inline">\(K\)</span> in a reasonable range, and then pick the value of <span class="math inline">\(K\)</span> that gives us the
best accuracy. The <code>tidymodels</code> package collection provides a very simple
syntax for tuning models: each parameter in the model to be tuned should be specified
as <code>tune()</code> in the model specification rather than given a particular value.</p>
<div class="sourceCode" id="cb224"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb224-1"><a href="classification-continued.html#cb224-1"></a>knn_spec &lt;-<span class="st"> </span><span class="kw">nearest_neighbor</span>(<span class="dt">weight_func =</span> <span class="st">&quot;rectangular&quot;</span>, <span class="dt">neighbors =</span> <span class="kw">tune</span>()) <span class="op">%&gt;%</span></span>
<span id="cb224-2"><a href="classification-continued.html#cb224-2"></a><span class="st">  </span><span class="kw">set_engine</span>(<span class="st">&quot;kknn&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb224-3"><a href="classification-continued.html#cb224-3"></a><span class="st">  </span><span class="kw">set_mode</span>(<span class="st">&quot;classification&quot;</span>)</span></code></pre></div>
<p>Then instead of using <code>fit</code> or <code>fit_resamples</code>, we will use the <code>tune_grid</code> function
to fit the model for each value in a range of parameter values. Here the <code>grid = 10</code>
argument specifies that the tuning should try 10 values of the number of neighbours
<span class="math inline">\(K\)</span> when tuning. We set the seed prior to tuning to ensure results are reproducible:</p>
<div class="sourceCode" id="cb225"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb225-1"><a href="classification-continued.html#cb225-1"></a><span class="kw">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb225-2"><a href="classification-continued.html#cb225-2"></a>knn_results &lt;-<span class="st"> </span><span class="kw">workflow</span>() <span class="op">%&gt;%</span></span>
<span id="cb225-3"><a href="classification-continued.html#cb225-3"></a><span class="st">  </span><span class="kw">add_recipe</span>(cancer_recipe) <span class="op">%&gt;%</span></span>
<span id="cb225-4"><a href="classification-continued.html#cb225-4"></a><span class="st">  </span><span class="kw">add_model</span>(knn_spec) <span class="op">%&gt;%</span></span>
<span id="cb225-5"><a href="classification-continued.html#cb225-5"></a><span class="st">  </span><span class="kw">tune_grid</span>(<span class="dt">resamples =</span> cancer_vfold, <span class="dt">grid =</span> <span class="dv">10</span>) <span class="op">%&gt;%</span></span>
<span id="cb225-6"><a href="classification-continued.html#cb225-6"></a><span class="st">  </span><span class="kw">collect_metrics</span>()</span>
<span id="cb225-7"><a href="classification-continued.html#cb225-7"></a>knn_results</span></code></pre></div>
<pre><code>## # A tibble: 20 x 6
##    neighbors .metric  .estimator  mean     n std_err
##        &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
##  1         2 accuracy binary     0.864    10 0.0167 
##  2         2 roc_auc  binary     0.897    10 0.0127 
##  3         3 accuracy binary     0.885    10 0.0104 
##  4         3 roc_auc  binary     0.927    10 0.0111 
##  5         5 accuracy binary     0.890    10 0.0136 
##  6         5 roc_auc  binary     0.927    10 0.00960
##  7         6 accuracy binary     0.890    10 0.0136 
##  8         6 roc_auc  binary     0.930    10 0.0111 
##  9         7 accuracy binary     0.890    10 0.00966
## 10         7 roc_auc  binary     0.931    10 0.0106 
## 11         9 accuracy binary     0.887    10 0.0104 
## 12         9 roc_auc  binary     0.935    10 0.0132 
## 13        10 accuracy binary     0.887    10 0.0104 
## 14        10 roc_auc  binary     0.934    10 0.0130 
## 15        12 accuracy binary     0.882    10 0.0164 
## 16        12 roc_auc  binary     0.940    10 0.0113 
## 17        13 accuracy binary     0.885    10 0.0170 
## 18        13 roc_auc  binary     0.939    10 0.0118 
## 19        15 accuracy binary     0.873    10 0.0153 
## 20        15 roc_auc  binary     0.940    10 0.0103</code></pre>
<p>We can select the best value of the number of neighbours (i.e., the one that results
in the highest classifier accuracy estimate) by plotting the accuracy versus <span class="math inline">\(K\)</span>:</p>
<div class="sourceCode" id="cb227"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb227-1"><a href="classification-continued.html#cb227-1"></a>accuracies &lt;-<span class="st"> </span>knn_results <span class="op">%&gt;%</span></span>
<span id="cb227-2"><a href="classification-continued.html#cb227-2"></a><span class="st">  </span><span class="kw">filter</span>(.metric <span class="op">==</span><span class="st"> &quot;accuracy&quot;</span>)</span>
<span id="cb227-3"><a href="classification-continued.html#cb227-3"></a></span>
<span id="cb227-4"><a href="classification-continued.html#cb227-4"></a>accuracy_vs_k &lt;-<span class="st"> </span><span class="kw">ggplot</span>(accuracies, <span class="kw">aes</span>(<span class="dt">x =</span> neighbors, <span class="dt">y =</span> mean)) <span class="op">+</span></span>
<span id="cb227-5"><a href="classification-continued.html#cb227-5"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb227-6"><a href="classification-continued.html#cb227-6"></a><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span></span>
<span id="cb227-7"><a href="classification-continued.html#cb227-7"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Neighbors&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Accuracy Estimate&quot;</span>)</span>
<span id="cb227-8"><a href="classification-continued.html#cb227-8"></a>accuracy_vs_k</span></code></pre></div>
<div class="figure"><span id="fig:06-find-k"></span>
<img src="_main_files/figure-html/06-find-k-1.png" alt="Plot of accuracy estimate versus number of neighbours" width="480" />
<p class="caption">
Figure 7.5: Plot of accuracy estimate versus number of neighbours
</p>
</div>
<p>This visualization suggests that <span class="math inline">\(K = 7\)</span> provides the highest accuracy.
But as you can see, there is no exact or perfect answer here;
any selection between <span class="math inline">\(K = 3\)</span> and <span class="math inline">\(13\)</span> would be reasonably justified, as all
of these differ in classifier accuracy by less than 1%. Remember: the
values you see on this plot are <em>estimates</em> of the true accuracy of our
classifier. Although the <span class="math inline">\(K=7\)</span> value is higher than the others on this plot,
that doesn’t mean the classifier is actually more accurate with this parameter
value! Generally, when selecting <span class="math inline">\(K\)</span> (and other parameters for other predictive
models), we are looking for a value where:</p>
<ul>
<li>we get roughly optimal accuracy, so that our model will likely be accurate</li>
<li>changing the value to a nearby one (e.g. from <span class="math inline">\(K=7\)</span> to 6 or 8) doesn’t decrease accuracy too much, so that our choice is reliable in the presence of uncertainty</li>
<li>the cost of training the model is not prohibitive (e.g., in our situation, if <span class="math inline">\(K\)</span> is too large, predicting becomes expensive!)</li>
</ul>
</div>
<div id="underoverfitting" class="section level3">
<h3><span class="header-section-number">7.4.3</span> Under/overfitting</h3>
<p>To build a bit more intuition, what happens if we keep increasing the number of neighbours <span class="math inline">\(K\)</span>? In fact, the accuracy
actually starts to decrease! Rather than setting <code>grid = 10</code> and letting <code>tidymodels</code> decide what values of <span class="math inline">\(K\)</span> to try,
let’s specify the values explicitly by creating a data frame with a <code>neighbors</code> variable.
Take a look as the plot below as we vary <span class="math inline">\(K\)</span> from 1 to almost the number of observations in the data set:</p>
<div class="sourceCode" id="cb228"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb228-1"><a href="classification-continued.html#cb228-1"></a><span class="kw">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb228-2"><a href="classification-continued.html#cb228-2"></a>k_lots &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">neighbors =</span> <span class="kw">seq</span>(<span class="dt">from =</span> <span class="dv">1</span>, <span class="dt">to =</span> <span class="dv">385</span>, <span class="dt">by =</span> <span class="dv">10</span>))</span>
<span id="cb228-3"><a href="classification-continued.html#cb228-3"></a>knn_results &lt;-<span class="st"> </span><span class="kw">workflow</span>() <span class="op">%&gt;%</span></span>
<span id="cb228-4"><a href="classification-continued.html#cb228-4"></a><span class="st">  </span><span class="kw">add_recipe</span>(cancer_recipe) <span class="op">%&gt;%</span></span>
<span id="cb228-5"><a href="classification-continued.html#cb228-5"></a><span class="st">  </span><span class="kw">add_model</span>(knn_spec) <span class="op">%&gt;%</span></span>
<span id="cb228-6"><a href="classification-continued.html#cb228-6"></a><span class="st">  </span><span class="kw">tune_grid</span>(<span class="dt">resamples =</span> cancer_vfold, <span class="dt">grid =</span> k_lots) <span class="op">%&gt;%</span></span>
<span id="cb228-7"><a href="classification-continued.html#cb228-7"></a><span class="st">  </span><span class="kw">collect_metrics</span>()</span>
<span id="cb228-8"><a href="classification-continued.html#cb228-8"></a></span>
<span id="cb228-9"><a href="classification-continued.html#cb228-9"></a>accuracies &lt;-<span class="st"> </span>knn_results <span class="op">%&gt;%</span></span>
<span id="cb228-10"><a href="classification-continued.html#cb228-10"></a><span class="st">  </span><span class="kw">filter</span>(.metric <span class="op">==</span><span class="st"> &quot;accuracy&quot;</span>)</span>
<span id="cb228-11"><a href="classification-continued.html#cb228-11"></a></span>
<span id="cb228-12"><a href="classification-continued.html#cb228-12"></a>accuracy_vs_k_lots &lt;-<span class="st"> </span><span class="kw">ggplot</span>(accuracies, <span class="kw">aes</span>(<span class="dt">x =</span> neighbors, <span class="dt">y =</span> mean)) <span class="op">+</span></span>
<span id="cb228-13"><a href="classification-continued.html#cb228-13"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb228-14"><a href="classification-continued.html#cb228-14"></a><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span></span>
<span id="cb228-15"><a href="classification-continued.html#cb228-15"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Neighbors&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Accuracy Estimate&quot;</span>)</span>
<span id="cb228-16"><a href="classification-continued.html#cb228-16"></a>accuracy_vs_k_lots</span></code></pre></div>
<div class="figure"><span id="fig:06-lots-of-ks"></span>
<img src="_main_files/figure-html/06-lots-of-ks-1.png" alt="Plot of accuracy estimate versus number of neighbours for many K values" width="480" />
<p class="caption">
Figure 7.6: Plot of accuracy estimate versus number of neighbours for many K values
</p>
</div>
<p><strong>Underfitting:</strong> What is actually happening to our classifier that causes
this? As we increase the number of neighbours, more and more of the training
observations (and those that are farther and farther away from the point) get a
“say” in what the class of a new observation is. This causes a sort of
“averaging effect” to take place, making the boundary between where our
classifier would predict a tumour to be malignant versus benign to smooth out
and become <em>simpler.</em> If you take this to the extreme, setting <span class="math inline">\(K\)</span> to the total
training data set size, then the classifier will always predict the same label
regardless of what the new observation looks like. In general, if the model
<em>isn’t influenced enough</em> by the training data, it is said to <strong>underfit</strong> the
data.</p>
<p><strong>Overfitting:</strong> In contrast, when we decrease the number of neighbours, each
individual data point has a stronger and stronger vote regarding nearby points.
Since the data themselves are noisy, this causes a more “jagged” boundary
corresponding to a <em>less simple</em> model. If you take this case to the extreme,
setting <span class="math inline">\(K = 1\)</span>, then the classifier is essentially just matching each new
observation to its closest neighbour in the training data set. This is just as
problematic as the large <span class="math inline">\(K\)</span> case, because the classifier becomes unreliable on
new data: if we had a different training set, the predictions would be
completely different. In general, if the model <em>is influenced too much</em> by the
training data, it is said to <strong>overfit</strong> the data.</p>
<p>You can see this effect in the plots below as we vary the number of neighbours <span class="math inline">\(K\)</span> in (1, 7, 20, 200):</p>
<center>
<div class="figure"><span id="fig:06-decision-grid-K"></span>
<img src="_main_files/figure-html/06-decision-grid-K-1.png" alt="Effect of K in overfitting and underfitting" width="960" />
<p class="caption">
Figure 7.7: Effect of K in overfitting and underfitting
</p>
</div>
</center>
</div>
</div>
<div id="splitting-data" class="section level2">
<h2><span class="header-section-number">7.5</span> Splitting data</h2>
<p><strong>Shuffling:</strong> When we split the data into train, test, and validation sets, we
make the assumption that there is no order to our originally collected data
set. However, if we think that there might be some order to the original data
set, then we can randomly shuffle the data before splitting it. The <code>tidymodels</code>
function <code>initial_split</code> and <code>vfold_cv</code> functions do this for us.</p>
<p><strong>Stratification:</strong> If the data are imbalanced, we also need to be extra
careful about splitting the data to ensure that enough of each class ends up in
each of the train, validation, and test partitions. The <code>strata</code> argument
in the <code>initial_split</code> and <code>vfold_cv</code> functions handles this for us too.</p>
</div>
<div id="summary" class="section level2">
<h2><span class="header-section-number">7.6</span> Summary</h2>
<p>Classification algorithms use one or more quantitative variables to predict the
value of a third, categorical variable. The K-nearest neighbour algorithm in
particular does this by first finding the K points in the training data nearest
to the new observation, and then returning the majority class vote from those
training observations. We can evaluate a classifier by splitting the data
randomly into a training and test data set, using the training set to build the
classifier, and using the test set to estimate its accuracy. To tune the
classifier (e.g., select the K in K-nearest neighbours), we maximize accuracy
estimates from cross-validation.</p>
<div class="figure"><span id="fig:06-overview"></span>
<img src="img/train-test-overview.jpeg" alt="Overview of K-nn classification" width="660" />
<p class="caption">
Figure 7.8: Overview of K-nn classification
</p>
</div>
<p>The overall workflow for performing K-nearest neighbour classification using <code>tidymodels</code> is as follows:</p>
<ol style="list-style-type: decimal">
<li>Use the <code>initial_split</code> function to split the data into a training and test set. Set the <code>strata</code> argument to the target variable. Put the test set aside for now.</li>
<li>Use the <code>vfold_cv</code> function to split up the training data for cross validation.</li>
<li>Create a <code>recipe</code> that specifies the target and predictor variables, as well as preprocessing steps for all variables. Pass the training data as the <code>data</code> argument of the recipe.</li>
<li>Create a <code>nearest_neighbors</code> model specification, with <code>neighbors = tune()</code>.</li>
<li>Add the recipe and model specification to a <code>workflow()</code>, and use the <code>tune_grid</code> function on the train/validation splits to estimate the classifier accuracy for a range of <span class="math inline">\(K\)</span> values.</li>
<li>Pick a value of <span class="math inline">\(K\)</span> that yields a high accuracy estimate that doesn’t change much if you change <span class="math inline">\(K\)</span> to a nearby value.</li>
<li>Make a new model specification for the best parameter value, and retrain the classifier using the <code>fit</code> function.</li>
<li>Evaluate the estimated accuracy of the classifier on the test set using the <code>predict</code> function.</li>
</ol>
<p><strong>Strengths:</strong></p>
<ol style="list-style-type: decimal">
<li>Simple and easy to understand</li>
<li>No assumptions about what the data must look like</li>
<li>Works easily for binary (two-class) and multi-class (&gt; 2 classes) classification problems</li>
</ol>
<p><strong>Weaknesses:</strong></p>
<ol style="list-style-type: decimal">
<li>As data gets bigger and bigger, K-nearest neighbour gets slower and slower, quite quickly</li>
<li>Does not perform well with a large number of predictors</li>
<li>Does not perform well when classes are imbalanced (when many more observations are in one of the classes compared to the others)</li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="classification.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="regression1.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/06-classification_continued.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
