<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 10 Clustering | Data Science: A First Introduction</title>
  <meta name="description" content="This is a textbook for teaching a first introduction to data science." />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 10 Clustering | Data Science: A First Introduction" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a textbook for teaching a first introduction to data science." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 10 Clustering | Data Science: A First Introduction" />
  
  <meta name="twitter:description" content="This is a textbook for teaching a first introduction to data science." />
  

<meta name="author" content="Tiffany-Anne Timbers" />
<meta name="author" content="Trevor Campbell" />
<meta name="author" content="Melissa Lee" />


<meta name="date" content="2021-05-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="regression2.html"/>
<link rel="next" href="inference.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.9.2.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.52.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.52.2/plotly-latest.min.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Data Science: A First Introduction</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> R, Jupyter, and the tidyverse</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#chapter-learning-objectives"><i class="fa fa-check"></i><b>1.1</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#jupyter-notebooks"><i class="fa fa-check"></i><b>1.2</b> Jupyter notebooks</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#loading-a-spreadsheet-like-dataset"><i class="fa fa-check"></i><b>1.3</b> Loading a spreadsheet-like dataset</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#assigning-value-to-a-data-frame"><i class="fa fa-check"></i><b>1.4</b> Assigning value to a data frame</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#creating-subsets-of-data-frames-with-select-filter"><i class="fa fa-check"></i><b>1.5</b> Creating subsets of data frames with <code>select</code> &amp; <code>filter</code></a><ul>
<li class="chapter" data-level="1.5.1" data-path="index.html"><a href="index.html#using-select-to-extract-multiple-columns"><i class="fa fa-check"></i><b>1.5.1</b> Using <code>select</code> to extract multiple columns</a></li>
<li class="chapter" data-level="1.5.2" data-path="index.html"><a href="index.html#using-select-to-extract-a-range-of-columns"><i class="fa fa-check"></i><b>1.5.2</b> Using <code>select</code> to extract a range of columns</a></li>
<li class="chapter" data-level="1.5.3" data-path="index.html"><a href="index.html#using-filter-to-extract-a-single-row"><i class="fa fa-check"></i><b>1.5.3</b> Using <code>filter</code> to extract a single row</a></li>
<li class="chapter" data-level="1.5.4" data-path="index.html"><a href="index.html#using-filter-to-extract-rows-with-values-above-a-threshold"><i class="fa fa-check"></i><b>1.5.4</b> Using <code>filter</code> to extract rows with values above a threshold</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#exploring-data-with-visualizations"><i class="fa fa-check"></i><b>1.6</b> Exploring data with visualizations</a><ul>
<li class="chapter" data-level="1.6.1" data-path="index.html"><a href="index.html#using-ggplot-to-create-a-scatter-plot"><i class="fa fa-check"></i><b>1.6.1</b> Using <code>ggplot</code> to create a scatter plot</a></li>
<li class="chapter" data-level="1.6.2" data-path="index.html"><a href="index.html#formatting-ggplot-objects"><i class="fa fa-check"></i><b>1.6.2</b> Formatting ggplot objects</a></li>
<li class="chapter" data-level="1.6.3" data-path="index.html"><a href="index.html#changing-the-units"><i class="fa fa-check"></i><b>1.6.3</b> Changing the units</a></li>
<li class="chapter" data-level="1.6.4" data-path="index.html"><a href="index.html#coloring-points-by-group"><i class="fa fa-check"></i><b>1.6.4</b> Coloring points by group</a></li>
<li class="chapter" data-level="1.6.5" data-path="index.html"><a href="index.html#putting-it-all-together"><i class="fa fa-check"></i><b>1.6.5</b> Putting it all together</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="reading.html"><a href="reading.html"><i class="fa fa-check"></i><b>2</b> Reading in data locally and from the web</a><ul>
<li class="chapter" data-level="2.1" data-path="reading.html"><a href="reading.html#overview"><i class="fa fa-check"></i><b>2.1</b> Overview</a></li>
<li class="chapter" data-level="2.2" data-path="reading.html"><a href="reading.html#chapter-learning-objectives-1"><i class="fa fa-check"></i><b>2.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="2.3" data-path="reading.html"><a href="reading.html#absolute-and-relative-file-paths"><i class="fa fa-check"></i><b>2.3</b> Absolute and relative file paths</a></li>
<li class="chapter" data-level="2.4" data-path="reading.html"><a href="reading.html#reading-tabular-data-from-a-plain-text-file-into-r"><i class="fa fa-check"></i><b>2.4</b> Reading tabular data from a plain text file into R</a><ul>
<li class="chapter" data-level="2.4.1" data-path="reading.html"><a href="reading.html#skipping-rows-when-reading-in-data"><i class="fa fa-check"></i><b>2.4.1</b> Skipping rows when reading in data</a></li>
<li class="chapter" data-level="2.4.2" data-path="reading.html"><a href="reading.html#read_delim-as-a-more-flexible-method-to-get-tabular-data-into-r"><i class="fa fa-check"></i><b>2.4.2</b> <code>read_delim</code> as a more flexible method to get tabular data into R</a></li>
<li class="chapter" data-level="2.4.3" data-path="reading.html"><a href="reading.html#reading-tabular-data-directly-from-a-url"><i class="fa fa-check"></i><b>2.4.3</b> Reading tabular data directly from a URL</a></li>
<li class="chapter" data-level="2.4.4" data-path="reading.html"><a href="reading.html#previewing-a-data-file-before-reading-it-into-r"><i class="fa fa-check"></i><b>2.4.4</b> Previewing a data file before reading it into R</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="reading.html"><a href="reading.html#reading-data-from-an-microsoft-excel-file"><i class="fa fa-check"></i><b>2.5</b> Reading data from an Microsoft Excel file</a></li>
<li class="chapter" data-level="2.6" data-path="reading.html"><a href="reading.html#reading-data-from-a-database"><i class="fa fa-check"></i><b>2.6</b> Reading data from a database</a><ul>
<li class="chapter" data-level="2.6.1" data-path="reading.html"><a href="reading.html#connecting-to-a-database"><i class="fa fa-check"></i><b>2.6.1</b> Connecting to a database</a></li>
<li class="chapter" data-level="2.6.2" data-path="reading.html"><a href="reading.html#interacting-with-a-database"><i class="fa fa-check"></i><b>2.6.2</b> Interacting with a database</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="reading.html"><a href="reading.html#writing-data-from-r-to-a-.csv-file"><i class="fa fa-check"></i><b>2.7</b> Writing data from R to a <code>.csv</code> file</a></li>
<li class="chapter" data-level="2.8" data-path="reading.html"><a href="reading.html#obtaining-data-from-the-web"><i class="fa fa-check"></i><b>2.8</b> Obtaining data from the web</a><ul>
<li class="chapter" data-level="2.8.1" data-path="reading.html"><a href="reading.html#web-scraping"><i class="fa fa-check"></i><b>2.8.1</b> Web scraping</a></li>
<li class="chapter" data-level="2.8.2" data-path="reading.html"><a href="reading.html#using-an-api"><i class="fa fa-check"></i><b>2.8.2</b> Using an API</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="reading.html"><a href="reading.html#additional-resources"><i class="fa fa-check"></i><b>2.9</b> Additional resources</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="wrangling.html"><a href="wrangling.html"><i class="fa fa-check"></i><b>3</b> Cleaning and wrangling data</a><ul>
<li class="chapter" data-level="3.1" data-path="wrangling.html"><a href="wrangling.html#overview-1"><i class="fa fa-check"></i><b>3.1</b> Overview</a></li>
<li class="chapter" data-level="3.2" data-path="wrangling.html"><a href="wrangling.html#chapter-learning-objectives-2"><i class="fa fa-check"></i><b>3.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="3.3" data-path="wrangling.html"><a href="wrangling.html#vectors-and-data-frames"><i class="fa fa-check"></i><b>3.3</b> Vectors and Data frames</a><ul>
<li class="chapter" data-level="3.3.1" data-path="wrangling.html"><a href="wrangling.html#what-is-a-data-frame"><i class="fa fa-check"></i><b>3.3.1</b> What is a data frame?</a></li>
<li class="chapter" data-level="3.3.2" data-path="wrangling.html"><a href="wrangling.html#what-is-a-vector"><i class="fa fa-check"></i><b>3.3.2</b> What is a vector?</a></li>
<li class="chapter" data-level="3.3.3" data-path="wrangling.html"><a href="wrangling.html#how-are-vectors-different-from-a-list"><i class="fa fa-check"></i><b>3.3.3</b> How are vectors different from a list?</a></li>
<li class="chapter" data-level="3.3.4" data-path="wrangling.html"><a href="wrangling.html#what-does-this-have-to-do-with-data-frames"><i class="fa fa-check"></i><b>3.3.4</b> What does this have to do with data frames?</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="wrangling.html"><a href="wrangling.html#tidy-data"><i class="fa fa-check"></i><b>3.4</b> Tidy Data</a><ul>
<li class="chapter" data-level="3.4.1" data-path="wrangling.html"><a href="wrangling.html#what-is-tidy-data"><i class="fa fa-check"></i><b>3.4.1</b> What is tidy data?</a></li>
<li class="chapter" data-level="3.4.2" data-path="wrangling.html"><a href="wrangling.html#why-is-tidy-data-important-in-r"><i class="fa fa-check"></i><b>3.4.2</b> Why is tidy data important in R?</a></li>
<li class="chapter" data-level="3.4.3" data-path="wrangling.html"><a href="wrangling.html#going-from-wide-to-long-or-tidy-using-pivot_longer"><i class="fa fa-check"></i><b>3.4.3</b> Going from wide to long (or tidy!) using <code>pivot_longer</code></a></li>
<li class="chapter" data-level="3.4.4" data-path="wrangling.html"><a href="wrangling.html#going-from-long-to-wide-using-pivot_wider"><i class="fa fa-check"></i><b>3.4.4</b> Going from long to wide using <code>pivot_wider</code></a></li>
<li class="chapter" data-level="3.4.5" data-path="wrangling.html"><a href="wrangling.html#using-separate-to-deal-with-multiple-delimiters"><i class="fa fa-check"></i><b>3.4.5</b> Using <code>separate</code> to deal with multiple delimiters</a></li>
<li class="chapter" data-level="3.4.6" data-path="wrangling.html"><a href="wrangling.html#notes-on-defining-tidy-data"><i class="fa fa-check"></i><b>3.4.6</b> Notes on defining tidy data</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="wrangling.html"><a href="wrangling.html#combining-functions-using-the-pipe-operator"><i class="fa fa-check"></i><b>3.5</b> Combining functions using the pipe operator, <code>%&gt;%</code>:</a><ul>
<li class="chapter" data-level="3.5.1" data-path="wrangling.html"><a href="wrangling.html#using-to-combine-filter-and-select"><i class="fa fa-check"></i><b>3.5.1</b> Using <code>%&gt;%</code> to combine <code>filter</code> and <code>select</code></a></li>
<li class="chapter" data-level="3.5.2" data-path="wrangling.html"><a href="wrangling.html#using-with-more-than-two-functions"><i class="fa fa-check"></i><b>3.5.2</b> Using <code>%&gt;%</code> with more than two functions</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="wrangling.html"><a href="wrangling.html#iterating-over-data-with-group_by-summarize"><i class="fa fa-check"></i><b>3.6</b> Iterating over data with <code>group_by</code> + <code>summarize</code></a><ul>
<li class="chapter" data-level="3.6.1" data-path="wrangling.html"><a href="wrangling.html#calculating-summary-statistics"><i class="fa fa-check"></i><b>3.6.1</b> Calculating summary statistics:</a></li>
<li class="chapter" data-level="3.6.2" data-path="wrangling.html"><a href="wrangling.html#calculating-group-summary-statistics"><i class="fa fa-check"></i><b>3.6.2</b> Calculating group summary statistics:</a></li>
<li class="chapter" data-level="3.6.3" data-path="wrangling.html"><a href="wrangling.html#additional-reading-on-the-dplyr-functions"><i class="fa fa-check"></i><b>3.6.3</b> Additional reading on the <code>dplyr</code> functions</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="wrangling.html"><a href="wrangling.html#using-purrrs-map-functions-to-iterate"><i class="fa fa-check"></i><b>3.7</b> Using <code>purrr</code>’s <code>map*</code> functions to iterate</a></li>
<li class="chapter" data-level="3.8" data-path="wrangling.html"><a href="wrangling.html#additional-resources-1"><i class="fa fa-check"></i><b>3.8</b> Additional resources</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="viz.html"><a href="viz.html"><i class="fa fa-check"></i><b>4</b> Effective data visualization</a><ul>
<li class="chapter" data-level="4.1" data-path="viz.html"><a href="viz.html#overview-2"><i class="fa fa-check"></i><b>4.1</b> Overview</a></li>
<li class="chapter" data-level="4.2" data-path="viz.html"><a href="viz.html#chapter-learning-objectives-3"><i class="fa fa-check"></i><b>4.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="4.3" data-path="viz.html"><a href="viz.html#choosing-the-visualization"><i class="fa fa-check"></i><b>4.3</b> Choosing the visualization</a></li>
<li class="chapter" data-level="4.4" data-path="viz.html"><a href="viz.html#refining-the-visualization"><i class="fa fa-check"></i><b>4.4</b> Refining the visualization</a></li>
<li class="chapter" data-level="4.5" data-path="viz.html"><a href="viz.html#creating-visualizations-with-ggplot2"><i class="fa fa-check"></i><b>4.5</b> Creating visualizations with <code>ggplot2</code></a><ul>
<li class="chapter" data-level="4.5.1" data-path="viz.html"><a href="viz.html#the-mauna-loa-co2-data-set"><i class="fa fa-check"></i><b>4.5.1</b> The Mauna Loa CO2 data set</a></li>
<li class="chapter" data-level="4.5.2" data-path="viz.html"><a href="viz.html#the-island-landmass-data-set"><i class="fa fa-check"></i><b>4.5.2</b> The island landmass data set</a></li>
<li class="chapter" data-level="4.5.3" data-path="viz.html"><a href="viz.html#the-old-faithful-eruptionwaiting-time-data-set"><i class="fa fa-check"></i><b>4.5.3</b> The Old Faithful eruption/waiting time data set</a></li>
<li class="chapter" data-level="4.5.4" data-path="viz.html"><a href="viz.html#the-michelson-speed-of-light-data-set"><i class="fa fa-check"></i><b>4.5.4</b> The Michelson speed of light data set</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="viz.html"><a href="viz.html#explaining-the-visualization"><i class="fa fa-check"></i><b>4.6</b> Explaining the visualization</a></li>
<li class="chapter" data-level="4.7" data-path="viz.html"><a href="viz.html#saving-the-visualization"><i class="fa fa-check"></i><b>4.7</b> Saving the visualization</a></li>
<li class="chapter" data-level="4.8" data-path="viz.html"><a href="viz.html#additional-resources-2"><i class="fa fa-check"></i><b>4.8</b> Additional resources</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="version-control.html"><a href="version-control.html"><i class="fa fa-check"></i><b>5</b> Collaboration with version control</a><ul>
<li class="chapter" data-level="5.1" data-path="version-control.html"><a href="version-control.html#overview-3"><i class="fa fa-check"></i><b>5.1</b> Overview</a></li>
<li class="chapter" data-level="5.2" data-path="version-control.html"><a href="version-control.html#chapter-learning-objectives-4"><i class="fa fa-check"></i><b>5.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="5.3" data-path="version-control.html"><a href="version-control.html#what-is-version-control-and-why-should-i-use-it"><i class="fa fa-check"></i><b>5.3</b> What is version control, and why should I use it?</a></li>
<li class="chapter" data-level="5.4" data-path="version-control.html"><a href="version-control.html#creating-a-space-for-your-project-online"><i class="fa fa-check"></i><b>5.4</b> Creating a space for your project online</a></li>
<li class="chapter" data-level="5.5" data-path="version-control.html"><a href="version-control.html#creating-and-editing-files-on-github"><i class="fa fa-check"></i><b>5.5</b> Creating and editing files on GitHub</a><ul>
<li class="chapter" data-level="5.5.1" data-path="version-control.html"><a href="version-control.html#the-pen-tool"><i class="fa fa-check"></i><b>5.5.1</b> The pen tool</a></li>
<li class="chapter" data-level="5.5.2" data-path="version-control.html"><a href="version-control.html#the-add-file-menu"><i class="fa fa-check"></i><b>5.5.2</b> The “Add file” menu</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="version-control.html"><a href="version-control.html#cloning-your-repository-on-jupyterhub"><i class="fa fa-check"></i><b>5.6</b> Cloning your repository on JupyterHub</a></li>
<li class="chapter" data-level="5.7" data-path="version-control.html"><a href="version-control.html#working-in-a-cloned-repository-on-jupyterhub"><i class="fa fa-check"></i><b>5.7</b> Working in a cloned repository on JupyterHub</a><ul>
<li class="chapter" data-level="5.7.1" data-path="version-control.html"><a href="version-control.html#specifying-files-to-commit"><i class="fa fa-check"></i><b>5.7.1</b> Specifying files to commit</a></li>
<li class="chapter" data-level="5.7.2" data-path="version-control.html"><a href="version-control.html#making-the-commit"><i class="fa fa-check"></i><b>5.7.2</b> Making the commit</a></li>
<li class="chapter" data-level="5.7.3" data-path="version-control.html"><a href="version-control.html#pushing-the-commits-to-github"><i class="fa fa-check"></i><b>5.7.3</b> Pushing the commits to GitHub</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="version-control.html"><a href="version-control.html#collaboration"><i class="fa fa-check"></i><b>5.8</b> Collaboration</a><ul>
<li class="chapter" data-level="5.8.1" data-path="version-control.html"><a href="version-control.html#giving-collaborators-access-to-your-project"><i class="fa fa-check"></i><b>5.8.1</b> Giving collaborators access to your project</a></li>
<li class="chapter" data-level="5.8.2" data-path="version-control.html"><a href="version-control.html#pulling-changes-from-github"><i class="fa fa-check"></i><b>5.8.2</b> Pulling changes from GitHub</a></li>
<li class="chapter" data-level="5.8.3" data-path="version-control.html"><a href="version-control.html#handling-merge-conflicts"><i class="fa fa-check"></i><b>5.8.3</b> Handling merge conflicts</a></li>
<li class="chapter" data-level="5.8.4" data-path="version-control.html"><a href="version-control.html#communicating-using-github-issues"><i class="fa fa-check"></i><b>5.8.4</b> Communicating using GitHub issues</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="version-control.html"><a href="version-control.html#additional-resources-3"><i class="fa fa-check"></i><b>5.9</b> Additional resources</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>6</b> Classification I: training &amp; predicting</a><ul>
<li class="chapter" data-level="6.1" data-path="classification.html"><a href="classification.html#overview-4"><i class="fa fa-check"></i><b>6.1</b> Overview</a></li>
<li class="chapter" data-level="6.2" data-path="classification.html"><a href="classification.html#chapter-learning-objectives-5"><i class="fa fa-check"></i><b>6.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="6.3" data-path="classification.html"><a href="classification.html#the-classification-problem"><i class="fa fa-check"></i><b>6.3</b> The classification problem</a></li>
<li class="chapter" data-level="6.4" data-path="classification.html"><a href="classification.html#exploring-a-labelled-data-set"><i class="fa fa-check"></i><b>6.4</b> Exploring a labelled data set</a></li>
<li class="chapter" data-level="6.5" data-path="classification.html"><a href="classification.html#classification-with-k-nearest-neighbours"><i class="fa fa-check"></i><b>6.5</b> Classification with K-nearest neighbours</a></li>
<li class="chapter" data-level="6.6" data-path="classification.html"><a href="classification.html#k-nearest-neighbours-with-tidymodels"><i class="fa fa-check"></i><b>6.6</b> K-nearest neighbours with <code>tidymodels</code></a></li>
<li class="chapter" data-level="6.7" data-path="classification.html"><a href="classification.html#data-preprocessing-with-tidymodels"><i class="fa fa-check"></i><b>6.7</b> Data preprocessing with <code>tidymodels</code></a><ul>
<li class="chapter" data-level="6.7.1" data-path="classification.html"><a href="classification.html#centering-and-scaling"><i class="fa fa-check"></i><b>6.7.1</b> Centering and scaling</a></li>
<li class="chapter" data-level="6.7.2" data-path="classification.html"><a href="classification.html#balancing"><i class="fa fa-check"></i><b>6.7.2</b> Balancing</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="classification.html"><a href="classification.html#putting-it-together-in-a-workflow"><i class="fa fa-check"></i><b>6.8</b> Putting it together in a <code>workflow</code></a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="classification-continued.html"><a href="classification-continued.html"><i class="fa fa-check"></i><b>7</b> Classification II: evaluation &amp; tuning</a><ul>
<li class="chapter" data-level="7.1" data-path="classification-continued.html"><a href="classification-continued.html#overview-5"><i class="fa fa-check"></i><b>7.1</b> Overview</a></li>
<li class="chapter" data-level="7.2" data-path="classification-continued.html"><a href="classification-continued.html#chapter-learning-objectives-6"><i class="fa fa-check"></i><b>7.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="7.3" data-path="classification-continued.html"><a href="classification-continued.html#evaluating-accuracy"><i class="fa fa-check"></i><b>7.3</b> Evaluating accuracy</a></li>
<li class="chapter" data-level="7.4" data-path="classification-continued.html"><a href="classification-continued.html#tuning-the-classifier"><i class="fa fa-check"></i><b>7.4</b> Tuning the classifier</a><ul>
<li class="chapter" data-level="7.4.1" data-path="classification-continued.html"><a href="classification-continued.html#cross-validation"><i class="fa fa-check"></i><b>7.4.1</b> Cross-validation</a></li>
<li class="chapter" data-level="7.4.2" data-path="classification-continued.html"><a href="classification-continued.html#parameter-value-selection"><i class="fa fa-check"></i><b>7.4.2</b> Parameter value selection</a></li>
<li class="chapter" data-level="7.4.3" data-path="classification-continued.html"><a href="classification-continued.html#underoverfitting"><i class="fa fa-check"></i><b>7.4.3</b> Under/overfitting</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="classification-continued.html"><a href="classification-continued.html#splitting-data"><i class="fa fa-check"></i><b>7.5</b> Splitting data</a></li>
<li class="chapter" data-level="7.6" data-path="classification-continued.html"><a href="classification-continued.html#summary"><i class="fa fa-check"></i><b>7.6</b> Summary</a></li>
<li class="chapter" data-level="7.7" data-path="classification-continued.html"><a href="classification-continued.html#additional-resources-4"><i class="fa fa-check"></i><b>7.7</b> Additional resources</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="regression1.html"><a href="regression1.html"><i class="fa fa-check"></i><b>8</b> Regression I: K-nearest neighbours</a><ul>
<li class="chapter" data-level="8.1" data-path="regression1.html"><a href="regression1.html#overview-6"><i class="fa fa-check"></i><b>8.1</b> Overview</a></li>
<li class="chapter" data-level="8.2" data-path="regression1.html"><a href="regression1.html#chapter-learning-objectives-7"><i class="fa fa-check"></i><b>8.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="8.3" data-path="regression1.html"><a href="regression1.html#regression"><i class="fa fa-check"></i><b>8.3</b> Regression</a></li>
<li class="chapter" data-level="8.4" data-path="regression1.html"><a href="regression1.html#exploring-a-labelled-data-set-1"><i class="fa fa-check"></i><b>8.4</b> Exploring a labelled data set</a></li>
<li class="chapter" data-level="8.5" data-path="regression1.html"><a href="regression1.html#k-nearest-neighbours-regression"><i class="fa fa-check"></i><b>8.5</b> K-nearest neighbours regression</a></li>
<li class="chapter" data-level="8.6" data-path="regression1.html"><a href="regression1.html#training-evaluating-and-tuning-the-model"><i class="fa fa-check"></i><b>8.6</b> Training, evaluating, and tuning the model</a></li>
<li class="chapter" data-level="8.7" data-path="regression1.html"><a href="regression1.html#underfitting-and-overfitting"><i class="fa fa-check"></i><b>8.7</b> Underfitting and overfitting</a></li>
<li class="chapter" data-level="8.8" data-path="regression1.html"><a href="regression1.html#evaluating-on-the-test-set"><i class="fa fa-check"></i><b>8.8</b> Evaluating on the test set</a></li>
<li class="chapter" data-level="8.9" data-path="regression1.html"><a href="regression1.html#strengths-and-limitations-of-k-nn-regression"><i class="fa fa-check"></i><b>8.9</b> Strengths and limitations of K-NN regression</a></li>
<li class="chapter" data-level="8.10" data-path="regression1.html"><a href="regression1.html#multivariate-k-nn-regression"><i class="fa fa-check"></i><b>8.10</b> Multivariate K-NN regression</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="regression2.html"><a href="regression2.html"><i class="fa fa-check"></i><b>9</b> Regression II: linear regression</a><ul>
<li class="chapter" data-level="9.1" data-path="regression2.html"><a href="regression2.html#overview-7"><i class="fa fa-check"></i><b>9.1</b> Overview</a></li>
<li class="chapter" data-level="9.2" data-path="regression2.html"><a href="regression2.html#chapter-learning-objectives-8"><i class="fa fa-check"></i><b>9.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="9.3" data-path="regression2.html"><a href="regression2.html#simple-linear-regression"><i class="fa fa-check"></i><b>9.3</b> Simple linear regression</a></li>
<li class="chapter" data-level="9.4" data-path="regression2.html"><a href="regression2.html#linear-regression-in-r"><i class="fa fa-check"></i><b>9.4</b> Linear regression in R</a></li>
<li class="chapter" data-level="9.5" data-path="regression2.html"><a href="regression2.html#comparing-simple-linear-and-k-nn-regression"><i class="fa fa-check"></i><b>9.5</b> Comparing simple linear and K-NN regression</a></li>
<li class="chapter" data-level="9.6" data-path="regression2.html"><a href="regression2.html#multivariate-linear-regression"><i class="fa fa-check"></i><b>9.6</b> Multivariate linear regression</a></li>
<li class="chapter" data-level="9.7" data-path="regression2.html"><a href="regression2.html#the-other-side-of-regression"><i class="fa fa-check"></i><b>9.7</b> The other side of regression</a></li>
<li class="chapter" data-level="9.8" data-path="regression2.html"><a href="regression2.html#additional-resources-5"><i class="fa fa-check"></i><b>9.8</b> Additional resources</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>10</b> Clustering</a><ul>
<li class="chapter" data-level="10.1" data-path="clustering.html"><a href="clustering.html#overview-8"><i class="fa fa-check"></i><b>10.1</b> Overview</a></li>
<li class="chapter" data-level="10.2" data-path="clustering.html"><a href="clustering.html#chapter-learning-objectives-9"><i class="fa fa-check"></i><b>10.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="10.3" data-path="clustering.html"><a href="clustering.html#clustering-1"><i class="fa fa-check"></i><b>10.3</b> Clustering</a></li>
<li class="chapter" data-level="10.4" data-path="clustering.html"><a href="clustering.html#k-means"><i class="fa fa-check"></i><b>10.4</b> K-means</a><ul>
<li class="chapter" data-level="10.4.1" data-path="clustering.html"><a href="clustering.html#measuring-cluster-quality"><i class="fa fa-check"></i><b>10.4.1</b> Measuring cluster quality</a></li>
<li class="chapter" data-level="10.4.2" data-path="clustering.html"><a href="clustering.html#the-clustering-algorithm"><i class="fa fa-check"></i><b>10.4.2</b> The clustering algorithm</a></li>
<li class="chapter" data-level="10.4.3" data-path="clustering.html"><a href="clustering.html#random-restarts"><i class="fa fa-check"></i><b>10.4.3</b> Random restarts</a></li>
<li class="chapter" data-level="10.4.4" data-path="clustering.html"><a href="clustering.html#choosing-k"><i class="fa fa-check"></i><b>10.4.4</b> Choosing K</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="clustering.html"><a href="clustering.html#data-pre-processing-for-k-means"><i class="fa fa-check"></i><b>10.5</b> Data pre-processing for K-means</a></li>
<li class="chapter" data-level="10.6" data-path="clustering.html"><a href="clustering.html#k-means-in-r"><i class="fa fa-check"></i><b>10.6</b> K-means in R</a></li>
<li class="chapter" data-level="10.7" data-path="clustering.html"><a href="clustering.html#additional-resources-6"><i class="fa fa-check"></i><b>10.7</b> Additional resources</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>11</b> Introduction to Statistical Inference</a><ul>
<li class="chapter" data-level="11.1" data-path="inference.html"><a href="inference.html#overview-9"><i class="fa fa-check"></i><b>11.1</b> Overview</a></li>
<li class="chapter" data-level="11.2" data-path="inference.html"><a href="inference.html#chapter-learning-objectives-10"><i class="fa fa-check"></i><b>11.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="11.3" data-path="inference.html"><a href="inference.html#why-do-we-need-sampling"><i class="fa fa-check"></i><b>11.3</b> Why do we need sampling?</a></li>
<li class="chapter" data-level="11.4" data-path="inference.html"><a href="inference.html#sampling-distributions"><i class="fa fa-check"></i><b>11.4</b> Sampling distributions</a><ul>
<li class="chapter" data-level="11.4.1" data-path="inference.html"><a href="inference.html#sampling-distributions-for-proportions"><i class="fa fa-check"></i><b>11.4.1</b> Sampling distributions for proportions</a></li>
<li class="chapter" data-level="11.4.2" data-path="inference.html"><a href="inference.html#sampling-distributions-for-means"><i class="fa fa-check"></i><b>11.4.2</b> Sampling distributions for means</a></li>
<li class="chapter" data-level="11.4.3" data-path="inference.html"><a href="inference.html#summary-1"><i class="fa fa-check"></i><b>11.4.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="inference.html"><a href="inference.html#bootstrapping"><i class="fa fa-check"></i><b>11.5</b> Bootstrapping</a><ul>
<li class="chapter" data-level="11.5.1" data-path="inference.html"><a href="inference.html#overview-10"><i class="fa fa-check"></i><b>11.5.1</b> Overview</a></li>
<li class="chapter" data-level="11.5.2" data-path="inference.html"><a href="inference.html#bootstrapping-in-r"><i class="fa fa-check"></i><b>11.5.2</b> Bootstrapping in R</a></li>
<li class="chapter" data-level="11.5.3" data-path="inference.html"><a href="inference.html#using-the-bootstrap-to-calculate-a-plausible-range"><i class="fa fa-check"></i><b>11.5.3</b> Using the bootstrap to calculate a plausible range</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="inference.html"><a href="inference.html#additional-resources-7"><i class="fa fa-check"></i><b>11.6</b> Additional resources</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="move-to-your-own-machine.html"><a href="move-to-your-own-machine.html"><i class="fa fa-check"></i><b>12</b> Moving to your own machine</a><ul>
<li class="chapter" data-level="12.1" data-path="move-to-your-own-machine.html"><a href="move-to-your-own-machine.html#overview-11"><i class="fa fa-check"></i><b>12.1</b> Overview</a></li>
<li class="chapter" data-level="12.2" data-path="move-to-your-own-machine.html"><a href="move-to-your-own-machine.html#chapter-learning-objectives-11"><i class="fa fa-check"></i><b>12.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="12.3" data-path="move-to-your-own-machine.html"><a href="move-to-your-own-machine.html#installing-software-on-your-own-computer"><i class="fa fa-check"></i><b>12.3</b> Installing software on your own computer</a><ul>
<li class="chapter" data-level="12.3.1" data-path="move-to-your-own-machine.html"><a href="move-to-your-own-machine.html#git"><i class="fa fa-check"></i><b>12.3.1</b> Git</a></li>
<li class="chapter" data-level="12.3.2" data-path="move-to-your-own-machine.html"><a href="move-to-your-own-machine.html#miniconda"><i class="fa fa-check"></i><b>12.3.2</b> Miniconda</a></li>
<li class="chapter" data-level="12.3.3" data-path="move-to-your-own-machine.html"><a href="move-to-your-own-machine.html#jupyterlab"><i class="fa fa-check"></i><b>12.3.3</b> JupyterLab</a></li>
<li class="chapter" data-level="12.3.4" data-path="move-to-your-own-machine.html"><a href="move-to-your-own-machine.html#r-and-the-irkernel"><i class="fa fa-check"></i><b>12.3.4</b> R and the IRkernel</a></li>
<li class="chapter" data-level="12.3.5" data-path="move-to-your-own-machine.html"><a href="move-to-your-own-machine.html#r-packages"><i class="fa fa-check"></i><b>12.3.5</b> R packages</a></li>
<li class="chapter" data-level="12.3.6" data-path="move-to-your-own-machine.html"><a href="move-to-your-own-machine.html#latex"><i class="fa fa-check"></i><b>12.3.6</b> LaTeX</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="move-to-your-own-machine.html"><a href="move-to-your-own-machine.html#moving-files-to-your-computer"><i class="fa fa-check"></i><b>12.4</b> Moving files to your computer</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Science: A First Introduction</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="clustering" class="section level1">
<h1><span class="header-section-number">Chapter 10</span> Clustering</h1>
<div id="overview-8" class="section level2">
<h2><span class="header-section-number">10.1</span> Overview</h2>
<p>As part of exploratory data analysis, it is often helpful to see if there are
meaningful subgroups (or <em>clusters</em>) in the data; this grouping can be used
for many purposes, such as generating new questions or improving predictive analyses.
This chapter provides an introduction to clustering using the <em>K-means</em> algorithm,
including techniques to choose the number of clusters.</p>
</div>
<div id="chapter-learning-objectives-9" class="section level2">
<h2><span class="header-section-number">10.2</span> Chapter learning objectives</h2>
<p>By the end of the chapter, students will be able to:</p>
<ul>
<li>Describe a case where clustering is appropriate, and what insight it might extract from the data.</li>
<li>Explain the K-means clustering algorithm.</li>
<li>Interpret the output of a K-means analysis.</li>
<li>Identify when it is necessary to scale variables before clustering, and do this using R.</li>
<li>Perform K-means clustering in R using <code>kmeans</code>.</li>
<li>Use the elbow method to choose the number of clusters for K-means.</li>
<li>Visualize the output of K-means clustering in R using coloured scatter plots.</li>
<li>Describe the advantages, limitations and assumptions of the K-means clustering algorithm.</li>
</ul>
</div>
<div id="clustering-1" class="section level2">
<h2><span class="header-section-number">10.3</span> Clustering</h2>
<p>Clustering is a data analysis task involving separating a data set into
subgroups of related data. For example, we might use clustering to separate a
data set of documents into groups that correspond to topics, a data set of
human genetic information into groups that correspond to ancestral
subpopulations, or a data set of online customers into groups that correspond
to purchasing behaviours. Once the data are separated, we can, for example,
use the subgroups to generate new questions about the data and follow up with a
predictive modelling exercise. In this course, clustering will be used only for
exploratory analysis, i.e., uncovering patterns in the data.</p>
<p>Note that clustering is a fundamentally different kind of task than classification or
regression. In particular, both classification and regression are <em>supervised tasks</em> where
there is a <em>predictive target</em> (a class label or value), and we
have examples of past data with labels/values that help us predict those of
future data. By contrast, clustering is an <em>unsupervised task</em>, as we are
trying to understand and examine the structure of data without any labels to
help us. This approach has both advantages and disadvantages. Clustering
requires no additional annotation or input on the data. For example, it would
be nearly impossible to annotate all the articles on Wikipedia with human-made
topic labels. However, we can still cluster the articles without this
information to find groupings corresponding to topics automatically.</p>
<p>However, because there is no predictive target, it is not as easy to evaluate
the “quality” of a clustering. With classification, we can use a test data set
to assess prediction performance. In clustering, there is not a single good
choice for evaluation. In this book, we will use visualization to ascertain the
quality of a clustering, and leave rigorous evaluation for more advanced
courses.</p>
<blockquote>
<p>There are also so-called <em>semisupervised</em> tasks, where only some of the data
come with labels/annotations, but the vast majority don’t. The goal
is to try to uncover underlying structure in the data that allows one to
guess the missing labels. This sort of task is beneficial, for example, when one
has an unlabelled data set that is too large to manually label, but one is willing to
provide a few informative example labels as a “seed” to guess the labels for all the data.</p>
</blockquote>
<p><strong>An illustrative example</strong></p>
Here we will present an illustrative example using a data set from the
<a href="https://allisonhorst.github.io/palmerpenguins/">{palmerpenguins} R data package</a>. This data set was
collected by <a href="https://www.uaf.edu/cfos/people/faculty/detail/kristen-gorman.php">Dr. Kristen Gorman</a> and
the <a href="https://lternet.edu/">Palmer Station, Antarctica LTER</a> and includes
measurements for adult penguins near Palmer Station <span class="citation">(Horst, Hill, and Gorman <a href="#ref-palmerpenguins" role="doc-biblioref">2020</a>)</span>. We have
modified the data set for use in this chapter. Here we will focus on using two
variables—penguin bill and flipper length, both in mm—to determine whether
there are distinct types of penguins in our data.
Understanding this might help us with species discovery and classification in a data-driven
way.
<div class="figure"><span id="fig:09-penguins"></span>
<img src="https://upload.wikimedia.org/wikipedia/commons/0/00/Brown_Bluff-2016-Tabarin_Peninsula%E2%80%93Gentoo_penguin_%28Pygoscelis_papua%29_03.jpg" alt="Gentoo penguin. [@penguinsimage]"  />
<p class="caption">
Figure 10.1: Gentoo penguin. <span class="citation">(Andrew Shiva <a href="#ref-penguinsimage" role="doc-biblioref">2016</a>)</span>
</p>
</div>
<p>Below we will work with <code>penguin_data</code>, a subset of 18 observations of the original data, which has already been scaled. We will discuss scaling for K-means in more detail later in this chapter.</p>
<div class="sourceCode" id="cb286"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb286-1"><a href="clustering.html#cb286-1"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb286-2"><a href="clustering.html#cb286-2"></a>penguin_data</span></code></pre></div>
<pre><code>## # A tibble: 18 x 2
##    flipper_length_scaled[,1] bill_length_scaled[,1]
##                        &lt;dbl&gt;                  &lt;dbl&gt;
##  1                    -0.190                 -0.641
##  2                    -1.33                  -1.14 
##  3                    -0.922                 -1.52 
##  4                    -0.922                 -1.11 
##  5                    -1.41                  -0.847
##  6                    -0.678                 -0.641
##  7                    -0.271                 -1.24 
##  8                    -0.434                 -0.902
##  9                     1.19                   0.720
## 10                     1.36                   0.646
## 11                     1.36                   0.963
## 12                     1.76                   0.440
## 13                     1.11                   1.21 
## 14                     0.786                  0.123
## 15                    -0.271                  0.627
## 16                    -0.271                  0.757
## 17                    -0.108                  1.78 
## 18                    -0.759                  0.776</code></pre>
<div class="figure"><span id="fig:10-toy-example-plot"></span>
<img src="_main_files/figure-html/10-toy-example-plot-1.png" alt="Subset data of penguin scaled bill length versus flipper length." width="417.6" />
<p class="caption">
Figure 10.2: Subset data of penguin scaled bill length versus flipper length.
</p>
</div>
<p>Based on the visualization in Figure <a href="clustering.html#fig:10-toy-example-plot">10.2</a>, we might
suspect there are a few subtypes of penguins, selected from combinations of
high/low flipper length and high/low bill length. How do we find this grouping
automatically, and how do we pick the number of subtypes? The way to rigorously
separate the data into groups is to use a clustering algorithm.
In this chapter, we will focus on the <em>K-means</em> algorithm, a widely-used
and often very effective clustering method, combined with the <em>elbow method</em> for
selecting the number of clusters. This procedure will separate the data into
the following groups denoted by colour:</p>
<div class="figure"><span id="fig:10-toy-example-clustering"></span>
<img src="_main_files/figure-html/10-toy-example-clustering-1.png" alt="Subset data of penguin scaled bill length versus flipper length with coloured groups" width="480" />
<p class="caption">
Figure 10.3: Subset data of penguin scaled bill length versus flipper length with coloured groups
</p>
</div>
<p>What are the labels for these groups? Unfortunately, we don’t have any. K-means,
like almost all clustering algorithms, just outputs meaningless “cluster labels”
that are typically whole numbers: 1, 2, 3, etc. But in a simple case like this,
where we can easily visualize the clusters on a scatter plot, we can give
human-made labels to the groups using their positions on
the plot:</p>
<ul>
<li>small flipper length and small bill length (<font color="#D55E00">orange cluster</font>),</li>
<li>small flipper length and large bill length (<font color="#0072B2">blue cluster</font>).</li>
<li>and large flipper length and large bill length (<font color="#F0E442">yellow cluster</font>).</li>
</ul>
<p>Once we have made these determinations, we can use them to inform our species
classifications or ask further questions about our data. For example, we might
be interested in understanding the relationship between flipper length and bill
length, and that relationship may differ depending on the type of penguin we
have.</p>
</div>
<div id="k-means" class="section level2">
<h2><span class="header-section-number">10.4</span> K-means</h2>
<div id="measuring-cluster-quality" class="section level3">
<h3><span class="header-section-number">10.4.1</span> Measuring cluster quality</h3>
<p>The K-means algorithm is a procedure that groups data into K clusters.
It starts with an initial clustering of the data, and then iteratively
improves it by making adjustments to the assignment of data
to clusters until it cannot improve any further. But how do we measure
the “quality” of a clustering, and what does it mean to improve it?
In K-means clustering, we measure the quality of a cluster by its
<em>within-cluster sum-of-squared-distances</em> (WSSD). Computing this involves two steps.
First, we find the cluster centers by computing the mean of each variable
over data points in the cluster. For example, suppose we have a
cluster containing four observations, and we are using two variables, <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, to cluster the data.
Then we would compute the <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> variables, <span class="math inline">\(\mu_x\)</span> and <span class="math inline">\(\mu_y\)</span>, of the cluster center via</p>
<p><span class="math display">\[\mu_x = \frac{1}{4}(x_1+x_2+x_3+x_4) \quad \mu_y = \frac{1}{4}(y_1+y_2+y_3+y_4).\]</span></p>
<p>In the first cluster from the example, there are 4 data points. These are shown with their cluster center
(<code>flipper_length_scaled = -0.35</code> and <code>bill_length_scaled = 0.99</code>) highlighted
in Figure <a href="clustering.html#fig:10-toy-example-clus1-center">10.4</a>.</p>
<div class="figure"><span id="fig:10-toy-example-clus1-center"></span>
<img src="_main_files/figure-html/10-toy-example-clus1-center-1.png" alt="Cluster 1 from the toy example, with center highlighted." width="417.6" />
<p class="caption">
Figure 10.4: Cluster 1 from the toy example, with center highlighted.
</p>
</div>
<p>The second step in computing the WSSD is to add up the squared distance between each point in the cluster and the cluster center.
We use the straight-line / Euclidean distance formula that we learned about in the classification chapter.
In the 4-observation cluster example above, we would compute the WSSD <span class="math inline">\(S^2\)</span> via</p>
<p><span class="math display">\[\begin{align*}
S^2 = \left((x_1 - \mu_x)^2 + (y_1 - \mu_y)^2\right) + \left((x_2 - \mu_x)^2 + (y_2 - \mu_y)^2\right) + \\ \left((x_3 - \mu_x)^2 + (y_3 - \mu_y)^2\right)  +  \left((x_4 - \mu_x)^2 + (y_4 - \mu_y)^2\right).
\end{align*}\]</span></p>
<p>These distances are denoted by lines in Figure <a href="clustering.html#fig:10-toy-example-clus1-dists">10.5</a> for the first cluster of the penguin data example.</p>
<div class="figure"><span id="fig:10-toy-example-clus1-dists"></span>
<img src="_main_files/figure-html/10-toy-example-clus1-dists-1.png" alt="Cluster 1 from the toy example, with distances to the center highlighted." width="417.6" />
<p class="caption">
Figure 10.5: Cluster 1 from the toy example, with distances to the center highlighted.
</p>
</div>
<p>The larger the value of <span class="math inline">\(S^2\)</span>, the more spread-out the cluster is, since large <span class="math inline">\(S^2\)</span> means that points are far from the cluster center.
Note, however, that “large” is relative to <em>both</em> the scale of the variables for clustering <em>and</em> the number of points in the cluster. A cluster where points are very close to the center might still have a large <span class="math inline">\(S^2\)</span> if there are many data points in the cluster.</p>
</div>
<div id="the-clustering-algorithm" class="section level3">
<h3><span class="header-section-number">10.4.2</span> The clustering algorithm</h3>
<p>We begin the K-means algorithm by picking K, and uniformly randomly assigning data to the K clusters.
Then K-means consists of two major steps that attempt to minimize the
sum of WSSDs over all the clusters, i.e. the <em>total WSSD</em>:</p>
<ol style="list-style-type: decimal">
<li><strong>Center update:</strong> Compute the center of each cluster.</li>
<li><strong>Label update:</strong> Reassign each data point to the cluster with the nearest center.</li>
</ol>
These two steps are repeated until the cluster assignments no longer change.
For example, in the penguin data example, our initialization might look like this:
<center>
<div class="figure"><span id="fig:10-toy-kmeans-init"></span>
<img src="_main_files/figure-html/10-toy-kmeans-init-1.png" alt="Random initialization of labels." width="417.6" />
<p class="caption">
Figure 10.6: Random initialization of labels.
</p>
</div>
</center>
And the first four iterations of K-means would look like (each row corresponds to an iteration,
where the left column depicts the center update,
and the right column depicts the reassignment of data to clusters):
<center>
<strong>Center Update</strong>                            <strong>Label Update</strong>
<img src="_main_files/figure-html/10-toy-kmeans-iter-1.png" width="768" /><img src="_main_files/figure-html/10-toy-kmeans-iter-2.png" width="768" /><img src="_main_files/figure-html/10-toy-kmeans-iter-3.png" width="768" /><img src="_main_files/figure-html/10-toy-kmeans-iter-4.png" width="768" />
</center>
<p>Note that at this point, we can terminate the algorithm since none of the assignments changed
in the fourth iteration; both the centers and labels will remain the same from this point onward.</p>
<blockquote>
<p>Is K-means <em>guaranteed</em> to stop at some point, or could it iterate forever? As it turns out,
thankfully, the answer is that K-means is guaranteed to stop after <em>some</em> number of iterations. For the interested reader, the
logic for this has three steps: (1) both the label update and the center update decrease total WSSD in each iteration,
(2) the total WSSD is always greater than or equal to 0, and (3) there are only a finite number of possible
ways to assign the data to clusters. So at some point, the total WSSD must stop decreasing, which means none of the assignments
are changing, and the algorithm terminates.</p>
</blockquote>
</div>
<div id="random-restarts" class="section level3">
<h3><span class="header-section-number">10.4.3</span> Random restarts</h3>
Unlike the classification and regression models we studied in previous chapters, K-means can get “stuck” in a bad solution.
For example, if we were unlucky and initialized K-means with the following labels:
<center>
<div class="figure"><span id="fig:10-toy-kmeans-bad-init"></span>
<img src="_main_files/figure-html/10-toy-kmeans-bad-init-1.png" alt="Random initialization of labels." width="417.6" />
<p class="caption">
Figure 10.7: Random initialization of labels.
</p>
</div>
</center>
Then the iterations of K-means would look like:
<center>
<strong>Center Update</strong>                            <strong>Label Update</strong>
<img src="_main_files/figure-html/10-toy-kmeans-bad-iter-1.png" width="768" /><img src="_main_files/figure-html/10-toy-kmeans-bad-iter-2.png" width="768" /><img src="_main_files/figure-html/10-toy-kmeans-bad-iter-3.png" width="768" /><img src="_main_files/figure-html/10-toy-kmeans-bad-iter-4.png" width="768" /><img src="_main_files/figure-html/10-toy-kmeans-bad-iter-5.png" width="768" />
</center>
<p>This looks like a relatively bad clustering of the data, but K-means cannot improve it.
To solve this problem when clustering data using K-means, we should randomly re-initialize the labels a few times, run K-means for each initialization,
and pick the clustering that has the lowest final total WSSD.</p>
</div>
<div id="choosing-k" class="section level3">
<h3><span class="header-section-number">10.4.4</span> Choosing K</h3>
<p>In order to cluster data using K-means, we also have to pick the number of clusters, K.
But unlike in classification, we have no data labels and cannot perform
cross-validation with some measure of model prediction error.
Further, if K is chosen too small, then multiple clusters get grouped together;
if K is too large, then clusters get subdivided. In both cases, we will potentially miss
interesting structure in the data. For example, take a look below at the K-means
clustering of our penguin flipper and bill length data for a number of clusters
ranging from 1 to 9.</p>
<div class="figure"><span id="fig:10-toy-kmeans-vary-k"></span>
<img src="_main_files/figure-html/10-toy-kmeans-vary-k-1.png" alt="Clustering of the penguin data for # clusters ranging from 1 to 9." width="1152" />
<p class="caption">
Figure 10.8: Clustering of the penguin data for # clusters ranging from 1 to 9.
</p>
</div>
If we set K less than 3, then the clustering merges separate groups of data; this causes a large
total WSSD, since the cluster center (denoted by an “x”) is not close to any of the data in the cluster. On
the other hand, if we set K greater than 3, the clustering subdivides subgroups of data; this does indeed still
decrease the total WSSD, but by only a <em>diminishing amount</em>. If we plot the total WSSD versus the number of
clusters, we see that the decrease in total WSSD levels off (or forms an “elbow shape”) when we reach roughly
the right number of clusters.
<center>
<div class="figure"><span id="fig:10-toy-kmeans-elbow"></span>
<img src="_main_files/figure-html/10-toy-kmeans-elbow-1.png" alt="Total WSSD for # clusters ranging from 1 to 9." width="960" />
<p class="caption">
Figure 10.9: Total WSSD for # clusters ranging from 1 to 9.
</p>
</div>
</center>
</div>
</div>
<div id="data-pre-processing-for-k-means" class="section level2">
<h2><span class="header-section-number">10.5</span> Data pre-processing for K-means</h2>
<p>Similar to K-nearest neighbours classification and regression, K-means
clustering uses straight-line distance to decide which points are similar to
each other. Therefore, the <em>scale</em> of each of the variables in the data
will influence which cluster data points end up being assigned.
Variables with a large scale will have a much larger
effect on deciding cluster assignment than variables with a small scale.
To address this problem, we typically standardize our data before clustering,
which ensures that each variable has a mean of 0 and standard deviation of 1.
The <code>scale</code> function in R can be used to do this.
We show an example of how to use this function
below using an unscaled version of data set in this chapter:</p>
<div class="sourceCode" id="cb288"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb288-1"><a href="clustering.html#cb288-1"></a>unscaled_data</span></code></pre></div>
<pre><code>## # A tibble: 18 x 2
##    bill_length_mm flipper_length_mm
##             &lt;dbl&gt;             &lt;dbl&gt;
##  1           39.2               196
##  2           36.5               182
##  3           34.5               187
##  4           36.7               187
##  5           38.1               181
##  6           39.2               190
##  7           36                 195
##  8           37.8               193
##  9           46.5               213
## 10           46.1               215
## 11           47.8               215
## 12           45                 220
## 13           49.1               212
## 14           43.3               208
## 15           46                 195
## 16           46.7               195
## 17           52.2               197
## 18           46.8               189</code></pre>
<div class="sourceCode" id="cb290"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb290-1"><a href="clustering.html#cb290-1"></a>scaled_data &lt;-<span class="st"> </span><span class="kw">map_df</span>(unscaled_data, scale)</span>
<span id="cb290-2"><a href="clustering.html#cb290-2"></a>scaled_data</span></code></pre></div>
<pre><code>## # A tibble: 18 x 2
##    bill_length_mm[,1] flipper_length_mm[,1]
##                 &lt;dbl&gt;                 &lt;dbl&gt;
##  1             -0.641                -0.190
##  2             -1.14                 -1.33 
##  3             -1.52                 -0.922
##  4             -1.11                 -0.922
##  5             -0.847                -1.41 
##  6             -0.641                -0.678
##  7             -1.24                 -0.271
##  8             -0.902                -0.434
##  9              0.720                 1.19 
## 10              0.646                 1.36 
## 11              0.963                 1.36 
## 12              0.440                 1.76 
## 13              1.21                  1.11 
## 14              0.123                 0.786
## 15              0.627                -0.271
## 16              0.757                -0.271
## 17              1.78                 -0.108
## 18              0.776                -0.759</code></pre>
</div>
<div id="k-means-in-r" class="section level2">
<h2><span class="header-section-number">10.6</span> K-means in R</h2>
<p>To perform K-means clustering in R, we use the <code>kmeans</code> function. It takes at
least two arguments: the data frame containing the data you wish to cluster,
and K, the number of clusters (here we choose K = 3). Note that since the K-means
algorithm uses a random initialization of assignments, we need to set the random
seed to make the clustering reproducible.</p>
<div class="sourceCode" id="cb292"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb292-1"><a href="clustering.html#cb292-1"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb292-2"><a href="clustering.html#cb292-2"></a>penguin_clust &lt;-<span class="st"> </span><span class="kw">kmeans</span>(scaled_data, <span class="dt">centers =</span> <span class="dv">3</span>)</span>
<span id="cb292-3"><a href="clustering.html#cb292-3"></a>penguin_clust</span></code></pre></div>
<pre><code>## K-means clustering with 3 clusters of sizes 4, 8, 6
## 
## Cluster means:
##   bill_length_mm flipper_length_mm
## 1      0.9858721        -0.3524358
## 2     -1.0050404        -0.7692589
## 3      0.6828058         1.2606357
## 
## Clustering vector:
##  [1] 2 2 2 2 2 2 2 2 3 3 3 3 3 3 1 1 1 1
## 
## Within cluster sum of squares by cluster:
## [1] 1.098928 2.121932 1.247042
##  (between_SS / total_SS =  86.9 %)
## 
## Available components:
## 
## [1] &quot;cluster&quot;      &quot;centers&quot;      &quot;totss&quot;        &quot;withinss&quot;     &quot;tot.withinss&quot;
## [6] &quot;betweenss&quot;    &quot;size&quot;         &quot;iter&quot;         &quot;ifault&quot;</code></pre>
<p>As you can see above, the clustering object returned by <code>kmeans</code> has a lot of information
that can be used to visualize the clusters, pick K, and evaluate the total WSSD.
To obtain this information in a tidy format, we will call in help
from the <code>broom</code> package. Let’s start by visualizing the clustering
as a coloured scatter plot. To do that
we use the <code>augment</code> function, which takes in the model and the original data
frame, and returns a data frame with the data and the cluster assignments for
each point:</p>
<div class="sourceCode" id="cb294"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb294-1"><a href="clustering.html#cb294-1"></a>clustered_data &lt;-<span class="st"> </span><span class="kw">augment</span>(penguin_clust, scaled_data)</span>
<span id="cb294-2"><a href="clustering.html#cb294-2"></a>clustered_data</span></code></pre></div>
<pre><code>## # A tibble: 18 x 3
##    bill_length_mm[,1] flipper_length_mm[,1] .cluster
##                 &lt;dbl&gt;                 &lt;dbl&gt; &lt;fct&gt;   
##  1             -0.641                -0.190 2       
##  2             -1.14                 -1.33  2       
##  3             -1.52                 -0.922 2       
##  4             -1.11                 -0.922 2       
##  5             -0.847                -1.41  2       
##  6             -0.641                -0.678 2       
##  7             -1.24                 -0.271 2       
##  8             -0.902                -0.434 2       
##  9              0.720                 1.19  3       
## 10              0.646                 1.36  3       
## 11              0.963                 1.36  3       
## 12              0.440                 1.76  3       
## 13              1.21                  1.11  3       
## 14              0.123                 0.786 3       
## 15              0.627                -0.271 1       
## 16              0.757                -0.271 1       
## 17              1.78                 -0.108 1       
## 18              0.776                -0.759 1</code></pre>
<p>Now that we have this information in a tidy data frame, we can make a visualization
of the cluster assignments for each point:</p>
<div class="sourceCode" id="cb296"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb296-1"><a href="clustering.html#cb296-1"></a>cluster_plot &lt;-<span class="st"> </span><span class="kw">ggplot</span>(clustered_data,</span>
<span id="cb296-2"><a href="clustering.html#cb296-2"></a>  <span class="kw">aes</span>(<span class="dt">x =</span> flipper_length_mm, <span class="dt">y =</span> bill_length_mm, <span class="dt">colour =</span> .cluster),</span>
<span id="cb296-3"><a href="clustering.html#cb296-3"></a>  <span class="dt">size =</span> <span class="dv">2</span>) <span class="op">+</span></span>
<span id="cb296-4"><a href="clustering.html#cb296-4"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb296-5"><a href="clustering.html#cb296-5"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Flipper Length (scaled)&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Bill Length (scaled)&quot;</span>, <span class="dt">colour =</span> <span class="st">&quot;Cluster&quot;</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb296-6"><a href="clustering.html#cb296-6"></a><span class="st">  </span><span class="kw">scale_color_manual</span>(<span class="dt">values =</span> <span class="kw">c</span>(<span class="st">&quot;dodgerblue3&quot;</span>,<span class="st">&quot;darkorange3&quot;</span>,  <span class="st">&quot;goldenrod1&quot;</span>))</span>
<span id="cb296-7"><a href="clustering.html#cb296-7"></a>cluster_plot</span></code></pre></div>
<p><img src="_main_files/figure-html/10-plot-clusters-2-1.png" width="417.6" /></p>
<p>As mentioned above, we also need to select K by finding
where the “elbow” occurs in the plot of total WSSD versus the number of clusters.
We can obtain the total WSSD (<code>tot.withinss</code>) from our
clustering using <code>broom</code>’s <code>glance</code> function. For example:</p>
<div class="sourceCode" id="cb297"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb297-1"><a href="clustering.html#cb297-1"></a><span class="kw">glance</span>(penguin_clust)</span></code></pre></div>
<pre><code>## # A tibble: 1 x 4
##   totss tot.withinss betweenss  iter
##   &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;
## 1    34         4.47      29.5     1</code></pre>
<p>To calculate the total WSSD for a variety of Ks, we will
create a data frame with a column named <code>k</code> with rows containing
each value of K we want to run K-means with (here, 1 to 9).</p>
<div class="sourceCode" id="cb299"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb299-1"><a href="clustering.html#cb299-1"></a>penguin_clust_ks &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">k =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">9</span>)</span>
<span id="cb299-2"><a href="clustering.html#cb299-2"></a>penguin_clust_ks</span></code></pre></div>
<pre><code>## # A tibble: 9 x 1
##       k
##   &lt;int&gt;
## 1     1
## 2     2
## 3     3
## 4     4
## 5     5
## 6     6
## 7     7
## 8     8
## 9     9</code></pre>
<p>Then we use <code>map</code> to apply the <code>kmeans</code> function to each
K. However, we need to use <code>map</code> a little bit differently than we have
before. This is because we need to iterate over <code>k</code>, which is the <em>second argument</em>
to the <code>kmeans</code> function. In the past, we have used <code>map</code> only to iterate over values of the
<em>first argument</em> of a function. Since that is the default, we could simply
write <code>map(data_frame, function_name)</code>. This won’t work here; we need to
provide our data frame as the first argument to the <code>kmeans</code> function.</p>
<p>The solution is to create something called an <em>anonymous function</em>.
An anonymous function is a function that has no name,
unlike other functions you’ve seen so far (<code>kmeans</code>, <code>select</code>, etc).
To do this we will write our map statement like this:</p>
<div class="sourceCode" id="cb301"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb301-1"><a href="clustering.html#cb301-1"></a><span class="kw">map</span>(penguin_clust_ks, <span class="cf">function</span>(k) <span class="kw">kmeans</span>(scaled_data, k))</span></code></pre></div>
<p>The anonymous function in the above call is <code>function(k) kmeans(scaled_data, k)</code>.
This function takes a single argument (<code>k</code>) and evaluates <code>kmeans(scaled_data, k)</code>.
Since <code>k</code> is the <em>first</em> (and only) argument to the function, we can use <code>map</code> just
like we did before! The rest of the call above does just that – it passes each row of
<code>penguin_clust_ks</code> to our anonymous function.</p>
<p>Below, we execute this <code>map</code> call inside of a <code>mutate</code> call on the
<code>penguin_clust_ks</code> data frame and get a list column that contains a K-means
clustering object for each value of K we had:</p>
<div class="sourceCode" id="cb302"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb302-1"><a href="clustering.html#cb302-1"></a>penguin_clust_ks &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">k =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">9</span>) <span class="op">%&gt;%</span></span>
<span id="cb302-2"><a href="clustering.html#cb302-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">penguin_clusts =</span> <span class="kw">map</span>(k, <span class="cf">function</span>(ks) <span class="kw">kmeans</span>(scaled_data, ks)))</span>
<span id="cb302-3"><a href="clustering.html#cb302-3"></a>penguin_clust_ks</span></code></pre></div>
<pre><code>## # A tibble: 9 x 2
##       k penguin_clusts
##   &lt;int&gt; &lt;list&gt;        
## 1     1 &lt;kmeans&gt;      
## 2     2 &lt;kmeans&gt;      
## 3     3 &lt;kmeans&gt;      
## 4     4 &lt;kmeans&gt;      
## 5     5 &lt;kmeans&gt;      
## 6     6 &lt;kmeans&gt;      
## 7     7 &lt;kmeans&gt;      
## 8     8 &lt;kmeans&gt;      
## 9     9 &lt;kmeans&gt;</code></pre>
<p>Next, we use <code>map</code> again to apply <code>glance</code> to each of the K-means
clustering objects to get the clustering statistics (including WSSD). The output
of glance is a data frame, and so we get another list column. This results in a
complex data frame with 3 columns, one for K, one for the
K-means clustering objects, and one for the clustering statistics:</p>
<div class="sourceCode" id="cb304"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb304-1"><a href="clustering.html#cb304-1"></a>penguin_clust_ks &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">k =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">9</span>) <span class="op">%&gt;%</span></span>
<span id="cb304-2"><a href="clustering.html#cb304-2"></a><span class="st">  </span><span class="kw">mutate</span>(</span>
<span id="cb304-3"><a href="clustering.html#cb304-3"></a>    <span class="dt">penguin_clusts =</span> <span class="kw">map</span>(k, <span class="op">~</span><span class="st"> </span><span class="kw">kmeans</span>(scaled_data, .x)),</span>
<span id="cb304-4"><a href="clustering.html#cb304-4"></a>    <span class="dt">glanced =</span> <span class="kw">map</span>(penguin_clusts, glance)</span>
<span id="cb304-5"><a href="clustering.html#cb304-5"></a>  )</span>
<span id="cb304-6"><a href="clustering.html#cb304-6"></a>penguin_clust_ks</span></code></pre></div>
<pre><code>## # A tibble: 9 x 3
##       k penguin_clusts glanced         
##   &lt;int&gt; &lt;list&gt;         &lt;list&gt;          
## 1     1 &lt;kmeans&gt;       &lt;tibble [1 × 4]&gt;
## 2     2 &lt;kmeans&gt;       &lt;tibble [1 × 4]&gt;
## 3     3 &lt;kmeans&gt;       &lt;tibble [1 × 4]&gt;
## 4     4 &lt;kmeans&gt;       &lt;tibble [1 × 4]&gt;
## 5     5 &lt;kmeans&gt;       &lt;tibble [1 × 4]&gt;
## 6     6 &lt;kmeans&gt;       &lt;tibble [1 × 4]&gt;
## 7     7 &lt;kmeans&gt;       &lt;tibble [1 × 4]&gt;
## 8     8 &lt;kmeans&gt;       &lt;tibble [1 × 4]&gt;
## 9     9 &lt;kmeans&gt;       &lt;tibble [1 × 4]&gt;</code></pre>
<p>Finally we extract the total WSSD from the <code>glanced</code> column. Given that each
item in this column is a data frame, we will need to use the <code>unnest</code> function
to unpack the data frames into simpler column data types.</p>
<div class="sourceCode" id="cb306"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb306-1"><a href="clustering.html#cb306-1"></a>clustering_statistics &lt;-<span class="st"> </span>penguin_clust_ks <span class="op">%&gt;%</span></span>
<span id="cb306-2"><a href="clustering.html#cb306-2"></a><span class="st">  </span><span class="kw">unnest</span>(glanced)</span>
<span id="cb306-3"><a href="clustering.html#cb306-3"></a></span>
<span id="cb306-4"><a href="clustering.html#cb306-4"></a>clustering_statistics</span></code></pre></div>
<pre><code>## # A tibble: 9 x 6
##       k penguin_clusts totss tot.withinss betweenss  iter
##   &lt;int&gt; &lt;list&gt;         &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;
## 1     1 &lt;kmeans&gt;          34       34.0    7.11e-15     1
## 2     2 &lt;kmeans&gt;          34       10.9    2.31e+ 1     1
## 3     3 &lt;kmeans&gt;          34        4.47   2.95e+ 1     1
## 4     4 &lt;kmeans&gt;          34        3.54   3.05e+ 1     1
## 5     5 &lt;kmeans&gt;          34        2.23   3.18e+ 1     2
## 6     6 &lt;kmeans&gt;          34        2.15   3.19e+ 1     3
## 7     7 &lt;kmeans&gt;          34        1.53   3.25e+ 1     2
## 8     8 &lt;kmeans&gt;          34        2.46   3.15e+ 1     1
## 9     9 &lt;kmeans&gt;          34        0.843  3.32e+ 1     2</code></pre>
<p>Now that we have <code>tot.withinss</code> and <code>k</code> as columns in a data frame, we can make a line plot
and search for the “elbow” to find which value of K to use.</p>
<div class="sourceCode" id="cb308"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb308-1"><a href="clustering.html#cb308-1"></a>elbow_plot &lt;-<span class="st"> </span><span class="kw">ggplot</span>(clustering_statistics, <span class="kw">aes</span>(<span class="dt">x =</span> k, <span class="dt">y =</span> tot.withinss)) <span class="op">+</span></span>
<span id="cb308-2"><a href="clustering.html#cb308-2"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb308-3"><a href="clustering.html#cb308-3"></a><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span></span>
<span id="cb308-4"><a href="clustering.html#cb308-4"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;K&quot;</span>) <span class="op">+</span></span>
<span id="cb308-5"><a href="clustering.html#cb308-5"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Total within-cluster sum of squares&quot;</span>) <span class="op">+</span></span>
<span id="cb308-6"><a href="clustering.html#cb308-6"></a><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">9</span>)</span>
<span id="cb308-7"><a href="clustering.html#cb308-7"></a>elbow_plot</span></code></pre></div>
<p><img src="_main_files/figure-html/10-plot-choose-k-1.png" width="417.6" /></p>
<p>It looks like 3 clusters is the right choice for this data.
But why is there a “bump” in the total WSSD plot here? Shouldn’t total WSSD always
decrease as we add more clusters? Technically yes, but remember: K-means can
get “stuck” in a bad solution. Unfortunately, for K = 8 we had an unlucky initialization
and found a bad clustering! We can help prevent finding a bad clustering by trying a
few different random initializations via the <code>nstart</code> argument (here we use 10 restarts).</p>
<div class="sourceCode" id="cb309"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb309-1"><a href="clustering.html#cb309-1"></a>penguin_clust_ks &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">k =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">9</span>) <span class="op">%&gt;%</span></span>
<span id="cb309-2"><a href="clustering.html#cb309-2"></a><span class="st">  </span><span class="kw">mutate</span>(</span>
<span id="cb309-3"><a href="clustering.html#cb309-3"></a>    <span class="dt">penguin_clusts =</span> <span class="kw">map</span>(k, <span class="op">~</span><span class="st"> </span><span class="kw">kmeans</span>(scaled_data, <span class="dt">nstart =</span> <span class="dv">10</span>, .x)),</span>
<span id="cb309-4"><a href="clustering.html#cb309-4"></a>    <span class="dt">glanced =</span> <span class="kw">map</span>(penguin_clusts, glance)</span>
<span id="cb309-5"><a href="clustering.html#cb309-5"></a>  )</span>
<span id="cb309-6"><a href="clustering.html#cb309-6"></a></span>
<span id="cb309-7"><a href="clustering.html#cb309-7"></a>clustering_statistics &lt;-<span class="st"> </span>penguin_clust_ks <span class="op">%&gt;%</span></span>
<span id="cb309-8"><a href="clustering.html#cb309-8"></a><span class="st">  </span><span class="kw">unnest</span>(glanced)</span>
<span id="cb309-9"><a href="clustering.html#cb309-9"></a></span>
<span id="cb309-10"><a href="clustering.html#cb309-10"></a>elbow_plot &lt;-<span class="st"> </span><span class="kw">ggplot</span>(clustering_statistics, <span class="kw">aes</span>(<span class="dt">x =</span> k, <span class="dt">y =</span> tot.withinss)) <span class="op">+</span></span>
<span id="cb309-11"><a href="clustering.html#cb309-11"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb309-12"><a href="clustering.html#cb309-12"></a><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span></span>
<span id="cb309-13"><a href="clustering.html#cb309-13"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;K&quot;</span>) <span class="op">+</span></span>
<span id="cb309-14"><a href="clustering.html#cb309-14"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Total within-cluster sum of squares&quot;</span>) <span class="op">+</span></span>
<span id="cb309-15"><a href="clustering.html#cb309-15"></a><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">9</span>)</span>
<span id="cb309-16"><a href="clustering.html#cb309-16"></a>elbow_plot</span></code></pre></div>
<p><img src="_main_files/figure-html/10-choose-k-nstart-1.png" width="417.6" /></p>
</div>
<div id="additional-resources-6" class="section level2">
<h2><span class="header-section-number">10.7</span> Additional resources</h2>
<ul>
<li>Chapter 10 of <a href="https://www.statlearning.com/">An Introduction to Statistical Learning</a> <span class="citation">(<a href="#ref-james2013introduction" role="doc-biblioref">2013</a>)</span> provides a great next stop in the process of learning about clustering and unsupervised learning in general. In the realm of clustering specifically, it provides a great companion introduction to K-means, but also covers <em>hierarchical</em> clustering for when you expect there to be subgroups, and then subgroups within subgroups, etc. in your data. In the realm of more general unsupervised learning, it covers <em>principal components analysis (PCA)</em>, which is a very popular technique in scientific applications for reducing the number of predictors in a dataset.</li>
</ul>
<!--We have linked a helpful companion vi 

<iframe width="840" height="473" src="https://www.youtube.com/embed/aIybuNt9ps4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-penguinsimage">
<p>Andrew Shiva. 2016. “Brown Bluff-2016-Tabarin Peninsula–Gentoo Penguin (Pygoscelis Papua).” <a href="https://upload.wikimedia.org/wikipedia/commons/0/00/Brown_Bluff-2016-Tabarin_Peninsula%E2%80%93Gentoo_penguin_%28Pygoscelis_papua%29_03.jpg">https://upload.wikimedia.org/wikipedia/commons/0/00/Brown_Bluff-2016-Tabarin_Peninsula%E2%80%93Gentoo_penguin_%28Pygoscelis_papua%29_03.jpg</a>.</p>
</div>
<div id="ref-palmerpenguins">
<p>Horst, Allison Marie, Alison Presmanes Hill, and Kristen B Gorman. 2020. <em>palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data</em>. <a href="https://doi.org/10.5281/zenodo.3960218">https://doi.org/10.5281/zenodo.3960218</a>.</p>
</div>
<div id="ref-james2013introduction">
<p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning</em>. <span class="math inline">\(1^\text{st}\)</span> ed. Springer.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="regression2.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="inference.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/09-clustering.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
