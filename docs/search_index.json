[["index.html", "Data Science: A First Introduction Chapter 1 R and the tidyverse 1.1 Preface 1.2 Overview 1.3 Chapter Learning Objectives 1.4 Ask a question 1.5 Canadian languages data set question 1.6 Loading a tabular data set 1.7 Naming things in R 1.8 Creating subsets of data frames with filter &amp; select 1.9 Exploring data with visualizations", " Data Science: A First Introduction Tiffany-Anne Timbers Trevor Campbell Melissa Lee 2021-07-18 Chapter 1 R and the tidyverse 1.1 Preface This textbook aims to be an approachable introduction to the world of data science! In this book, we define data science as the process of generating insight from data through reproducible and auditable processes. At a high level, in this book, readers will learn how to use reproducible tools to do data analysis and identify and solve common problems in data science. One of the high-level goals of this book is using “reproducible tools to do analysis.” In other words, we want to teach you how to do an analysis where someone else can easily reproduce the results of your analysis. Why is reproducibility necessary? It allows someone else to double-check and validate your work and creates transparency! We will use a programming language called R (R Core Team 2021) as our reproducible tool to help us do data analysis. You will spend the first four chapters learning how to use the R to load, clean, wrangle (i.e. restructure the data, so it’s in a usable format) and visualize data while answering descriptive and exploratory data analysis questions. The next six chapters illustrate how to answer predictive, exploratory and inferential data analysis questions with common methods in data science, including classification, regression, clustering and estimation. In the final chapters (?? - ??), we discuss how to edit and write R code with Jupyter (another reproducible tool), use version control for collaboration, and install and configure the software needed for data science on your own computer. Readers may want to refer to these chapters earlier as they start exploring the examples and exercises in this book. Figure 1.1 summarizes what you will learn in each chapter of this book. Figure 1.1: Where are we going? 1.2 Overview This chapter provides an introduction to the R programming language. We will learn about some fundamental R topics and then go through an entire data analysis task! We will see how to load, clean, wrangle, and visualize some data to answer a specific question at a high level. Later in the book, we will learn each of these steps in more detail, but let’s jump in now to see how much we can do with data science! 1.3 Chapter Learning Objectives By the end of the chapter, readers will be able to: load the tidyverse package into R create new variables and objects in R using the assignment symbol use the help and documentation tools in R match the names of the following functions from the tidyverse package to their documentation descriptions: read_csv select filter arrange slice ggplot aes 1.4 Ask a question The second high-level goal in this book is to “identify and solve common problems in data science.” The four common data science methods that are useful for answering predictive, exploratory and inferential data analysis questions covered in this book are: Classification: predicting a class/category for a new observation. This could answer a question such as whether a tumour is cancerous or benign given some tumour cell measurements? Regression: predicting a quantitative value for a new observation. This could answer a question such as what will be the 10 km race time for a 20-year-old woman with a BMI of 25 who trains four times a week? Clustering: finding previously unknown/unlabelled subgroups in a dataset. This could answer a question such as what products commonly bought together on Amazon? Estimation: make a good guess of an average or a proportion for the wider population from a representative sample (group of people or units), and quantify how good that guess is. This could answer a question such as what is the proportion of undergraduate students that own an iPhone? We will explain these methods in more detail later, and we will map each to the broader data analysis question (listed in Table 1.1) they help answer and discuss what kinds of data are needed to answer such questions (Leek and Peng 2015; Peng and Matsui 2015). Table 1.1: Types of data analysis questions Question type Description Example Descriptive A question that asks about summarized characteristics of a data set without interpretation (i.e., report a fact). How many people live in each province and territory in Canada? Exploratory A question asks if there are patterns, trends, or relationships within a single data set. Often used to propose hypotheses for future study. Does political party voting change with indicators of wealth in a set of data collected on 2,000 people living in Canada? Inferential A question that looks for patterns, trends, or relationships in a single data set and also asks for quantification of how applicable these findings are to the wider population. Does political party voting change with indicators of wealth for all people living in Canada? Predictive A question that asks about predicting measurements or labels for individuals (people or things). The focus is on what things predict some outcome, but not what causes the outcome. What political party will someone vote for in the next Canadian election? Causal A question that asks about whether changing one factor will lead to a change in another factor, on average, in the wider population. Does wealth lead to voting for a certain political party in Canadian elections? Mechanistic A question that asks about the underlying mechanism of the observed patterns, trends, or relationships (i.e., how does it happen?) How does wealth lead to voting for a certain political party in Canadian elections? Source: What is the question? by Jeffery T. Leek, Roger D. Peng &amp; The Art of Data Science by Roger Peng &amp; Elizabeth Matsui Causal or mechanistic questions in Table 1.1 are beyond the scope of this book. 1.5 Canadian languages data set question Now that we’ve learned about different types of questions. We need some context with which to frame the question we want to ask. Before the arrival of colonists, Indigenous peoples lived in Canada with their own cultures and many different languages. Many Aboriginal languages are unique to Canada and not spoken anywhere else in the world (Canada 2018b). Sadly, colonization has led to the loss of many Aboriginal languages. For instance, generations of children were not allowed to speak their mother tongue (the first language an individual learns in childhood) in Canadian residential schools. Colonizers also renamed places they had “discovered” (K. Wilson 2018). Acts such as these have significantly harmed the continuity of Aboriginal languages in Canada, and some languages are considered “endangered” as few people report speaking them. To learn more, please see Canadian Geographic’s article on Mapping Indigenous languages in Canada (Walker 2017), They Came for the Children: Canada, Aboriginal peoples, and residential schools (Truth and Canada 2012) and the Truth and Reconciliation Commission of Canada’s Calls to Action (Truth and Canada 2015). According to the 2016 census, more than 60 Aboriginal languages were reported as being spoken. Suppose we want to ask the question: What ten Aboriginal languages were most often reported in 2016 as mother tongues in Canada, and how many people speak each of them? This question is an example of a descriptive question since we are summarizing the characteristics of a data set without further interpretation. To answer descriptive questions, we often want to summarize data usings numbers, table or graphs. For our specific question, we could create a table or a visualization that shows us ten languages and their associated counts. Now that we have asked our question, the next step is collecting or identifying the data set with which we will be working. In this chapter, we will be working with a data set originally from {canlang} R data package (Timbers 2020), which has language data collected in the 2016 Canadian census (Canada 2016). In this data frame there are 214 rows (corresponding to the 214 languages recorded on the 2016 Canadian census) and 6 columns: category: Higher level language category (describing whether the language is an Official Canadian language, an Aboriginal language, or a Non-Official and Non-Aboriginal language). language: Language queried about on the Canadian Census. mother_tongue: Total count of Canadians from the Census who reported the language as their mother tongue. Mother tongue is generally defined as the language someone was exposed to since birth. most_at_home: Total count of Canadians from the Census who reported the language as spoken most often at home. most_at_work: Total count of Canadians from the Census who reported the language as used most often at work for the population. lang_known: Total count of Canadians from the Census who reported knowledge of language for the population in private households. You can find the description of this data set above provided with the {canlang} R data package. A note about the data in data science! Data science cannot be done without a deep understanding of the data and problem domain. In this book, we have simplified the data sets used in our examples to concentrate on concepts. In real life, we cannot and should not do data science without a domain expert. Alternatively, it is common to practice data science in your domain of expertise! Remember that when we work with data, it is essential to think about how the data were collected, which affects the conclusions we can draw. If your data are biased, then your results will be biased! Now that we have asked our question and identified the data we are using, we will use R to load the data, do some wrangling (i.e. restructure the data into a usable format) and visualize the data to answer our question! 1.6 Loading a tabular data set Almost always, the first step in data analysis is to load a data set into R. A data set is a collection of numbers or values and can come in many different forms! Tables in Microsoft Excel are an example of tablular data, which are rectangular-shaped, spreadsheet-like data as shown in Figure 1.2. When we load this type of data into R, it is represented as a data frame object. Figure 1.2 shows an R data frame is very similar to a spreadsheet where the rows are observations (the things that we collect the data on e.g. voters, cities etc.), and the columns are variables (the characteristics of those observations e.g. voters’ political affiliation, cities’ population etc.). Figure 1.2: A spreadsheet versus a data frame in R The first kind of data file that we will learn how to load into R (as a data frame) is the comma-separated values format (.csv for short). These files have names ending in .csv, and can be opened and saved using common spreadsheet programs like Microsoft Excel and Google Sheets. For example, the .csv file named can_lang.csv is included with the code for this book. If we were to open this data in a plain text editor (a program that removes all formatting from programs like Word, such as Notepad), we would see each row on its own line, and each entry in the table separated by a comma: category,language,mother_tongue,most_at_home,most_at_work,lang_known Aboriginal languages,&quot;Aboriginal languages, n.o.s.&quot;,590,235,30,665 Non-Official &amp; Non-Aboriginal languages,Afrikaans,10260,4785,85,23415 Non-Official &amp; Non-Aboriginal languages,&quot;Afro-Asiatic languages, n.i.e.&quot;,1150,445,10,2775 Non-Official &amp; Non-Aboriginal languages,Akan (Twi),13460,5985,25,22150 Non-Official &amp; Non-Aboriginal languages,Albanian,26895,13135,345,31930 Aboriginal languages,&quot;Algonquian languages, n.i.e.&quot;,45,10,0,120 Aboriginal languages,Algonquin,1260,370,40,2480 Non-Official &amp; Non-Aboriginal languages,American Sign Language,2685,3020,1145,21930 Non-Official &amp; Non-Aboriginal languages,Amharic,22465,12785,200,33670 To load this data into R, so that we can do things with it (e.g, perform analyses or create data visualizations), we will need to use a function. A function is a special word in R that takes instructions (we call these arguments) and does something. The function we will use to load a .csv file into R is called read_csv. In its most basic use-case, read_csv expects that the data file: has column names (or headers), uses a comma (,) to separate the columns, and does not have row names. Below you’ll see the code used to load the data into R using the read_csv function. However, the read_csv function is not included in the base installation of R, meaning that it is not one of the primary functions ready to use when you install R. Therefore, you need to load it from somewhere else before you can use it. The place we will load it from is an R package. An R package is a collection of functions that can be used in addition to the built-in R package functions once loaded. The read_csv function, in particular, can be made accessible through the library function by loading the tidyverse package (Wickham et al. 2019). tidyverse contains many functions, which allow us to load, clean, wrangle and visualize data. library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ## ✔ ggplot2 3.3.3 ✔ purrr 0.3.4 ## ✔ tibble 3.1.2 ✔ dplyr 1.0.6 ## ✔ tidyr 1.1.3 ✔ stringr 1.4.0 ## ✔ readr 1.4.0 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() In case you want to know more (optional): Notice that we got some extra output from R saying Attaching packages and Conflicts below our code line. These are examples of messages in R, which give the user more information that might be handy to know. It is normal and expected that a message is printed out after loading the tidyverse and some packages. Generally, this message let’s you know if functions from the different packages were loaded share the same name (which is confusing to R), and if so, which one you can access using just it’s name (and which one you need to refer the package name and the function name to refer to it, this is called masking). For example, the dplyr and the stats package both use a function called filter. R is going to “mask” the filter function from the stats package, meaning that it will default to the dplyr version of this function. Additionally, tidyverse is a special R package - it is a meta-package that bundles together several related and commonly used packages. Because of this it lists the packages it does the job of loading. For example, in this message, R tells us it loads the ggplot2 package when it loads the tidyverse package. Messages are not errors, so generally you don’t need to take action when you see a message, but you can read a message and decide for yourself. In future when we load this package in this book we will silence these messages to help with readability of the book. After loading the tidyverse package, we can call the read_csv function and pass it a single argument: the name of the file, \"can_lang.csv\". We have to put quotes around file names and other letters and words that we use in our code to distinguish it from the special words that make up the R programming language. The file’s name is the only argument we need to provide because our file satisfies everything else the read_csv function expects in the default use-case. read_csv(&quot;data/can_lang.csv&quot;) ## # A tibble: 214 x 6 ## category language mother_tongue most_at_home most_at_work lang_known ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Aboriginal la… Aboriginal… 590 235 30 665 ## 2 Non-Official … Afrikaans 10260 4785 85 23415 ## 3 Non-Official … Afro-Asiat… 1150 445 10 2775 ## 4 Non-Official … Akan (Twi) 13460 5985 25 22150 ## 5 Non-Official … Albanian 26895 13135 345 31930 ## 6 Aboriginal la… Algonquian… 45 10 0 120 ## 7 Aboriginal la… Algonquin 1260 370 40 2480 ## 8 Non-Official … American S… 2685 3020 1145 21930 ## 9 Non-Official … Amharic 22465 12785 200 33670 ## 10 Non-Official … Arabic 419890 223535 5585 629055 ## # … with 204 more rows In case you want to know more (optional): There is another function, which also loads csv files called read.csv, which is a base R function, so we don’t need to load a package to use it. Why are we using read_csv instead of read.csv? We use the read_csv function from the tidyverse because it’s faster, and it has other advantages, such as a more readable format when printing the data frame. Therefore we will use read_csv from here on out. 1.7 Naming things in R When we loaded the language data collected in the 2016 Canadian census above using read_csv, we did not give this data frame a name. Therefore the data was just printed on the screen, and we cannot do anything else with it. That isn’t very useful. What would be more useful would be to give a name to the data frame that read_csv outputs, so that we could refer to it later for analysis and visualization. There are two possible ways to assign a name to something in R — using either the assignment symbol (&lt;-) or the equals symbol (=). first_name &lt;- &quot;Naming using the assignment symbol&quot; second_name = &quot;Naming using the equals symbol&quot; From a style perspective, the assignment symbol is preferred and is what we will use in this book. When we name something in R using the assignment symbol, &lt;-, we do not need to surround the name we are giving (on &lt;-’s left-hand side) with quotes, like the file name, because we are formally telling R about this new word and giving it a value. Only characters and words that act as values (on &lt;-’s right-hand side) need to be surrounded by quotes. Object names can consist of letters, numbers, periods . and underscores _. Other symbols won’t work since they have their own meanings in R. For example, + is an addition symbol used to add numbers. If we try to assign a name with the + symbol to an object R will complain and we will get an error! name+ &lt;- 1 Error: unexpected assignment in &quot;name+ &lt;-&quot; There are certain conventions for naming objects in R. When naming an object we suggest using only lower case letters, numbers and underscores _ to separate the words in a name. R is case sensitive, which means that Letter and letter would be two different objects in R. You should also try to give your objects meaningful names. For instance, you can name a data frame x. However, using more meaningful terms, such as language_data, will help you remember what each name in your code represents. We recommend following the Tidyverse naming conventions outlined in the Tidyverse Style Guide (Wickham 2020). Let’s now use the assignment symbol to give the name can_lang to the 2016 Canadian census language data frame that we get from read_csv. can_lang &lt;- read_csv(&quot;data/can_lang.csv&quot;) Wait a minute, nothing happened this time! Or at least it looks like that? But actually, something did happen! The data was loaded in and now has the name can_lang associated with it. And we can use that name to access the data frame and do things with it. We can type the name of the data frame to print the first few rows on the screen. can_lang ## # A tibble: 214 x 6 ## category language mother_tongue most_at_home most_at_work lang_known ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Aboriginal la… Aboriginal… 590 235 30 665 ## 2 Non-Official … Afrikaans 10260 4785 85 23415 ## 3 Non-Official … Afro-Asiat… 1150 445 10 2775 ## 4 Non-Official … Akan (Twi) 13460 5985 25 22150 ## 5 Non-Official … Albanian 26895 13135 345 31930 ## 6 Aboriginal la… Algonquian… 45 10 0 120 ## 7 Aboriginal la… Algonquin 1260 370 40 2480 ## 8 Non-Official … American S… 2685 3020 1145 21930 ## 9 Non-Official … Amharic 22465 12785 200 33670 ## 10 Non-Official … Arabic 419890 223535 5585 629055 ## # … with 204 more rows 1.8 Creating subsets of data frames with filter &amp; select Now that we’ve loaded our data into R, we can start wrangling the data to answer our question, what ten Aboriginal languages were most often reported in 2016 as mother tongues in Canada, and how many people speak each of them? To answer this question, we can construct a table with the ten Aboriginal languages that have highest counts in the mother_tongue column. The select and filter functions from the tidyverse package will help us here. The select function allows you to create a subset of the columns of a data frame, while the filter function allows you to obtain a subset of the rows with specific values. Therefore, we can filter rows for only the Aboriginal languages in the data set and then use select to obtain only the columns we want to include in our table. Let’s take a look at the language data collected in the 2016 Canadian census again to familiarize ourselves with it. We will do this by printing the data we loaded earlier in the chapter to the screen. can_lang ## # A tibble: 214 x 6 ## category language mother_tongue most_at_home most_at_work lang_known ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Aboriginal la… Aboriginal… 590 235 30 665 ## 2 Non-Official … Afrikaans 10260 4785 85 23415 ## 3 Non-Official … Afro-Asiat… 1150 445 10 2775 ## 4 Non-Official … Akan (Twi) 13460 5985 25 22150 ## 5 Non-Official … Albanian 26895 13135 345 31930 ## 6 Aboriginal la… Algonquian… 45 10 0 120 ## 7 Aboriginal la… Algonquin 1260 370 40 2480 ## 8 Non-Official … American S… 2685 3020 1145 21930 ## 9 Non-Official … Amharic 22465 12785 200 33670 ## 10 Non-Official … Arabic 419890 223535 5585 629055 ## # … with 204 more rows 1.8.1 Using filter to extract rows Looking at our can_lang data, we see the column category contains different high-level categories of languages, which include “Aboriginal languages,” “Non-Official &amp; Non-Aboriginal languages” and “Official languages.” To answer our question (what ten Aboriginal languages were most often reported in 2016 as mother tongues in Canada, and how many people speak each of them?) we want to filter our data set so we restrict our attention to only the languages in the “Aboriginal languages” category. We can use the filter function to obtain the subset of rows with desired values from a data frame. Our first argument is the name of the data frame object, can_lang. The second argument is a logical statement to use when filtering the rows. We are interested in looking at the languages in the “Aboriginal languages” higher-level category in this data set and to filter only those rows, we use the equivalency operator == to compare the values of the category column with the value \"Aboriginal languages\". Similar to when we loaded the data file and put quotes around the filename, here we need to put quotes around \"Aboriginal languages\". Using quotes tells R that this is a character value and not one of the special words that make up R programming language, nor one of the names we have given to data frames in the code we have already written. What’s a logical statement? A logical statement is a sentence that is TRUE or FALSE. For example, 1 &gt; 2 is logical statement. If you type this statement into R, you are asking R “Is the number 1 greater than 2?” and you will get FALSE as your answer. With these arguments, filter returns a data frame that has all the columns of the input data frame but only the rows we asked for in our logical filter statement. aboriginal_lang &lt;- filter(can_lang, category == &quot;Aboriginal languages&quot;) aboriginal_lang ## # A tibble: 67 x 6 ## category language mother_tongue most_at_home most_at_work lang_known ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Aboriginal … Aboriginal l… 590 235 30 665 ## 2 Aboriginal … Algonquian l… 45 10 0 120 ## 3 Aboriginal … Algonquin 1260 370 40 2480 ## 4 Aboriginal … Athabaskan l… 50 10 0 85 ## 5 Aboriginal … Atikamekw 6150 5465 1100 6645 ## 6 Aboriginal … Babine (Wets… 110 20 10 210 ## 7 Aboriginal … Beaver 190 50 0 340 ## 8 Aboriginal … Blackfoot 2815 1110 85 5645 ## 9 Aboriginal … Carrier 1025 250 15 2100 ## 10 Aboriginal … Cayuga 45 10 10 125 ## # … with 57 more rows It’s good practice to check the output we get makes sense after we perform a function in R. We can see the original can_lang data set contained 214 rows with categories other than just “Aboriginal languages.” The data frame aboriginal_lang contains 67 rows and looks like it only contains languages in the “Aboriginal languages” in the category column, which is what we want! 1.8.2 Using select to extract columns Now let’s use select to extract the language and mother_tongue columns from this data frame. To extract these columns, we need to provide the select function with three arguments. The first argument is the name of the data frame object, which in this example is aboriginal_lang. The second and third arguments are the column names that we want to select, here language and mother_tongue. After passing these three arguments, the select function returns two columns (the language and mother_tongue columns that we asked for) as a data frame. selected_lang &lt;- select(aboriginal_lang, language, mother_tongue) selected_lang ## # A tibble: 67 x 2 ## language mother_tongue ## &lt;chr&gt; &lt;dbl&gt; ## 1 Aboriginal languages, n.o.s. 590 ## 2 Algonquian languages, n.i.e. 45 ## 3 Algonquin 1260 ## 4 Athabaskan languages, n.i.e. 50 ## 5 Atikamekw 6150 ## 6 Babine (Wetsuwet&#39;en) 110 ## 7 Beaver 190 ## 8 Blackfoot 2815 ## 9 Carrier 1025 ## 10 Cayuga 45 ## # … with 57 more rows Note: we didn’t actually need to select the columns in our data frame before moving on to the next step! However, select allows us to display only the columns in the data frame we want and can make it easier for us to view the data especially if we have a large data frame with lots of columns. 1.8.3 Using arrange to order and slice to select rows by index number We have used filter and select to obtain a table with only the Aboriginal languages in the data set and their associated counts. However, we want to know the ten languages that are spoken most often. As a next step, we could order the mother_tongue column from greatest to least and then select only the top ten rows. This is where the arrange and slice functions come to the rescue! The arrange function allows us to order the rows of a data frame by the values of a selected column. We need to pass the data frame, and the variable to order by in the first two arguments of this function. Since we want to choose the ten Aboriginal languages most often reported as a mother tongue language, we will use the arrange function to order the rows in our selected_lang data frame by the mother_tongue column. We want to order the rows in descending order (from largest to smallest) so we include desc before the column name mother_tongue to sort in descending order. arranged_lang &lt;- arrange(selected_lang, by = desc(mother_tongue)) arranged_lang ## # A tibble: 67 x 2 ## language mother_tongue ## &lt;chr&gt; &lt;dbl&gt; ## 1 Cree, n.o.s. 64050 ## 2 Inuktitut 35210 ## 3 Ojibway 17885 ## 4 Oji-Cree 12855 ## 5 Dene 10700 ## 6 Montagnais (Innu) 10235 ## 7 Mi&#39;kmaq 6690 ## 8 Atikamekw 6150 ## 9 Plains Cree 3065 ## 10 Stoney 3025 ## # … with 57 more rows Next we will use the slice function, which selects rows according to their row number. Since we want to choose ten languages, we will indicate we want the rows 1 to 10 using the argument 1:10. ten_lang &lt;- slice(arranged_lang, 1:10) ten_lang ## # A tibble: 10 x 2 ## language mother_tongue ## &lt;chr&gt; &lt;dbl&gt; ## 1 Cree, n.o.s. 64050 ## 2 Inuktitut 35210 ## 3 Ojibway 17885 ## 4 Oji-Cree 12855 ## 5 Dene 10700 ## 6 Montagnais (Innu) 10235 ## 7 Mi&#39;kmaq 6690 ## 8 Atikamekw 6150 ## 9 Plains Cree 3065 ## 10 Stoney 3025 We have now answered our initial question by generating this frequency table! Are we done? Technically, this table answers our question. However, we can go one step further and create a visualization to answer our question as well. Visualizations are a great tool for summarizing information to help you effectively communicate with your audience. While tables are great for displaying information, a visualization can sometimes provide more insight. For example, if we were really interested in comparing the values between the different languages (e.g. how many more people reported “Cree, n.o.s.” versus “Inuktitut” versus “Ojibway” etc.), then a visualization is a more effective way to communicate your results than a frequency table. 1.9 Exploring data with visualizations Creating effective data visualizations is an essential piece to any data analysis. We will develop a visualization of the language data collected in the 2016 Canadian census we’ve been working with to help us understand the ten Aboriginal languages that were most often reported in 2016 as mother tongues in Canada and the number of people that speak each of them. 1.9.1 Using ggplot to create a bar plot In our data set, we can see that language and mother_tongue are in separate columns. In addition, there is a single row (or observation) for each language. The data are, therefore, in what we call a tidy data format. Tidy data is a fundamental concept and will be a significant focus in the remainder of this book: many of the functions from tidyverse require tidy data, including the ggplot function that we will use shortly for our visualization. We will formally introduce this concept in chapter 3. We will make a bar plot to visualize our data. A bar plot is a chart where the heights of the bars represent certain values, like counts or proportions. We will make a bar plot using the most_at_home and language columns from our ten_lang data frame. To create a bar plot of these two variables using the ggplot function, we would do the following: Figure 1.3: Creating a bar plot with the ggplot function In Figure 1.4, we create a bar plot using the ggplot function following the instructions described in figure 1.3: ggplot(ten_lang, aes(x = language, y = mother_tongue)) + geom_bar(stat = &quot;identity&quot;) Figure 1.4: Bar plot of the ten Aboriginal languages most often reported by Canadians as their mother tongue In case you have used R before and are curious: There are a small number of situations in which you can have a single R expression span multiple lines. Here, the + symbol at the end of the first line tells R that the expression isn’t done yet and to continue reading on the following line. While not strictly necessary, this sort of pattern will appear a lot when using ggplot as it keeps things more readable. 1.9.2 Formatting ggplot objects It is exciting that we can already visualize our data to help answer our question, but we are not done yet! We can (and should) do more to improve the interpretability of the data visualization that we created. For example, by default, R uses the column names as the axis labels. However, usually, these column names do not have enough information about the variable in the column. We really should replace this default with a more informative label. For the example above, R uses the column name mother_tongue as the label for the y-axis, but most people will not know what that is. And even if they did, they will not know how we measure this variable, nor which group of people the measurements were taken. An axis label that reads “Mother tongue (Number of Canadians residents)” would be much more informative. Adding additional layers to our visualizations that we create in ggplot is one common and easy way to improve and refine our data visualizations. New layers are added to ggplot objects using the + symbol. For example, we can use the xlab and ylab functions to add layers where we specify meaningful and informative labels for the x and y axes. Again, since we are specifying words (e.g. \"Mother tongue (Number of Canadians residents)\") as arguments to xlab and ylab, we surround them with double-quotes. We can add many more layers to format the plot further, and we will explore these in chapter 4. ggplot(ten_lang, aes(x = language, y = mother_tongue)) + geom_bar(stat = &quot;identity&quot;) + xlab(&quot;Language&quot;) + ylab(&quot;Mother tongue (Number of Canadians residents)&quot;) Figure 1.5: Bar plot of the ten Aboriginal languages most often reported by Canadians as their mother tongue with x and y labels In figure 1.5, the x labels overlap, making it challenging to read the different languages. One solution is to rotate the plot, so the bars are horizontal, and thus the labels will be more readable. We will swap the x and y coordinate axes: ggplot(ten_lang, aes(x = mother_tongue, y = language)) + geom_bar(stat = &quot;identity&quot;) + xlab(&quot;Mother tongue (Number of Canadians residents)&quot;) + ylab(&quot;Language&quot;) Figure 1.6: Horizontal bar plot of the ten Aboriginal languages most often reported by Canadians as their mother tongue From figure 1.6, we have answered our initial question. However, our visualization could be made more transparent by organizing the bars according to the number of Canadian residents reporting each language rather than in alphabetical order. We can reorder the bars using the reorder function, which orders a variable (here language) based on the values of the second variable (mother_tongue): ggplot(ten_lang, aes(x = mother_tongue, y = reorder(language, mother_tongue))) + geom_bar(stat = &quot;identity&quot;) + xlab(&quot;Mother tongue (Number of Canadians residents)&quot;) + ylab(&quot;Language&quot;) Figure 1.7: Bar plot of the ten Aboriginal languages most often reported by Canadians as their mother tongue with bars reordered From figure 1.7, we have answered our question since we can see what the ten most often reported Aboriginal languages were according to the 2016 Candian census and how many people speak them. For instance, we can see that the Aboriginal language most often reported was Cree n.o.s. with over 60,000 Canadian residents reporting it as their mother tongue. “n.o.s.” means “not otherwise specified,” so Cree n.o.s. refers to individuals who reported Cree as their mother tongue. In this data set, the Cree languages include the following categories: Cree n.o.s., Swampy Cree, Plains Cree, Woods Cree, and a ‘Cree not included elsewhere’ category (which includes Moose Cree, Northern East Cree and Southern East Cree) (Canada 2018a). Learning how to describe data visualizations is a very useful skill. We will provide descriptions for you in this book (as we did above) until we get to chapter 4, which focuses on data visualization. Then, we will explicitly teach you how to do this yourself and not over-state or over-interpret the results from a visualization. 1.9.3 Putting it all together In the code chunk below, we put everything from this chapter together. We have added a few more layers to make the data visualization even more effective. Specifically, we changed the colour of the bars and changed the background from grey to white to improve the contrast. Note that we actually skipped the select step that we did above just to show you that we didn’t actually need to select the columns to create the visualization! Notice that we provided comments beside some of the code below using the hashtag symbol #. You can use comments to explain lines of code for others or yourself in the future! If R sees a # sign, it will ignore all the code that comes after it on that line. It’s good practice to get in the habit of commenting your code to improve your code’s readability. library(tidyverse) can_lang &lt;- read_csv(&quot;data/can_lang.csv&quot;) aboriginal_lang &lt;- filter(can_lang, category == &quot;Aboriginal languages&quot;) arranged_lang &lt;- arrange(aboriginal_lang, by = desc(mother_tongue)) ten_lang &lt;- slice(arranged_lang, 1:10) ggplot(ten_lang, aes( x = mother_tongue, y = reorder(language, mother_tongue) )) + geom_bar(stat = &quot;identity&quot;, fill = &quot;steelblue&quot;) + xlab(&quot;Mother tongue (Number of Canadians residents)&quot;) + ylab(&quot;Language&quot;) + theme_bw() # use a theme to have a white background Figure 1.8: Putting it all together: Bar plot of the ten Aboriginal languages most often reported by Canadians as their mother tongue This exercise demonstrates the power of R. In relatively few lines of code, we performed an entire data science workflow with a highly effective data visualization! We asked a question, loaded the data into R, wrangled the data (using filter, arrange and slice) and created a data visualization to help answer our question. In this chapter, you got a quick taste of the data science workflow, but we will learn each of these steps in more detail in the coming chapters! References "],["reading.html", "Chapter 2 Reading in data locally and from the web 2.1 Overview 2.2 Chapter learning objectives 2.3 Absolute and relative file paths 2.4 Reading tabular data from a plain text file into R 2.5 Reading data from an Microsoft Excel file 2.6 Reading data from a database 2.7 Writing data from R to a .csv file 2.8 Obtaining data from the web 2.9 Additional resources", " Chapter 2 Reading in data locally and from the web 2.1 Overview In this chapter, you’ll learn to read spreadsheet-like data of various formats into R from your local device and the web. “Reading” (or “loading”) is the process of converting data (stored as plain text, a database, HTML, etc.) into an object (e.g., a data frame) that R can easily access and manipulate. Thus reading data is the gateway to any data analysis; you won’t be able to analyze data unless you’ve loaded it first. And because there are many ways to store data, there are similarly many ways to read data into R. The more time you spend upfront matching the data reading method to the type of data you have, the less time you will have to devote to re-formatting, cleaning and wrangling your data (the second step to all data analyses). It’s like making sure your shoelaces are tied well before going for a run so that you don’t trip later on! 2.2 Chapter learning objectives By the end of the chapter, students will be able to: define the following: absolute file path relative file path uniform resource locator (URL) read data into R using a relative path and a URL compare and contrast the following functions: read_csv read_tsv read_csv2 read_delim read_excel match the following tidyverse read_* function arguments to their descriptions: file delim col_names skip choose the appropriate tidyverse read_* function and function arguments to load a given plain text tabular data set into R use readxl package’s read_excel function and arguments to load a sheet from an excel file into R connect to a database using the DBI package’s dbConnect function list the tables in a database using the DBI package’s dbListTables function create a reference to a database table that is queriable using the tbl from the dbplyr package retrieve data from a database query and bring it into R using the collect function from the dbplyr package use write_csv to save a data frame to a .csv file (optional) obtain data using application programming interfaces (APIs) and web scraping read HTML source code from a URL using the rvest package read data from the Twitter API using the rtweet package compare downloading tabular data from a plain text file (e.g. .csv), accessing data from an API, and scraping the HTML source code from a website 2.3 Absolute and relative file paths When you load a data set into R, you first need to tell R where those files live. The file could live on your computer (local) or somewhere on the internet (remote). In this section, we will discuss the case where the file lives on your computer. The place where the file lives on your computer is called the “path.” You can think of the path as directions to the file. There are two kinds of paths: relative paths and absolute paths. A relative path is where the file is with respect to where you currently are on the computer (e.g., where the Jupyter notebook file that you’re working in is). On the other hand, an absolute path is where the file is in respect to the base (or root) folder of the computer’s filesystem. Suppose our computer’s filesystem looks like the picture below, and we are working in the Jupyter notebook titled worksheetk_02.ipynb. If we want to read in the .csv file named happiness_report.csv into our Jupyter notebook using R, we could do this using either a relative or an absolute path. We show both choices below. Figure 2.1: Example file system Reading happiness_report.csv using a relative path: happiness_data &lt;- read_csv(&quot;data/happiness_report.csv&quot;) Reading happiness_report.csv using an absolute path: happiness_data &lt;- read_csv(&quot;/home/jupyter/dsci-100/worksheet_02/data/happiness_report.csv&quot;) So which one should you use? Generally speaking, to ensure your code can be run on a different computer, you should use relative paths. An added bonus is that it’s also less typing! This is because the absolute path of a file (the names of folders between the computer’s root / and the file) isn’t usually the same across different computers. For example, suppose Fatima and Jayden are working on a project together on the happiness_report.csv data. Fatima’s file is stored at /home/Fatima/project/data/happiness_report.csv, while Jayden’s is stored at /home/Jayden/project/data/happiness_report.csv. Even though Fatima and Jayden stored their files in the same place on their computers (in their home folders), the absolute paths are different due to their different usernames. If Jayden has code that loads the happiness_report.csv data using an absolute path, the code won’t work on Fatima’s computer. But the relative path from inside the project folder (data/happiness_report.csv) is the same on both computers; any code that uses relative paths will work on both! See this video for another explanation: Source: Udacity course “Linux Command Line Basics” 2.4 Reading tabular data from a plain text file into R Now we will learn more about reading tabular data from a plain text file into R, as well as how to write tabular data to a file. Last chapter, we learned about using the tidyverse read_csv function when reading files that match that function’s expected defaults (column names are present, and commas are used as the delimiter/separator between columns). In this section, we will learn how to read files that do not satisfy the default expectations of read_csv. Before we jump into the cases where the data aren’t in the expected default format for tidyverse and read_csv, let’s revisit the more straightforward case where the defaults hold, and the only argument we need to give to the function is the path to the file, data/can_lang.csv. The can_lang data set contains language data from the 2016 Canadian census. We put data/ before the file’s name when we are loading the data set because this data set is located in a sub-folder, named data, relative to where we are running our R code. Here is what the file would look like in a plain text editor: category,language,mother_tongue,most_at_home,most_at_work,lang_known Aboriginal languages,&quot;Aboriginal languages, n.o.s.&quot;,590,235,30,665 Non-Official &amp; Non-Aboriginal languages,Afrikaans,10260,4785,85,23415 Non-Official &amp; Non-Aboriginal languages,&quot;Afro-Asiatic languages, n.i.e.&quot;,1150,445,10,2775 Non-Official &amp; Non-Aboriginal languages,Akan (Twi),13460,5985,25,22150 Non-Official &amp; Non-Aboriginal languages,Albanian,26895,13135,345,31930 Aboriginal languages,&quot;Algonquian languages, n.i.e.&quot;,45,10,0,120 Aboriginal languages,Algonquin,1260,370,40,2480 Non-Official &amp; Non-Aboriginal languages,American Sign Language,2685,3020,1145,21930 Non-Official &amp; Non-Aboriginal languages,Amharic,22465,12785,200,33670 And here is a review of how we can use read_csv to load it into R. First we load the tidyverse package to gain access to useful functions for reading the data. library(tidyverse) Note: it is normal and expected that a message is printed out after loading the tidyverse and some packages. Generally, this message let’s you know if functions from the different packages were loaded share the same name (which is confusing to R), and if so, which one you can access using just it’s name (and which one you need to refer the package name and the function name to refer to it, this is called masking). Additionally, the tidyverse is a special R package - it is a meta-package that bundles together several related and commonly used packages. Because of this it lists the packages it does the job of loading. In future when we load this package in this book we will silence these messages to help with readability of the book. Next we use read_csv to load the data into R, and in that call we specify the relative path to the file. canlang_data &lt;- read_csv(&quot;data/can_lang.csv&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## category = col_character(), ## language = col_character(), ## mother_tongue = col_double(), ## most_at_home = col_double(), ## most_at_work = col_double(), ## lang_known = col_double() ## ) Note: it is also normal and expected that a message is printed out after using the read_csv and related functions. This message functions to let you know the data types of each of the columns that R inferred while reading the data into R. In future when we use this and related functions to load data in this book we will silence these messages to help with readability of the book. canlang_data ## # A tibble: 214 x 6 ## category language mother_tongue most_at_home most_at_work lang_known ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Aboriginal la… Aboriginal… 590 235 30 665 ## 2 Non-Official … Afrikaans 10260 4785 85 23415 ## 3 Non-Official … Afro-Asiat… 1150 445 10 2775 ## 4 Non-Official … Akan (Twi) 13460 5985 25 22150 ## 5 Non-Official … Albanian 26895 13135 345 31930 ## 6 Aboriginal la… Algonquian… 45 10 0 120 ## 7 Aboriginal la… Algonquin 1260 370 40 2480 ## 8 Non-Official … American S… 2685 3020 1145 21930 ## 9 Non-Official … Amharic 22465 12785 200 33670 ## 10 Non-Official … Arabic 419890 223535 5585 629055 ## # … with 204 more rows 2.4.1 Skipping rows when reading in data Often times information about how data was collected, or other relevant information, is included at the top of the data file. This information is usually written in sentence and paragraph form, with no delimiter because it is not organized into columns. An example of this is shown below. This information gives the data scientist useful context and information about the data, however, it is not well formatted or intended to be read into a data frame cell along with the tabular data that follows later in the file. Data source: https://ttimbers.github.io/canlang/ Data originally published in: Statistics Canada Census of Population 2016. Reproduced and distributed on an as is basis with the permission of Statistics Canada. category,language,mother_tongue,most_at_home,most_at_work,lang_known Aboriginal languages,&quot;Aboriginal languages, n.o.s.&quot;,590,235,30,665 Non-Official &amp; Non-Aboriginal languages,Afrikaans,10260,4785,85,23415 Non-Official &amp; Non-Aboriginal languages,&quot;Afro-Asiatic languages, n.i.e.&quot;,1150,445,10,2775 Non-Official &amp; Non-Aboriginal languages,Akan (Twi),13460,5985,25,22150 Non-Official &amp; Non-Aboriginal languages,Albanian,26895,13135,345,31930 Aboriginal languages,&quot;Algonquian languages, n.i.e.&quot;,45,10,0,120 Aboriginal languages,Algonquin,1260,370,40,2480 Non-Official &amp; Non-Aboriginal languages,American Sign Language,2685,3020,1145,21930 Non-Official &amp; Non-Aboriginal languages,Amharic,22465,12785,200,33670 With this extra information being present at the top of the file, using read_csv as we did previously does not allow us to correctly load the data into R. In the case of this file we end up only reading in one column of the data set: canlang_data &lt;- read_csv(&quot;data/can_lang_meta-data.csv&quot;) ## Warning: 215 parsing failures. ## row col expected actual file ## 3 -- 1 columns 6 columns &#39;data/can_lang_meta-data.csv&#39; ## 4 -- 1 columns 6 columns &#39;data/can_lang_meta-data.csv&#39; ## 5 -- 1 columns 6 columns &#39;data/can_lang_meta-data.csv&#39; ## 6 -- 1 columns 6 columns &#39;data/can_lang_meta-data.csv&#39; ## 7 -- 1 columns 6 columns &#39;data/can_lang_meta-data.csv&#39; ## ... ... ......... ......... ............................. ## See problems(...) for more details. Note: In contrast to the normal and expected messages above, this time R printed out a warning for us indicating that there might be a problem with how our data is being read in. canlang_data ## # A tibble: 217 x 1 ## `Data source: https://ttimbers.github.io/canlang/` ## &lt;chr&gt; ## 1 Data originally published in: Statistics Canada Census of Population 2016. ## 2 Reproduced and distributed on an as is basis with the permission of Statisti… ## 3 category ## 4 Aboriginal languages ## 5 Non-Official &amp; Non-Aboriginal languages ## 6 Non-Official &amp; Non-Aboriginal languages ## 7 Non-Official &amp; Non-Aboriginal languages ## 8 Non-Official &amp; Non-Aboriginal languages ## 9 Aboriginal languages ## 10 Aboriginal languages ## # … with 207 more rows To successfully read data like this into R, the skip argument can be useful to tell R how many lines to skip before it should start reading in the data. In the example above, we would set this value to 3: canlang_data &lt;- read_csv(&quot;data/can_lang_meta-data.csv&quot;, skip = 3) canlang_data ## # A tibble: 214 x 6 ## category language mother_tongue most_at_home most_at_work lang_known ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Aboriginal la… Aboriginal… 590 235 30 665 ## 2 Non-Official … Afrikaans 10260 4785 85 23415 ## 3 Non-Official … Afro-Asiat… 1150 445 10 2775 ## 4 Non-Official … Akan (Twi) 13460 5985 25 22150 ## 5 Non-Official … Albanian 26895 13135 345 31930 ## 6 Aboriginal la… Algonquian… 45 10 0 120 ## 7 Aboriginal la… Algonquin 1260 370 40 2480 ## 8 Non-Official … American S… 2685 3020 1145 21930 ## 9 Non-Official … Amharic 22465 12785 200 33670 ## 10 Non-Official … Arabic 419890 223535 5585 629055 ## # … with 204 more rows 2.4.2 read_delim as a more flexible method to get tabular data into R When our tabular data comes in a different format, we can use the read_delim function instead. For example, a different version of this same data set has no column names and uses tabs as the delimiter instead of commas. Here is how the file would look in a plain text editor: Aboriginal languages Aboriginal languages, n.o.s. 590 235 30 665 Non-Official &amp; Non-Aboriginal languages Afrikaans 10260 4785 85 23415 Non-Official &amp; Non-Aboriginal languages Afro-Asiatic languages, n.i.e. 1150 445 10 2775 Non-Official &amp; Non-Aboriginal languages Akan (Twi) 13460 5985 25 22150 Non-Official &amp; Non-Aboriginal languages Albanian 26895 13135 345 31930 Aboriginal languages Algonquian languages, n.i.e. 45 10 0 120 Aboriginal languages Algonquin 1260 370 40 2480 Non-Official &amp; Non-Aboriginal languages American Sign Language 2685 3020 1145 21930 Non-Official &amp; Non-Aboriginal languages Amharic 22465 12785 200 33670 Non-Official &amp; Non-Aboriginal languages Arabic 419890 223535 5585 629055 To get this into R using the read_delim() function, we specify the first argument as the path to the file (as done with read_csv), and then provide values to the delim argument (here a tab, which we represent by \"\\t\") and the col_names argument (here we specify that there are no column names to assign, and give it the value of FALSE). Both read_csv() and read_delim() have a col_names argument and the default is TRUE. canlang_data &lt;- read_delim(&quot;data/can_lang.tsv&quot;, delim = &quot;\\t&quot;, col_names = FALSE) canlang_data ## # A tibble: 214 x 6 ## X1 X2 X3 X4 X5 X6 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Aboriginal languages Aboriginal languages… 590 235 30 665 ## 2 Non-Official &amp; Non-Aborigin… Afrikaans 10260 4785 85 23415 ## 3 Non-Official &amp; Non-Aborigin… Afro-Asiatic languag… 1150 445 10 2775 ## 4 Non-Official &amp; Non-Aborigin… Akan (Twi) 13460 5985 25 22150 ## 5 Non-Official &amp; Non-Aborigin… Albanian 26895 13135 345 31930 ## 6 Aboriginal languages Algonquian languages… 45 10 0 120 ## 7 Aboriginal languages Algonquin 1260 370 40 2480 ## 8 Non-Official &amp; Non-Aborigin… American Sign Langua… 2685 3020 1145 21930 ## 9 Non-Official &amp; Non-Aborigin… Amharic 22465 12785 200 33670 ## 10 Non-Official &amp; Non-Aborigin… Arabic 419890 223535 5585 629055 ## # … with 204 more rows Data frames in R need to have column names, thus if you read data into R as a data frame without column names then R assigns column names for them. If you used the read_* functions to read the data into R, then R gives each column a name of X1, X2, …, XN, where N is the number of columns in the data set. 2.4.3 Reading tabular data directly from a URL We can also use read_csv() or read_delim() (and related functions) to read in tabular data directly from a uniform resource locator (URL) that contains tabular data. In this case, we provide the URL to read_csv() as the path to the file instead of a path to a local file on our computer. Similar to when we specify a path on our local computer, here we need to surround the URL by quotes. All other arguments that we use are the same as when using these functions with a local file on our computer. canlang_data &lt;- read_csv(&quot;https://raw.githubusercontent.com/UBC-DSCI/introduction-to-datascience/master/data/can_lang.csv&quot;) canlang_data ## # A tibble: 214 x 6 ## category language mother_tongue most_at_home most_at_work lang_known ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Aboriginal la… Aboriginal… 590 235 30 665 ## 2 Non-Official … Afrikaans 10260 4785 85 23415 ## 3 Non-Official … Afro-Asiat… 1150 445 10 2775 ## 4 Non-Official … Akan (Twi) 13460 5985 25 22150 ## 5 Non-Official … Albanian 26895 13135 345 31930 ## 6 Aboriginal la… Algonquian… 45 10 0 120 ## 7 Aboriginal la… Algonquin 1260 370 40 2480 ## 8 Non-Official … American S… 2685 3020 1145 21930 ## 9 Non-Official … Amharic 22465 12785 200 33670 ## 10 Non-Official … Arabic 419890 223535 5585 629055 ## # … with 204 more rows 2.4.4 Previewing a data file before reading it into R In all the examples above, we gave you previews of the data file before we read it into R. This is essential so that you can see whether or not there are column names, what the delimiters are, and if there are lines you need to skip. You should do this yourself when trying to read in data files. In Jupyter, you preview data as a plain text file by right-clicking on the file’s name in the Jupyter home menu and selecting “Open with” and then selecting “Editor.” Figure 2.2: Opening data files with an editor in Jupyter Figure 2.3: A data file as viewed in an editor in Jupyter If you do not specify to open the data file with an editor, then Jupyter will render a nice table for you and you will not be able to see the column delimiters, and therefore you will not know which function to use, nor which arguments to use and values to specify for them. This is also demonstrated in the video below: 2.5 Reading data from an Microsoft Excel file There are many other ways to store tabular data sets beyond plain text files, and similarly, many ways to load those data sets into R. For example, it is very common to encounter, and need to load into R, data stored as a Microsoft Excel spreadsheet (with the filename extension .xlsx). To be able to do this, a key thing to know is that even though .csv and .xlsx files look almost identical when loaded into Excel, the data themselves are stored completely differently. While .csv files are plain text files, where the characters you see when you open the file in a text editor are exactly the data they represent, this is not the case for .xlsx files. Take a look at what a .xlsx file would look like in a text editor: ,?&#39;O _rels/.rels???J1??&gt;E?{7? &lt;?V????w8?&#39;J???&#39;QrJ???Tf?d??d?o?wZ&#39;???@&gt;?4&#39;?|??hlIo??F t 8f??3wn ????t??u&quot;/ %~Ed2??&lt;?w?? ?Pd(??J-?E???7?&#39;t(?-GZ?????y???c~N?g[^_r?4 yG?O ?K??G?RPX?&lt;??,?&#39;O[Content_Types].xml???n?0E%?J ]TUEe??O??c[???????6q??s??d?m???\\???H?^????3} ?rZY? ?:L60?^?????XTP+?|?3???&quot;~?3T1W3???,?#p?R?!??w(??R???[S?D?kP?P!XS(?i?t?$?ei X?a??4VT?,D?Jq D ?????u?]??;??L?.8AhfNv}?hHF*??Jr?Q?%?g?U??CtX&quot;8x&gt;?.|????5j?/$???JE?c??~??4iw?????E;?+?S??w?cV+?:???2l???=?2nel???;|?V??????c&#39;?????9?P&amp;Bcj,?&#39;OdocProps/app.xml??1 ?0???k????A?u?U?]??{#?:;/&lt;?g?Cd????M+?=???Z?O??R+??u?P?X KV@??M$??a???d?_???4??5v?R????9D????t??Fk?Ú&#39;P?=?,?&#39;OdocProps/core.xml??MO?0 ??J?{???3j?h&#39;??(q??U4J ??=i?I&#39;?b??[v?!??{gk? F2????v5yj??&quot;J???,?d???J???C??l??4?-?`$?4t?K?.;?%c?J??G&lt;?H???? X????z???6?????~q??X??????q^&gt;??tH???*?D???M?g ??D?????????d?:g).?3.??j?P?F?&#39;Oxl/_rels/workbook.xml.rels??Ak1??J?{7???R?^J?kk@Hf7??I?L???E]A?Þ?{a??`f?????b?6xUQ?@o?m}??o????X{???Q?????;?y?\\? O ?YY??4?L??S??k?252j?? ??V ?C?g?C]??????? ? ???E??TENyf6% ?Y????|??:%???}^ N?Q?N&#39;????)??F?\\??P?G??,?&#39;O&#39;xl/printerSettings/printerSettings1.bin?Wmn? ??Sp&gt;?G???q?# ?I??5R&#39;???q????(?L ??m??8F?5&lt; L`??`?A??2{dp??9R#?&gt;7??Xu???/?X??HI?|? ??r)???\\?VA8?2dFfq???I]]o 5`????6A ? This type of file representation allows Excel files to store additional things that you cannot store in a .csv file, such as fonts, text formatting, graphics, multiple sheets and more. And despite looking odd in a plain text editor, we can read Excel spreadsheets into R using the readxl package developed specifically for this purpose. library(readxl) canlang_data &lt;- read_excel(&quot;data/can_lang.xlsx&quot;) canlang_data ## # A tibble: 214 x 6 ## category language mother_tongue most_at_home most_at_work lang_known ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Aboriginal la… Aboriginal… 590 235 30 665 ## 2 Non-Official … Afrikaans 10260 4785 85 23415 ## 3 Non-Official … Afro-Asiat… 1150 445 10 2775 ## 4 Non-Official … Akan (Twi) 13460 5985 25 22150 ## 5 Non-Official … Albanian 26895 13135 345 31930 ## 6 Aboriginal la… Algonquian… 45 10 0 120 ## 7 Aboriginal la… Algonquin 1260 370 40 2480 ## 8 Non-Official … American S… 2685 3020 1145 21930 ## 9 Non-Official … Amharic 22465 12785 200 33670 ## 10 Non-Official … Arabic 419890 223535 5585 629055 ## # … with 204 more rows If the .xlsx file has multiple sheets, you have to use the sheet argument to specify the sheet number or name. You can also specify cell ranges using the range argument. This functionality is useful when a single sheet contains multiple tables (a sad thing that happens to many Excel spreadsheets). As with plain text files, you should always explore the data file before importing it into R. Exploring the data beforehand helps you decide which arguments you need to to load the data into R successfully. If you do not have the Excel program on your computer, you can use other programs to preview the file. Examples include Google Sheets and Libre Office. 2.6 Reading data from a database Another very common form of data storage is the relational database. There are many relational database management systems, such as SQLite, MySQL, PostgreSQL, Oracle, and many more. These different relational database management systems each have their own advantages and limitations. Almost all employ SQL (structured query language) to pull data from the database. Thankfully, you don’t need to know SQL to analyze data from a database; several packages have been written that allows R to connect to relational databases and use the R programming language as the front end (what the user types in) to pull data from them. These different relational database management systems have their own advantages, limitations, and excels in particular scenarios. In this book, we will give examples of how to do this using R with SQLite and PostgreSQL databases. 2.6.1 Connecting to a database 2.6.1.1 Reading data from a SQLite database SQLite is probably the simplest relational database that one can use in combination with R. SQLite databases are self-contained and usually stored and accessed locally on one computer. Data is usually stored in a file with a .db extension. Similar to Excel files, these are not plain text files and cannot be read in a plain text editor. The first thing you need to do to read data into R from a database is to connect to the database. We do that using the dbConnect function from the DBI (database interface) package. This does not read in the data, but simply tells R where the database is and opens up a communication channel. library(DBI) con_lang_data &lt;- dbConnect(RSQLite::SQLite(), &quot;data/can_lang.db&quot;) Often relational databases have many tables, and their power comes from the useful ways they can be joined. Thus anytime you want to access data from a relational database, you need to know the table names. You can get the names of all the tables in the database using the dbListTables function: tables &lt;- dbListTables(con_lang_data) tables ## [1] &quot;lang&quot; We only get one table name returned from calling dbListTables, which tells us that there is only one table in this database. To reference a table in the database to do things like select columns and filter rows, we use the tbl function from the dbplyr package. The package dbplyr allows us to work with data stored in databases as if they were local data frames, which is useful because we can do a lot with big datasets without actually having to bring these vast amounts of data into your computer! library(dbplyr) lang_db &lt;- tbl(con_lang_data, &quot;lang&quot;) lang_db ## # Source: table&lt;lang&gt; [?? x 6] ## # Database: sqlite 3.35.5 [/ids/data/can_lang.db] ## category language mother_tongue most_at_home most_at_work lang_known ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Aboriginal la… Aboriginal… 590 235 30 665 ## 2 Non-Official … Afrikaans 10260 4785 85 23415 ## 3 Non-Official … Afro-Asiat… 1150 445 10 2775 ## 4 Non-Official … Akan (Twi) 13460 5985 25 22150 ## 5 Non-Official … Albanian 26895 13135 345 31930 ## 6 Aboriginal la… Algonquian… 45 10 0 120 ## 7 Aboriginal la… Algonquin 1260 370 40 2480 ## 8 Non-Official … American S… 2685 3020 1145 21930 ## 9 Non-Official … Amharic 22465 12785 200 33670 ## 10 Non-Official … Arabic 419890 223535 5585 629055 ## # … with more rows Although it looks like we just got a data frame from the database, we didn’t! It’s a reference, showing us data that is still in the SQLite database (note the first two lines of the output). It does this because databases are often more efficient at selecting, filtering and joining large data sets than R. And typically, the database will not even be stored on your computer, but rather a more powerful machine somewhere on the web. So R is lazy and waits to bring this data into memory until you explicitly tell it to do so using the collect function from the dbplyr package. Here we will filter for only rows in the Aboriginal languages category according to the 2016 Canada Census, and then use collect to finally bring this data into R as a data frame. aboriginal_lang_db &lt;- filter(lang_db, category == &quot;Aboriginal languages&quot;) aboriginal_lang_db ## # Source: lazy query [?? x 6] ## # Database: sqlite 3.35.5 [/ids/data/can_lang.db] ## category language mother_tongue most_at_home most_at_work lang_known ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Aboriginal … Aboriginal l… 590 235 30 665 ## 2 Aboriginal … Algonquian l… 45 10 0 120 ## 3 Aboriginal … Algonquin 1260 370 40 2480 ## 4 Aboriginal … Athabaskan l… 50 10 0 85 ## 5 Aboriginal … Atikamekw 6150 5465 1100 6645 ## 6 Aboriginal … Babine (Wets… 110 20 10 210 ## 7 Aboriginal … Beaver 190 50 0 340 ## 8 Aboriginal … Blackfoot 2815 1110 85 5645 ## 9 Aboriginal … Carrier 1025 250 15 2100 ## 10 Aboriginal … Cayuga 45 10 10 125 ## # … with more rows aboriginal_lang_data &lt;- collect(aboriginal_lang_db) aboriginal_lang_data ## # A tibble: 67 x 6 ## category language mother_tongue most_at_home most_at_work lang_known ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Aboriginal … Aboriginal l… 590 235 30 665 ## 2 Aboriginal … Algonquian l… 45 10 0 120 ## 3 Aboriginal … Algonquin 1260 370 40 2480 ## 4 Aboriginal … Athabaskan l… 50 10 0 85 ## 5 Aboriginal … Atikamekw 6150 5465 1100 6645 ## 6 Aboriginal … Babine (Wets… 110 20 10 210 ## 7 Aboriginal … Beaver 190 50 0 340 ## 8 Aboriginal … Blackfoot 2815 1110 85 5645 ## 9 Aboriginal … Carrier 1025 250 15 2100 ## 10 Aboriginal … Cayuga 45 10 10 125 ## # … with 57 more rows Why bother to use the collect function? The data looks pretty similar in both outputs shown above. And dbplyr provides lots of functions similar to filter that you can use to directly feed the database reference (what tbl gives you) into downstream analysis functions (e.g., ggplot2 for data visualization and lm for linear regression modeling). However, this does not work in every case; look what happens when we try to use nrow to count rows in a data frame: nrow(aboriginal_lang_db) ## [1] NA or tail to preview the last 6 rows of a data frame: tail(aboriginal_lang_db) ## Error: tail() is not supported by sql sources Additionally, some operations will not work to extract columns or single values from the reference given by the tbl function. Thus, once you have finished your data wrangling of the tbl database reference object, it is advisable to bring it into your local machine’s memory using collect as a data frame. Warning: Usually, databases are very big! Reading the object into your local machine may give an error or take a lot of time to run so be careful if you plan to do this! 2.6.1.2 Reading data from a PostgreSQL database PostgreSQL (also called Postgres) is a very popular and open-source option for relational database software. Unlike SQLite, PostgreSQL uses a client–server database engine, as it was designed to be used and accessed on a network. This means that you have to provide more information to R when connecting to Postgres databases. The additional information that you need to include when you call the dbConnect function is listed below: dbname - the name of the database (a single PostgreSQL instance can host more than one database) host - the URL pointing to where the database is located port - the communication endpoint between R and the PostgreSQL database (this is typically 5432 for PostgreSQL) user - the username for accessing the database password - the password for accessing the database Additionally, we must use the RPostgres package instead of RSQLite in the dbConnect function call. Below we demonstrate how to connect to a version of the can_mov_db database, which contains information about Canadian movies (note - this is a synthetic, or artificial, database). library(RPostgres) can_mov_db_con &lt;- dbConnect(RPostgres::Postgres(), dbname = &quot;can_mov_db&quot;, host = &quot;r7k3-mds1.stat.ubc.ca&quot;, port = 5432, user = &quot;user0001&quot;, password = &#39;################&#39;) 2.6.2 Interacting with a database After opening the connection, everything looks and behaves almost identically to when we were using an SQLite database in R. For example, we can again use dbListTables to find out what tables are in the can_mov_db database: dbListTables(can_mov_db_con) [1] &quot;themes&quot; &quot;medium&quot; &quot;titles&quot; &quot;title_aliases&quot; &quot;forms&quot; [6] &quot;episodes&quot; &quot;names&quot; &quot;names_occupations&quot; &quot;occupation&quot; &quot;ratings&quot; We see that there are 10 tables in this database. Let’s first look at the \"ratings\" table to find the lowest rating that exists in the can_mov_db database: ratings_db &lt;- tbl(can_mov_db_con, &quot;ratings&quot;) ratings_db # Source: table&lt;ratings&gt; [?? x 3] # Database: postgres [user0001@r7k3-mds1.stat.ubc.ca:5432/can_mov_db] title average_rating num_votes &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; 1 The Grand Seduction 6.6 150 2 Rhymes for Young Ghouls 6.3 1685 3 Mommy 7.5 1060 4 Incendies 6.1 1101 5 Bon Cop, Bad Cop 7.0 894 6 Goon 5.5 1111 7 Monsieur Lazhar 5.6 610 8 What if 5.3 1401 9 The Barbarian Invations 5.8 99 10 Away from Her 6.9 2311 # … with more rows To find the lowest rating that exists in the data base, we first need to extract the average_rating column using select: avg_rating_db &lt;- select(ratings_db, average_rating) avg_rating_db # Source: lazy query [?? x 1] # Database: postgres [user0001@r7k3-mds1.stat.ubc.ca:5432/can_mov_db] average_rating &lt;dbl&gt; 1 6.6 2 6.3 3 7.5 4 6.1 5 7.0 6 5.5 7 5.6 8 5.3 9 5.8 10 6.9 # … with more rows Next we use min to find the minimum rating in that column: min(avg_rating_db) Error in min(avg_rating_db) : invalid &#39;type&#39; (list) of argument Instead of the minimum, we get an error! This is another example of when we need to use the collect function to bring the data into R for further computation: avg_rating_data &lt;- collect(avg_rating_db) min(avg_rating_data) [1] 1 We see the lowest rating given to a movie is 1, indicating that it must have been a really bad movie… Why should we bother with databases at all? Opening a database stored in a .db file involved a lot more effort than just opening a .csv, .tsv, or any of the other plain text or Excel formats. It was a bit of a pain to use a database in that setting since we had to use dbplyr to translate tidyverse-like commands (filter, select, head, etc.) into SQL commands that the database understands. Not all tidyverse commands can currently be translated with SQLite databases. For example, we can compute a mean with an SQLite database but can’t easily compute a median. So you might be wondering why should we use databases at all? Databases are beneficial in a large-scale setting: they enable storing large data sets across multiple computers with automatic redundancy and backups they allow multiple users to access them simultaneously and remotely without conflicts and errors they provide mechanisms for ensuring data integrity and validating input they provide security to keep data safe For example, there are billions of Google searches conducted daily. Can you imagine if Google stored all of the data from those queries in a single .csv file!? Chaos would ensue! 2.7 Writing data from R to a .csv file At the middle and end of a data analysis, we often want to write a data frame that has changed (either through filtering, selecting, mutating or summarizing) to a file to share it with others or use it for another step in the analysis. The most straightforward way to do this is to use the write_csv function from the tidyverse package. The default arguments for this file are to use a comma (,) as the delimiter and include column names. Below we demonstrate creating a new version of the Canadian languages data set without the official languages category according to the Canadian 2016 Census, and then writing this to a .csv file: no_official_lang_data &lt;- filter(can_lang, category != &quot;Official languages&quot;) write_csv(no_official_lang_data, &quot;data/no_official_languages.csv&quot;) 2.8 Obtaining data from the web This section is not required reading for the remainder of the textbook. It is included for those readers interested in learning a little bit more about how to obtain different types of data from the web. Data doesn’t just magically appear on your computer; you need to get it from somewhere. Earlier in the chapter we showed you how to access data stored in a plaintext, spreadsheet-like format (e.g., comma- or tab-separated) from a web URL using one of the read_* functions from the tidyverse. But as time goes on, it is increasingly uncommon to find data (especially large amounts of data) in this format available for download from a URL. Instead, websites now often offer something known as an application programming interface (API), which provides a programmatic way to ask for subsets of a dataset. This allows the website owner to control who has access to the data, what portion of the data they have access to, and how much data they can access. Typically, the website owner will give you a token (a secret string of characters somewhat like a password) that you have to provide when accessing the API. Another interesting thought: websites themselves are data! When you type a URL into your browser window, your browser asks the web server (another computer on the internet whose job it is to respond to requests for the website) to give it the website’s data, and then your browser translates that data into something you can see. If the website shows you some information that you’re interested in, you could create a data set for yourself by copying and pasting that information into a file. This process of taking information directly from what a website displays is called web scraping (or sometimes screen scraping). Now, of course, copying and pasting information manually is a painstaking and error-prone process, especially when there is a lot of information to gather. So instead of asking your browser to translate the information that the web server provides into something you can see, you can collect that data programmatically—in the form of hypertext markup language (HTML) and cascading style sheet (CSS) code—and process it to extract useful information. This subsection will show you the basics of both web scraping with the rvest R package and accessing the Twitter API using the rtweet R package. 2.8.1 Web scraping HTML and CSS selectors When you enter a URL into your browser, your browser connects to the web server at that URL and asks for the source code for the website. This is the data that the browser translates into something you can see; so if we are going to create our own data by scraping a website, we have to first understand what that data looks like! For example, let’s say we are interested in knowing the average rental price (per square foot) of the most recently available one-bedroom apartments in Vancouver from https://vancouver.craigslist.org. When we visit the Vancouver Craigslist website and search for one-bedroom apartments, this is what we are shown: Based on what our browser shows us, it’s pretty easy to find the size and price for each apartment listed. But we would like to be able to obtain that information using R, without any manual human effort or copying and pasting. We do this by examining the source code that the web server actually sent our browser to display for us. We show a snippet of it below (and link to the entire source code here): &lt;span class=&quot;result-meta&quot;&gt; &lt;span class=&quot;result-price&quot;&gt;$800&lt;/span&gt; &lt;span class=&quot;housing&quot;&gt; 1br - &lt;/span&gt; &lt;span class=&quot;result-hood&quot;&gt; (13768 108th Avenue)&lt;/span&gt; &lt;span class=&quot;result-tags&quot;&gt; &lt;span class=&quot;maptag&quot; data-pid=&quot;6786042973&quot;&gt;map&lt;/span&gt; &lt;/span&gt; &lt;span class=&quot;banish icon icon-trash&quot; role=&quot;button&quot;&gt; &lt;span class=&quot;screen-reader-text&quot;&gt;hide this posting&lt;/span&gt; &lt;/span&gt; &lt;span class=&quot;unbanish icon icon-trash red&quot; role=&quot;button&quot; aria-hidden=&quot;true&quot;&gt;&lt;/span&gt; &lt;a href=&quot;#&quot; class=&quot;restore-link&quot;&gt; &lt;span class=&quot;restore-narrow-text&quot;&gt;restore&lt;/span&gt; &lt;span class=&quot;restore-wide-text&quot;&gt;restore this posting&lt;/span&gt; &lt;/a&gt; &lt;/span&gt; &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;result-row&quot; data-pid=&quot;6788463837&quot;&gt; &lt;a href=&quot;https://vancouver.craigslist.org/nvn/apa/d/north-vancouver-luxury-1-bedroom/6788463837.html&quot; class=&quot;result-image gallery&quot; data-ids=&quot;1:00U0U_lLWbuS4jBYN,1:00T0T_9JYt6togdOB,1:00r0r_hlMkwxKqoeq,1:00n0n_2U8StpqVRYX,1:00M0M_e93iEG4BRAu,1:00a0a_PaOxz3JIfI,1:00o0o_4VznEcB0NC5,1:00V0V_1xyllKkwa9A,1:00G0G_lufKMygCGj6,1:00202_lutoxKbVTcP,1:00R0R_cQFYHDzGrOK,1:00000_hTXSBn1SrQN,1:00r0r_2toXdps0bT1,1:01616_dbAnv07FaE7,1:00g0g_1yOIckt0O1h,1:00m0m_a9fAvCYmO9L,1:00C0C_8EO8Yl1ELUi,1:00I0I_iL6IqV8n5MB,1:00b0b_c5e1FbpbWUZ,1:01717_6lFcmuJ2glV&quot;&gt; &lt;span class=&quot;result-price&quot;&gt;$2285&lt;/span&gt; &lt;/a&gt; &lt;p class=&quot;result-info&quot;&gt; &lt;span class=&quot;icon icon-star&quot; role=&quot;button&quot;&gt; &lt;span class=&quot;screen-reader-text&quot;&gt;favorite this post&lt;/span&gt; &lt;/span&gt; &lt;time class=&quot;result-date&quot; datetime=&quot;2019-01-06 12:06&quot; title=&quot;Sun 06 Jan 12:06:01 PM&quot;&gt;Jan 6&lt;/time&gt; &lt;a href=&quot;https://vancouver.craigslist.org/nvn/apa/d/north-vancouver-luxury-1-bedroom/6788463837.html&quot; data-id=&quot;6788463837&quot; class=&quot;result-title hdrlnk&quot;&gt;Luxury 1 Bedroom CentreView with View - Lonsdale&lt;/a&gt; Oof…you can tell that the source code for a web page is not really designed for humans to understand easily. However, if you look through it closely, you will find that the information we’re interested in is hidden among the muck. For example, near the top of the snippet above you can see a line that looks like &lt;span class=&quot;result-price&quot;&gt;$800&lt;/span&gt; That is definitely storing the price of a particular apartment. With some more investigation, you should be able to find things like the date and time of the listing, the address of the listing, and more. So this source code most likely contains all the information we are interested in! Let’s dig into that line above a bit more. You can see that that bit of code has an opening tag (words between &lt; and &gt;, like &lt;span&gt;) and a closing tag (the same with a slash, like &lt;/span&gt;). HTML source code generally stores its data between opening and closing tags like these. Above you can see that the information we want ($800) is stored between an opening and closing tag (&lt;span&gt; and &lt;/span&gt;). In the opening tag, you can also see a very useful “class” (a special word that is sometimes included with opening tags): class=\"result-price\". Since we want R to programmatically sort through all of the source code for the website to find apartment prices, maybe we can look for all the tags with the \"result-price\" class, and grab the information between the opening and closing tag. Indeed, take a look at another line of the source snippet above: &lt;span class=&quot;result-price&quot;&gt;$2285&lt;/span&gt; It’s yet another price for an apartment listing, and the tags surrounding it have the \"result-price\" class. Wonderful! Now that we know what pattern we are looking for—a dollar amount between opening and closing tags that have the \"result-price\" class—we should be able to use code to pull out all of the matching patterns from the source code to obtain our data. This sort of “pattern” is known as a CSS selector (where CSS stands for cascading style sheet). The above was a particularly simple example of “finding the pattern to look for”; many websites are quite a bit larger and more complex, and so is their website source code. Fortunately there are tools available to make this process easier. For example, SelectorGadget from the Chrome Web Store is an open source tool that simplifies identifying the generating and finding CSS selectors. At the end of the chapter in the additional resources section, we include a link to a short video on how to install and use the SelectorGadget tool to obtain CSS selectors for use in web scraping. After installing and enabling the tool, you can simply click the website element for which you want an appropriate selector. For example, if we click the price of an apartment listing, we find that SelectorGadget shows us the selector .result-price in its toolbar, and highlights all the other apartment prices that would be obtained using that selector. If we then click the size of an apartment listing, SelectorGadget shows us the span selector, and highlights much of the page; this indicates that the span selector is not specific enough to just capture apartment sizes. To narrow the selector, we can click one of the highlighted elements that we do not want. For example, we can deselect the “pic/map” links, resulting in only the data we want highlighted using the .housing selector. So to scrape information about the square footage and rental price of apartment listings, we need to use the two CSS selectors .housing and .result-price, respectively. The selector gadget returns them to us as a comma separated list (here .housing , .result-price), which is exactly the format we need to provide to R if we are using more than one CSS selector. STOP! Are you allowed to scrape that website? Before scraping data from the web, you should always check whether or not you are allowed to scrape it! There are two documents that are important for this: the robots.txt file and the Terms of Service document. Let’s take a look at Craigslist’s Terms of Service document: “You agree not to copy/collect CL content via robots, spiders, scripts, scrapers, crawlers, or any automated or manual equivalent (e.g., by hand).” source: https://www.craigslist.org/about/terms.of.use Want to learn more about the legalities of web scraping and crawling? Read this interesting blog post titled “Web Scraping and Crawling Are Perfectly Legal, Right?” by Benoit Bernard. So what to do now? Well, we could ask the owner of Craigslist for permission to scrape. However, we are not likely to get a response, and even if we did they would not likely give us permission. The more realistic answer is that we simply cannot scrape Craigslist. If we still want to find data about rental prices in Vancouver, we must go elsewhere. To continue learning how to scrape data from the web, let’s instead scrape data on the population of Canadian cities from Wikipedia. We have checked the Terms of Service document, and it does not mention that web scraping is disallowed. We will use the SelectorGadget tool to pick elements that we are interested in (city names and population counts) and deselect others to indicate that we are not interested in them (province names), as shown in the screenshot below. We include a link to a short video tutorial on this process at the end of the chapter in the additional resources section. SelectorGadget provides in its toolbar the following list of CSS selectors to use: td:nth-child(5), td:nth-child(7), .infobox:nth-child(122) td:nth-child(1), .infobox td:nth-child(3) Using rvest Now that we have our CSS selectors we can use the rvest R package to scrape our desired data from the website. We start by loading the rvest package: library(rvest) Next, we tell R what page we want to scrape by providing the webpage’s URL in quotations to the function read_html: page &lt;- read_html(&quot;https://en.wikipedia.org/wiki/Canada&quot;) The read_html function directly downloads the source code for the page at the URL you specify, just like your browser would if you navigated to that site. But instead of displaying the website to you, the read_html function just returns the HTML source code itself, which we have stored in the page variable. Next, we send the page object to the html_nodes function, along with the CSS selectors we obtained from the SelectorGadget tool. Make sure to surround the selectors with quotations; html_nodes expects that argument to be a string. The html_nodes function then selects nodes from the HTML document that match the CSS selectors you specified. A node is an HTML tag pair combined with the content stored between the tags. For our CSS selector td:nth-child(5), an example node that would be selected would be: &lt;td style=&quot;text-align:left;background:#f0f0f0;&quot;&gt; &lt;a href=&quot;/wiki/London,_Ontario&quot; title=&quot;London, Ontario&quot;&gt;London&lt;/a&gt; &lt;/td&gt; We store the result of the html_nodes function in the population_nodes variable. Note that below we use the paste function with a comma separator (sep=\",\") to build the list of selectors while maintaining code readability; this avoids having one very long line of code with the string \"td:nth-child(5),td:nth-child(7),.infobox:nth-child(122) td:nth-child(1),.infobox td:nth-child(3)\" as the second argument of html_nodes: selectors &lt;- paste(&quot;td:nth-child(5)&quot;, &quot;td:nth-child(7)&quot;, &quot;.infobox:nth-child(122) td:nth-child(1)&quot;, &quot;.infobox td:nth-child(3)&quot;, sep=&quot;,&quot;) population_nodes &lt;- html_nodes(page, selectors) head(population_nodes) ## {xml_nodeset (6)} ## [1] &lt;td style=&quot;text-align:right;&quot;&gt;5,928,040&lt;/td&gt; ## [2] &lt;td style=&quot;text-align:left;background:#f0f0f0;&quot;&gt;&lt;a href=&quot;/wiki/London,_On ... ## [3] &lt;td style=&quot;text-align:right;&quot;&gt;494,069\\n&lt;/td&gt; ## [4] &lt;td style=&quot;text-align:right;&quot;&gt;4,098,927&lt;/td&gt; ## [5] &lt;td style=&quot;text-align:left;background:#f0f0f0;&quot;&gt;\\n&lt;a href=&quot;/wiki/St._Cath ... ## [6] &lt;td style=&quot;text-align:right;&quot;&gt;406,074\\n&lt;/td&gt; Next we extract the meaningful data—in other words, we get rid of the HTML code syntax and tags—from the nodes using the html_text function. In the case of the example node above, html_text function returns \"London\". population_text &lt;- html_text(population_nodes) head(population_text) ## [1] &quot;5,928,040&quot; &quot;London&quot; &quot;494,069\\n&quot; ## [4] &quot;4,098,927&quot; &quot;St. Catharines–Niagara&quot; &quot;406,074\\n&quot; Fantastic! We seem to have extracted the data of interest from the raw HTML source code. But we are not quite done; the data is not yet in an optimal format for data analysis. Both the city names and population are encoded as characters in a single vector, instead of being in a data frame with one character column for city and one numeric column for population (like a spreadsheet). Additionally, the populations contain commas (not useful for programmatically dealing with numbers), and some even contain a line break character at the end (\\n). In the next chapter, we will learn more about how to wrangle data such as this into a more useful format for data analysis using R. 2.8.2 Using an API Rather than posting a data file at a URL for you to download, many websites these days provide an API that must be accessed through a programming language like R. The benefit of this is that data owners have much more control over the data they provide to users. However, unlike web scraping, there is no consistent way to access an API across websites. Every website typically has its own API designed specially for its own use-case. Therefore we will just provide one example of accessing data through an API in this book, with the hope that it gives you enough of a basic idea that you can learn how to use another API if needed. In particular, in this book we will show you the basics of how to use the rtweet package in R to access data from the Twitter API. One nice feature of this particular API is that you don’t need a special token to access it; you simply need to make an account with them. Your access to the data will then be authenticated and controlled through your account username and password. If you have a Twitter account already (or are willing to make one), you can follow along with the examples that we show here. To get started, load the rtweet package: library(rtweet) This package provides an extensive set of functions to search Twitter for tweets, users, their followers, and more. Let’s construct a small dataset of the last 400 tweets and retweets from the Tidyverse account. A few of the most recent tweets are shown below. Figure 2.4: The Tidyverse Twitter feed. Source STOP! Think about your API usage carefully! When you access an API, you are initiating a transfer of data from a web server to your computer. Web servers are expensive to run and do not have infinite resources. If you try to ask for too much data at once, you can use up a huge amount of the server’s bandwidth. If you try to ask for data too frequently—e.g., if you make many requests to the server in quick succession—you can also bog the server down and make it unable to talk to anyone else. Most servers have mechanisms to revoke your access if you are not careful, but you should try to prevent issues from happening in the first place by being extra careful with how you write and run your code. You should also keep in mind that when a website owner grants you API access, they also usually specify a limit (or quota) of how much data you can ask for. Be careful not to overrun your quota! In this example, we should take a look at the Twitter website to see what limits we should abide by when using the API. Using rtweet After checking the website, it seems like asking for 400 tweets one time is acceptable. So we can use the get_timelines function to ask for the last 400 tweets from the Tidyverse account. tidyverse_tweets &lt;- get_timelines(&#39;tidyverse&#39;, n=400) When you call the get_timelines for the first time (or any other rtweet function that accesses the API), you will see a browser pop-up that looks something like this: Figure 2.5: The rtweet authorization prompt. This is the rtweet package asking you to provide your own Twitter account’s login information. When rtweet talks to the Twitter API, it uses your account information to authenticate requests; Twitter then can keep track of how much data you’re asking for, and how frequently you’re asking. If you want to follow along with this example using your own Twitter account, you should read over the list of permissions you are granting rtweet very carefully and make sure you are comfortable with it. Note that rtweet can be used to manage most aspects of your account (make posts, follow others, etc.), which is why rtweet asks for such extensive permissions. If you decide to allow rtweet to talk to the Twitter API using your account information, then input your username and password and hit “Sign In.” Twitter will probably send you an email to say that there was an unusual login attempt on your account, and in that case you will have to take the one-time code they send you and provide that to the rtweet login page too. Every API has its own way to authenticate users when they try to access data. Many APIs require you to sign up to receive a token, which is a secret password that you input into the R package (like rtweet) that you are using to access the API. With the authentication setup out of the way, let’s run the get_timelines function again to actually access the API and take a look at what was returned: tidyverse_tweets &lt;- get_timelines(&#39;tidyverse&#39;, n=400) tidyverse_tweets ## # A tibble: 293 x 71 ## created_at reply_to_status_id quoted_created_at reply_to_user_id ## &lt;dttm&gt; &lt;lgl&gt; &lt;dttm&gt; &lt;lgl&gt; ## 1 2021-04-29 11:59:04 NA NA NA ## 2 2021-04-26 17:05:47 NA NA NA ## 3 2021-04-24 09:13:12 NA NA NA ## 4 2021-04-18 06:06:21 NA NA NA ## 5 2021-04-12 05:48:33 NA NA NA ## 6 2021-04-08 17:45:34 NA NA NA ## 7 2021-04-01 05:01:38 NA NA NA ## 8 2021-03-25 06:05:49 NA NA NA ## 9 2021-03-18 17:16:21 NA NA NA ## 10 2021-03-12 19:12:49 NA NA NA ## # … with 283 more rows, and 67 more variables: reply_to_screen_name &lt;lgl&gt;, ## # is_quote &lt;lgl&gt;, is_retweet &lt;lgl&gt;, quoted_verified &lt;lgl&gt;, ## # retweet_verified &lt;lgl&gt;, protected &lt;lgl&gt;, verified &lt;lgl&gt;, ## # account_lang &lt;lgl&gt;, profile_background_url &lt;lgl&gt;, user_id &lt;dbl&gt;, ## # status_id &lt;dbl&gt;, screen_name &lt;chr&gt;, text &lt;chr&gt;, source &lt;chr&gt;, ## # ext_media_type &lt;lgl&gt;, lang &lt;chr&gt;, quoted_status_id &lt;dbl&gt;, ## # quoted_text &lt;chr&gt;, quoted_source &lt;chr&gt;, quoted_user_id &lt;dbl&gt;, ## # quoted_screen_name &lt;chr&gt;, quoted_name &lt;chr&gt;, quoted_location &lt;chr&gt;, ## # quoted_description &lt;chr&gt;, retweet_status_id &lt;dbl&gt;, retweet_text &lt;chr&gt;, ## # retweet_source &lt;chr&gt;, retweet_user_id &lt;dbl&gt;, retweet_screen_name &lt;chr&gt;, ## # retweet_name &lt;chr&gt;, retweet_location &lt;chr&gt;, retweet_description &lt;chr&gt;, ## # place_url &lt;lgl&gt;, place_name &lt;lgl&gt;, place_full_name &lt;lgl&gt;, place_type &lt;lgl&gt;, ## # country &lt;lgl&gt;, country_code &lt;lgl&gt;, status_url &lt;chr&gt;, name &lt;chr&gt;, ## # location &lt;lgl&gt;, description &lt;chr&gt;, url &lt;chr&gt;, profile_url &lt;chr&gt;, ## # profile_expanded_url &lt;chr&gt;, profile_banner_url &lt;chr&gt;, ## # profile_image_url &lt;chr&gt;, display_text_width &lt;dbl&gt;, favorite_count &lt;dbl&gt;, ## # retweet_count &lt;dbl&gt;, quote_count &lt;lgl&gt;, reply_count &lt;lgl&gt;, ## # quoted_favorite_count &lt;dbl&gt;, quoted_retweet_count &lt;dbl&gt;, ## # quoted_followers_count &lt;dbl&gt;, quoted_friends_count &lt;dbl&gt;, ## # quoted_statuses_count &lt;dbl&gt;, retweet_favorite_count &lt;dbl&gt;, ## # retweet_retweet_count &lt;dbl&gt;, retweet_followers_count &lt;dbl&gt;, ## # retweet_friends_count &lt;dbl&gt;, retweet_statuses_count &lt;dbl&gt;, ## # followers_count &lt;dbl&gt;, friends_count &lt;dbl&gt;, listed_count &lt;dbl&gt;, ## # statuses_count &lt;dbl&gt;, favourites_count &lt;dbl&gt; The data has quite a few variables! Let’s reduce this down to a few variables of interest: created_at, retweet_screen_name, is_retweet, and text,. tidyverse_tweets &lt;- tidyverse_tweets %&gt;% select(created_at, retweet_screen_name, is_retweet, text) tidyverse_tweets ## # A tibble: 293 x 4 ## created_at retweet_screen_n… is_retweet text ## &lt;dttm&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; ## 1 2021-04-29 11:59:04 yutannihilat_en TRUE &quot;Just curious, after the re… ## 2 2021-04-26 17:05:47 statwonk TRUE &quot;List columns provide a ton… ## 3 2021-04-24 09:13:12 topepos TRUE &quot;A new release of the {{rec… ## 4 2021-04-18 06:06:21 ninarbrooks TRUE &quot;Always typing `? pivot_lon… ## 5 2021-04-12 05:48:33 rfunctionaday TRUE &quot;If you are fluent in {dply… ## 6 2021-04-08 17:45:34 RhesusMaCassidy TRUE &quot;R-Ladies of Göttingen! The… ## 7 2021-04-01 05:01:38 dvaughan32 TRUE &quot;I am ridiculously excited … ## 8 2021-03-25 06:05:49 rdpeng TRUE &quot;New book out on using tidy… ## 9 2021-03-18 17:16:21 SolomonKurz TRUE &quot;The 0.2.0 version of my #b… ## 10 2021-03-12 19:12:49 hadleywickham TRUE &quot;rvest 1.0.0 out now! — htt… ## # … with 283 more rows If you look back up at the image of the Tidyverse twitter page, you will recognize the text of the most recent few tweets in the above dataframe. In other words, we have successfully created a small dataset using the Twitter API—neat! This data is also quite different from what we obtained from web scraping; it is already well-organized into a tidyverse dataframe (although not every API will provide data in such a nice format). From this point onward, the tidyverse_tweets dataframe is stored on your machine, and you can play with it to your heart’s content. For example, you can use write_csv to save it to a file and read_csv to read it into R again later; and after reading the next few chapters you will have the skills to compute the percentage of retweets versus tweets, find the most oft-retweeted account, make visualizations of the data, and much more! If you decide that you want to ask the Twitter API for more data (see the rtweet page for more examples of what is possible), just be mindful as usual about how much data you are requesting and how frequently you are making requests. 2.9 Additional resources The readr page on the tidyverse website is where you should look if you want to learn more about the functions in this chapter, the full set of arguments you can use, and other related functions. The site also provides a very nice cheat sheet that summarizes many of the data wrangling functions from this chapter. Sometimes you might run into data in such poor shape that none of the reading functions we cover in this chapter works. In that case, you can consult the data import chapter from R for Data Science, which goes into a lot more detail about how R parses text from files into data frames. The documentation for many of the reading functions we cover in this chapter can be found on the tidyverse website. This site shows you the full set of arguments available for each function. The rio package provides an alternative set of tools for reading and writing data in R. It aims to be a “Swiss army knife” for data reading/writing/converting, and supports a wide variety of data types (including data formats generated by other statistical software like SPSS and SAS). If you read the subsection on obtaining data from the web via scraping and APIs, we provide two companion tutorial video links: A brief video tutorial on using the SelectorGadget tool to obtain desired CSS selectors for extracting the price and size data for apartment listings on CraigsList Another brief video tutorial on using the SelectorGadget tool to obtain desired CSS selectors for extracting Canadian city names and 2016 census populations from Wikipedia "],["wrangling.html", "Chapter 3 Cleaning and wrangling data 3.1 Overview 3.2 Chapter learning objectives 3.3 Data frames and Vectors 3.4 Tidy Data 3.5 Using select to extract a range of columns 3.6 Using filter to extract rows 3.7 Using mutate to modify or add columns 3.8 Creating a visualization with tidy data 3.9 Combining functions using the pipe operator, |&gt; 3.10 Iterating over data with group_by + summarize 3.11 Using purrr’s map* functions to iterate 3.12 Additional resources", " Chapter 3 Cleaning and wrangling data 3.1 Overview This chapter is centred around defining tidy data, a data format that is suitable for analysis, and the tools needed to transform raw data into this format. This will be presented in the context of a real-world data science application, providing more practice working through a whole case study. 3.2 Chapter learning objectives By the end of the chapter, readers will be able to: define the term “tidy data” discuss the advantages and disadvantages of storing data in a tidy data format define what vector and data frames are in R, and describe how they relate to each other describe the types of data that can be stored in integer, double, character, logical, and factor vectors in R recall and use the following tidyverse functions and operators for their intended data wrangling tasks: select filter mutate |&gt; c() %in% pivot_longer pivot_wider separate summarize group_by map 3.3 Data frames and Vectors We now know how to load data into R from various file formats. Given that most of the tools we have used to do this represent the data as a data frame in R, we will spend some time learning more about data frames to deepen our understanding of them. This will be useful prerequisite knowledge to use and manipulate these objects in our data analysis. 3.3.1 What is a data frame? To define a data frame, first, let’s introduce a few commonly used terms in statistics: observation - all of the quantities or a qualities we collect from a given entity/object (e.g., adult) variable - any characteristic, number, or quantity that can be measured or collected (e.g., adult height) value - a single collected quantity or a quality from a given entity/object (e.g., the adult height of a particular person) From a data perspective, a data frame is a rectangle where the rows are the observations: Figure 3.1: Rows are observations in a data frame and the columns are the variables: Figure 3.2: Columns are variables in a data frame From a computer programming perspective, in R, a data frame is a special subtype of a list object whose elements (columns) are vectors. For example, the data frame in Figure 3.3 has three vectors whose names are region, year and population. Figure 3.3: Data frame with 3 vectors 3.3.2 What is a vector? In R, vectors are objects that can contain one or more elements. The vector elements are ordered, and they must all be of the same data type. R has several different basic data types, as shown in table 3.1. Lists, which are sometimes called recursive vectors because lists can contain other lists. In the vector shown in Figure 3.4, the elements are of integer type: Figure 3.4: Example of a numeric type vector You can create vectors in R using the concatenate c() function. To create the vector year in Figure 3.4 we write: year &lt;- c(2015, 2013, 2011, 2016, 2018) year ## [1] 2015 2013 2011 2016 2018 Table 3.1: Basic data types in R Data type Description Example character Letters or numbers surrounded by quotes “1” , “Hello world!” double Numbers with decimals values 1.2333 integer Numbers that do not contain decimals 1L, 20L (where “L” tells R to store as an integer) logical A value of true and false TRUE, FALSE factor Used to represent data with a limited number of values a size variable with levels “large,” “medium” and “small” “Numeric” is an umbrella term that includes integer and double data types. Most of the time when working with numbers in R they will be double types. For instance, a double data type is the default when you create a vector of numbers using c(), and when you read in whole numbers via read_csv. There are other basic data types in R, such as raw and complex, which we won’t cover in this textbook. We will go into more details about factors in chapter 4. 3.3.3 How are vectors different from a list? Lists are also objects in R that have multiple elements, you can think of them as super-vectors. Vectors and lists differ by the requirement of element type consistency. All elements within a single vector must be of the same type (e.g., all elements are numbers), whereas elements within a single list can be of different types (e.g., characters, numbers, logicals and even other lists can be elements in the same list). Figure 3.5: A vector versus a list Note that there are actually two types of vectors in R: atomic vectors and lists. Though when people say “vector,” they are usually referring to atomic vectors. Confusing, we know! You can think of atomic vectors as the “atoms” of R since they are built from R’s most fundamental data types. 3.3.4 What does this have to do with data frames? As mentioned earlier, a data frame is really a special type of list where the elements can only be vectors. Representing data with such an object enables us to easily work with our data in a tabular manner. It also allows us to have columns, containing qualities or values, for different statistical variables linked in a row for one observation - these columns cannot be independently sorted. This is similar to a table in a database. Figure 3.6: Data frame and vector types The functions from the tidyverse package that we are using often give us a special class of data frame called a tibble. Tibbles have some additional features and benefits over the built-in data frame object. These include the ability to add grouping (and other useful) attributes and more predictable type preservation when subsetting. Because a tibble is just a data frame with some added features, we will collectively refer to both built-in R data frames and tibbles as data frames in this book. You can use the function class on a data object to assess whether a data frame is a built-in R data frame or a tibble. If the data object is a data frame class will return \"data.frame\", whereas if the data object is a tibble it will return \"tbl_df\" \"tbl\" \"data.frame\". You can easily convert built-in R data frames to tibbles using the tidyverse as_tibble function. Vectors, data frames and lists are basic types of data structures in R which are core to most data analyses. We summarize them in table 3.2. Table 3.2: Basic data structures in R Data Structure Description vector An ordered collection of one, or more, values of the same data type (e.g. all values are numeric). Note: in R, a vector of length one is used to represent a single value. data frame A list of vectors of the same length, which has column and row names. We typically use a data frame to represent a data set. list An ordered collection of one, or more, values. The elements in the list can be of different data types. There are several other data structures in the R programming language (e.g., matrices), however, they are beyond the scope of this book. 3.4 Tidy Data There are many ways a tabular data set can be organized. This chapter will focus on introducing the tidy data format of organization and how to make your raw (and likely messy) data tidy. 3.4.1 What is tidy data? Tidy data satisfy the following three criteria (Wickham and others 2014): each row is a single observation, each column is a single variable, and each value is a single cell (i.e., its row and column position in the data frame is not shared with another value) Figure 3.7: Tidy data 3.4.2 Why is tidy data important in R? First, tidy data is a straightforward way to work with data and thus, one of R’s most popular plotting toolsets, the ggplot2 package (which is one of the packages that the tidyverse package loads), expects the data to be in a tidy format. Second, most statistical analysis functions also expect data in a tidy format. Given that both of these tasks are central in almost all data analysis projects, it is well worth spending the time to get your data into a tidy format upfront. Luckily there are many well-designed tidyverse data cleaning/wrangling tools to help you easily tidy your data. Let’s explore them below! 3.4.3 Going from wide to long (or tidy!) using pivot_longer One common step to get data into a tidy format is to combine columns stored in separate columns but are really part of the same variable. Data is often stored in a wider, not tidy, format because this format is usually more intuitive for human readability and understanding, and humans create data sets. For example, in Figure 3.8, the table on the left is an untidy format because the year values (2011, 2016) are listed as the column headers. One problem with using data in this format is that we don’t know what the values under each year represent in the table. Also, we can’t easily access the year values, so if we wanted to find the maximum year, for example, it’s hard to do when the values are not in their own column. We can reshape this data set to a long format by creating a column called “year” and a column called “population,” which is the table on the right. Figure 3.8: Going from wide to long data We can use the function pivot_longer, which combines columns, making the data frame longer and narrower. To learn how to use pivot_longer, we will work with the region_lang_top5_cities_wide.csv data set. This data set contains contains the counts of how many Canadians cited each language as their mother tongue for five major Canadian cities (Toronto, Montréal, Vancouver, Calgary and Edmonton) from the 2016 Canadian census. We will load the tidyverse packing so we can use our wrangling functions and the canlang package since it contains the region_lang and region_data data sets that we will use later in the chapter. Our data set is stored in an untidy format, as shown below: library(tidyverse) library(canlang) lang_wide &lt;- read_csv(&quot;data/region_lang_top5_cities_wide.csv&quot;) lang_wide ## # A tibble: 214 x 7 ## category language Toronto Montréal Vancouver Calgary Edmonton ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Aboriginal langua… Aboriginal la… 80 30 70 20 25 ## 2 Non-Official &amp; No… Afrikaans 985 90 1435 960 575 ## 3 Non-Official &amp; No… Afro-Asiatic … 360 240 45 45 65 ## 4 Non-Official &amp; No… Akan (Twi) 8485 1015 400 705 885 ## 5 Non-Official &amp; No… Albanian 13260 2450 1090 1365 770 ## 6 Aboriginal langua… Algonquian la… 5 5 0 0 0 ## 7 Aboriginal langua… Algonquin 5 30 5 5 0 ## 8 Non-Official &amp; No… American Sign… 470 50 265 100 180 ## 9 Non-Official &amp; No… Amharic 7460 665 1140 4075 2515 ## 10 Non-Official &amp; No… Arabic 85175 151955 14320 18965 17525 ## # … with 204 more rows What is wrong with our untidy format above? From a data analysis perspective, this format is not ideal because the values of the variable region (Toronto, Montréal, Vancouver, Calgary and Edmonton) are stored as column names. Thus they are not easily accessible to the data analysis functions we will want to apply to our data set. Additionally, the values of the mother tongue variable are spread across multiple columns, which will prevent us from doing any desired visualization or statistical tasks until we combine them into one column. For instance, suppose we want to know the languages with the highest number of Canadians reporting it as their mother tongue among all five regions. This question would be tough to answer with the data in its current format. It would be much easier to answer if we tidy our data first. To accomplish this data transformation, we will use the tidyverse function pivot_longer. Figure 3.9 summarizes how we want to reshape our data from wide to long with using the pivot_longer function. Figure 3.9: Going from wide to long with the pivot_longer function Figure 3.10 details what we need to specify to use pivot_longer. Figure 3.10: Syntax for the pivot_longer function For the above example, we use pivot_longer to combine the Toronto, Montréal, Vancouver, Calgary and Edmonton columns into a single column called region, and create a column called mother_tongue that contains the count of how many Canadians report each language as their mother tongue for each metropolitan area. We use a colon , between Toronto, and Edmonton tells R to select all the columns in between Toronto and Edmonton: lang_mother_tidy &lt;- pivot_longer(lang_wide, cols = Toronto:Edmonton, names_to = &quot;region&quot;, values_to = &quot;mother_tongue&quot; ) lang_mother_tidy ## # A tibble: 1,070 x 4 ## category language region mother_tongue ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Aboriginal languages Aboriginal languages,… Toronto 80 ## 2 Aboriginal languages Aboriginal languages,… Montréal 30 ## 3 Aboriginal languages Aboriginal languages,… Vancouv… 70 ## 4 Aboriginal languages Aboriginal languages,… Calgary 20 ## 5 Aboriginal languages Aboriginal languages,… Edmonton 25 ## 6 Non-Official &amp; Non-Aboriginal … Afrikaans Toronto 985 ## 7 Non-Official &amp; Non-Aboriginal … Afrikaans Montréal 90 ## 8 Non-Official &amp; Non-Aboriginal … Afrikaans Vancouv… 1435 ## 9 Non-Official &amp; Non-Aboriginal … Afrikaans Calgary 960 ## 10 Non-Official &amp; Non-Aboriginal … Afrikaans Edmonton 575 ## # … with 1,060 more rows Splitting code across lines: In the code above, the call to the pivot_longer function is split across several lines. This is allowed and encouraged when programming in R when your code line gets too long to read clearly. About 80 characters per line is recommended and when doing this, it is important to end the line with a comma , so that R knows the function should continue to the next line. The data above is now tidy because all three criteria for tidy data have now been met: All the variables (category, language, region and mother_tongue) are now their own columns in the data frame. Each observation, i.e., each category, language, region, and count of Canadians where that language is the mother tongue, are in a single row. Each value is a single cell, i.e., its row, column position in the data frame is not shared with another value. 3.4.4 Going from long to wide using pivot_wider Suppose we have observations spread across multiple rows rather than in a single row. For example, in Figure 3.11, the table on the left is an untidy format because the count column contains two variables (population and commuter count) and each observation is split across two rows. Using data in this format means it’s harder to apply functions when the values are not in their own column. For example, finding the maximum number of commuters is possible to do with the untidy table but requires a few more steps and is cumbersome. We can reshape this data set to a wider format by creating a column called “population” and a column called “commuters,” which is the table on the right. Figure 3.11: Going from long to wide data To tidy this type of data in R, we can use the function pivot_wider, which generally increases the number of columns (widens) and decreases the number of rows in a data set. The data set region_lang_top5_cities_long.csv contains the number of Canadians reporting the primary language at home and work for five major cities (Toronto, Montréal, Vancouver, Calgary and Edmonton). lang_long &lt;- read_csv(&quot;data/region_lang_top5_cities_long.csv&quot;) lang_long ## # A tibble: 2,140 x 5 ## region category language type count ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Montréal Aboriginal languages Aboriginal languages, n.o.s. most_at_ho… 15 ## 2 Montréal Aboriginal languages Aboriginal languages, n.o.s. most_at_wo… 0 ## 3 Toronto Aboriginal languages Aboriginal languages, n.o.s. most_at_ho… 50 ## 4 Toronto Aboriginal languages Aboriginal languages, n.o.s. most_at_wo… 0 ## 5 Calgary Aboriginal languages Aboriginal languages, n.o.s. most_at_ho… 5 ## 6 Calgary Aboriginal languages Aboriginal languages, n.o.s. most_at_wo… 0 ## 7 Edmonton Aboriginal languages Aboriginal languages, n.o.s. most_at_ho… 10 ## 8 Edmonton Aboriginal languages Aboriginal languages, n.o.s. most_at_wo… 0 ## 9 Vancouver Aboriginal languages Aboriginal languages, n.o.s. most_at_ho… 15 ## 10 Vancouver Aboriginal languages Aboriginal languages, n.o.s. most_at_wo… 0 ## # … with 2,130 more rows What is wrong with this format above? In this example, each observation should be a language in a region. However, in the messy data set above, each observation is split across multiple rows. One where the count for most_at_home is recorded and the other where the count for most_at_work is recorded. Suppose we wanted to visualize the relationship between the number of Canadians reporting their primary language at home and work. It would be difficult to do that with the data in its current format. We can see how we would like to transform the data from long to wide with pivot_wider in Figure 3.12. Figure 3.12: Going from long to wide with pivot_wider To tidy this data, we will use pivot_wider, and Figure 3.13 details what we need to specify to use this function. Figure 3.13: Syntax for the pivot_wider function We will apply the function as detailed in Figure 3.13. lang_home_tidy &lt;- pivot_wider(lang_long, names_from = type, values_from = count ) lang_home_tidy ## # A tibble: 1,070 x 5 ## region category language most_at_home most_at_work ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Montréal Aboriginal languages Aboriginal langua… 15 0 ## 2 Toronto Aboriginal languages Aboriginal langua… 50 0 ## 3 Calgary Aboriginal languages Aboriginal langua… 5 0 ## 4 Edmonton Aboriginal languages Aboriginal langua… 10 0 ## 5 Vancouv… Aboriginal languages Aboriginal langua… 15 0 ## 6 Montréal Non-Official &amp; Non-Abo… Afrikaans 10 0 ## 7 Toronto Non-Official &amp; Non-Abo… Afrikaans 265 0 ## 8 Calgary Non-Official &amp; Non-Abo… Afrikaans 505 15 ## 9 Edmonton Non-Official &amp; Non-Abo… Afrikaans 300 0 ## 10 Vancouv… Non-Official &amp; Non-Abo… Afrikaans 520 10 ## # … with 1,060 more rows The data above is now tidy! We can go through the three criteria again to check that this data is a tidy data set. All the variables are their own columns in the data frame, i.e., most_at_home, and most_at_work have been separated into their own columns in the data frame. Each observation, i.e., each category, language, region, most_at_home and most_at_work, are in a single row. Each value is a single cell, i.e., its row, column position in the data frame is not shared with another value. You might notice that we have the same number of columns in our tidy data set as we did in our messy one. Therefore pivot_wider didn’t really “widen” our data as the name suggests. However, if we had more than two categories in the original type column, then we would see the data set “widen.” 3.4.5 Using separate to deal with multiple delimiters Data are also not considered tidy when multiple values are stored in the same cell. The data set we show below is even messier than the ones we dealt with above: the Toronto, Montréal, Vancouver, Calgary and Edmonton columns contain the number of Canadians reporting their primary language at home and work in one column separated by the delimiter “/.” The column names are the values of a variable, AND each value does not have its own cell! To turn this messy data into tidy data, we’ll have to fix these issues. lang_messy &lt;- read_csv(&quot;data/region_lang_top5_cities_messy.csv&quot;) lang_messy ## # A tibble: 214 x 7 ## category language Toronto Montréal Vancouver Calgary Edmonton ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Aboriginal langu… Aboriginal la… 50/0 15/0 15/0 5/0 10/0 ## 2 Non-Official &amp; N… Afrikaans 265/0 10/0 520/10 505/15 300/0 ## 3 Non-Official &amp; N… Afro-Asiatic … 185/10 65/0 10/0 15/0 20/0 ## 4 Non-Official &amp; N… Akan (Twi) 4045/20 440/0 125/10 330/0 445/0 ## 5 Non-Official &amp; N… Albanian 6380/215 1445/20 530/10 620/25 370/10 ## 6 Aboriginal langu… Algonquian la… 5/0 0/0 0/0 0/0 0/0 ## 7 Aboriginal langu… Algonquin 0/0 10/0 0/0 0/0 0/0 ## 8 Non-Official &amp; N… American Sign… 720/245 70/0 300/140 85/25 190/85 ## 9 Non-Official &amp; N… Amharic 3820/55 315/0 540/10 2730/50 1695/35 ## 10 Non-Official &amp; N… Arabic 45025/1… 72980/1… 8680/275 11010/… 10590/3… ## # … with 204 more rows First we’ll use pivot_longer to create two columns, region and value, similar to what we did previously: lang_messy_longer &lt;- pivot_longer(lang_messy, cols = Toronto:Edmonton, names_to = &quot;region&quot;, values_to = &quot;value&quot; ) lang_messy_longer ## # A tibble: 1,070 x 4 ## category language region value ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Aboriginal languages Aboriginal languages, n.o… Toronto 50/0 ## 2 Aboriginal languages Aboriginal languages, n.o… Montréal 15/0 ## 3 Aboriginal languages Aboriginal languages, n.o… Vancouv… 15/0 ## 4 Aboriginal languages Aboriginal languages, n.o… Calgary 5/0 ## 5 Aboriginal languages Aboriginal languages, n.o… Edmonton 10/0 ## 6 Non-Official &amp; Non-Aboriginal lang… Afrikaans Toronto 265/0 ## 7 Non-Official &amp; Non-Aboriginal lang… Afrikaans Montréal 10/0 ## 8 Non-Official &amp; Non-Aboriginal lang… Afrikaans Vancouv… 520/… ## 9 Non-Official &amp; Non-Aboriginal lang… Afrikaans Calgary 505/… ## 10 Non-Official &amp; Non-Aboriginal lang… Afrikaans Edmonton 300/0 ## # … with 1,060 more rows Then we’ll use separate to split the value column into two columns, one that contains only the counts of Canadians that speak each language most at home, and one that contains the counts for most at work for each region. To use separate we need to specify the: data: the data set col: the name of a the column we need to split into: a character vector of the new column names we would like to put the split data into sep: the separator on which to split lang_no_delimiter &lt;- separate(lang_messy_longer, col = value, into = c(&quot;most_at_home&quot;, &quot;most_at_work&quot;), sep = &quot;/&quot; ) lang_no_delimiter ## # A tibble: 1,070 x 5 ## category language region most_at_home most_at_work ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Aboriginal languages Aboriginal langua… Toronto 50 0 ## 2 Aboriginal languages Aboriginal langua… Montré… 15 0 ## 3 Aboriginal languages Aboriginal langua… Vancou… 15 0 ## 4 Aboriginal languages Aboriginal langua… Calgary 5 0 ## 5 Aboriginal languages Aboriginal langua… Edmont… 10 0 ## 6 Non-Official &amp; Non-Abor… Afrikaans Toronto 265 0 ## 7 Non-Official &amp; Non-Abor… Afrikaans Montré… 10 0 ## 8 Non-Official &amp; Non-Abor… Afrikaans Vancou… 520 10 ## 9 Non-Official &amp; Non-Abor… Afrikaans Calgary 505 15 ## 10 Non-Official &amp; Non-Abor… Afrikaans Edmont… 300 0 ## # … with 1,060 more rows Is this data set now tidy? If we recall the three criteria for tidy data: each row is a single observation, each column is a single variable, and each value is a single cell. We can see that this data now satisfies all three criteria, making it easier to analyze. However, we aren’t done yet! We have a few more tasks we need to do to get this data in a more usable format, which we will see in the sections below. 3.4.6 Notes on defining tidy data Is there only one shape for tidy data for a given data set? Not necessarily! It depends on the statistical question you are asking and what the variables are for that question. For tidy data, each variable should be its own column. So, just as it’s essential to match your statistical question with the appropriate data analysis tool (classification, clustering, hypothesis testing, etc.). It’s important to match your statistical question with the appropriate variables and ensure they are represented as individual columns to make the data tidy. 3.5 Using select to extract a range of columns Chapter 1 discussed two tidyverse functions: select and filter to select columns and filter rows from a data frame. We will now learn more ways we can use these functions. Recall, the select function creates a subset of the columns of a data frame, while the filter subsets rows with specific values. We can use select to obtain a subset of the data frame constructed from a range of columns rather than typing each column name out. To do this, we use the colon (:) operator to denote the range. For example, to get all the columns in the lang_no_delimiter data frame from language to most_at_work we pass language:most_at_work as the second argument to the select function. column_range &lt;- select(lang_no_delimiter, language:most_at_work) column_range ## # A tibble: 1,070 x 4 ## language region most_at_home most_at_work ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Aboriginal languages, n.o.s. Toronto 50 0 ## 2 Aboriginal languages, n.o.s. Montréal 15 0 ## 3 Aboriginal languages, n.o.s. Vancouver 15 0 ## 4 Aboriginal languages, n.o.s. Calgary 5 0 ## 5 Aboriginal languages, n.o.s. Edmonton 10 0 ## 6 Afrikaans Toronto 265 0 ## 7 Afrikaans Montréal 10 0 ## 8 Afrikaans Vancouver 520 10 ## 9 Afrikaans Calgary 505 15 ## 10 Afrikaans Edmonton 300 0 ## # … with 1,060 more rows 3.6 Using filter to extract rows 3.6.1 Using filter to extract rows with == Recall we can use the filter function to obtain the subset of rows with desired values from a data frame. Again, our first argument is the name of the data frame object, lang_no_delimiter. The second argument is a logical statement to use when filtering the rows. Here, for example, we’ll say that we are interested in rows where the category is “Official languages.” We use the equivalency operator (==) to compare the values of the category column with the value \"Official languages\". With these arguments, filter returns a data frame with all the columns of the input data frame but only the rows we asked for in our logical filter statement. official_langs &lt;- filter(lang_no_delimiter, category == &quot;Official languages&quot;) official_langs ## # A tibble: 10 x 5 ## category language region most_at_home most_at_work ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Official languages English Toronto 3836770 3218725 ## 2 Official languages English Montréal 620510 412120 ## 3 Official languages English Vancouver 1622735 1330555 ## 4 Official languages English Calgary 1065070 844740 ## 5 Official languages English Edmonton 1050410 792700 ## 6 Official languages French Toronto 29800 11940 ## 7 Official languages French Montréal 2669195 1607550 ## 8 Official languages French Vancouver 8630 3245 ## 9 Official languages French Calgary 8630 2140 ## 10 Official languages French Edmonton 10950 2520 3.6.2 Using filter to extract rows with != What if we want all the other language categories in the data set except for those in the “Official languages” category? We can accomplish this with the != logical operator, which means “NOT EQUAL TO.” So if we want to find all the rows where the category does NOT equal “Official languages” we write the code below. filter(lang_no_delimiter, category != &quot;Official languages&quot;) ## # A tibble: 1,060 x 5 ## category language region most_at_home most_at_work ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Aboriginal languages Aboriginal langua… Toronto 50 0 ## 2 Aboriginal languages Aboriginal langua… Montré… 15 0 ## 3 Aboriginal languages Aboriginal langua… Vancou… 15 0 ## 4 Aboriginal languages Aboriginal langua… Calgary 5 0 ## 5 Aboriginal languages Aboriginal langua… Edmont… 10 0 ## 6 Non-Official &amp; Non-Abor… Afrikaans Toronto 265 0 ## 7 Non-Official &amp; Non-Abor… Afrikaans Montré… 10 0 ## 8 Non-Official &amp; Non-Abor… Afrikaans Vancou… 520 10 ## 9 Non-Official &amp; Non-Abor… Afrikaans Calgary 505 15 ## 10 Non-Official &amp; Non-Abor… Afrikaans Edmont… 300 0 ## # … with 1,050 more rows 3.6.3 Using filter to extract rows with , &amp;` Let’s suppose we want to look at only the rows for English in Vancouver. Here we want to filter the data set to find rows that satisfy multiple conditions simultaneously. We can do this with &amp;, which is a logical operator meaning “AND.” We are filtering the official_langs data frame where the region equals “Vancouver” AND the language equals “English.” filter(official_langs, region == &quot;Vancouver&quot; &amp; language == &quot;English&quot;) ## # A tibble: 1 x 5 ## category language region most_at_home most_at_work ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Official languages English Vancouver 1622735 1330555 Notice that we can accomplish the same thing using a comma , instead of the ampersand sign &amp;. The filter function treats the comma , between the logical statements as an “AND.” filter(official_langs, region == &quot;Vancouver&quot; , language == &quot;English&quot;) ## # A tibble: 1 x 5 ## category language region most_at_home most_at_work ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Official languages English Vancouver 1622735 1330555 3.6.4 Using filter to extract rows with | Suppose we were interested in the rows for only the Albertan cities in our data set (Edmonton and Calgary). We can’t use &amp; as we did above because region cannot be both Edmonton and Calgary simultaneously. Instead, we can use the | logical operator, which gives us the cases where one condition OR another condition is satisfied OR both. In the code below, we ask R to return the rows where the region columns are equal to “Calgary” OR “Edmonton.” filter(official_langs, region == &quot;Calgary&quot; | region == &quot;Edmonton&quot;) ## # A tibble: 4 x 5 ## category language region most_at_home most_at_work ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Official languages English Calgary 1065070 844740 ## 2 Official languages English Edmonton 1050410 792700 ## 3 Official languages French Calgary 8630 2140 ## 4 Official languages French Edmonton 10950 2520 3.6.5 Using filter to extract rows with %in% Suppose we want to see the populations of our five cities. The region_data data set from the canlang package contains statistics for number of households, land area, population and number of dwellings for different regions according to the 2016 Canadian census. region_data ## # A tibble: 35 x 5 ## region households area population dwellings ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Belleville 43002 1355. 103472 45050 ## 2 Lethbridge 45696 3047. 117394 48317 ## 3 Thunder Bay 52545 2618. 121621 57146 ## 4 Peterborough 50533 1637. 121721 55662 ## 5 Saint John 52872 3793. 126202 58398 ## 6 Brantford 52530 1086. 134203 54419 ## 7 Moncton 61769 2625. 144810 66699 ## 8 Guelph 59280 604. 151984 63324 ## 9 Trois-Rivières 72502 1053. 156042 77734 ## 10 Saguenay 72479 3079. 160980 77968 ## # … with 25 more rows To get the population of our five cities we can filter the data set using the %in% operator. The %in% operator is used to see if an element belongs to a vector. Here we are filtering if any of the five cities (“Toronto,” “Montréal,” “Vancouver,” “Calgary,” “Edmonton”) match the rows in the region column. filter(region_data, region %in% c(&quot;Toronto&quot;, &quot;Montréal&quot;, &quot;Vancouver&quot;, &quot;Calgary&quot;, &quot;Edmonton&quot;)) ## # A tibble: 5 x 5 ## region households area population dwellings ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Edmonton 502143 9858. 1321426 537634 ## 2 Calgary 519693 5242. 1392609 544870 ## 3 Vancouver 960894 3040. 2463431 1027613 ## 4 Montréal 1727310 4638. 4098927 1823281 ## 5 Toronto 2135909 6270. 5928040 2235145 What’s the difference between == and %in%? Let’s suppose we have two vectors, vector1 and vector2. If you type vector1 == vector2 into R it will compare the vectors element by element. R checks if the first element of vector1 equals the first element of vector2, the second element of vector1 equals the second element of vector2 etc. On the other hand, vector1 %in% vector2 compares the first element of vector1 to all the elements in vector2. Then the second element of vector1 is then compared to all the elements in vector2 etc. Notice the difference between == and %in% in the example below. c(&quot;Vancouver&quot;, &quot;Toronto&quot;) == c(&quot;Toronto&quot;, &quot;Vancouver&quot;) ## [1] FALSE FALSE c(&quot;Vancouver&quot;, &quot;Toronto&quot;) %in% c(&quot;Toronto&quot;, &quot;Vancouver&quot;) ## [1] TRUE TRUE 3.6.6 Using filter to extract rows with values above or below a threshold We see that 2,669,195 people reported speaking French in Montréal as their primary language at home. If we are interested in finding the languages in regions with higher numbers of people who speak it as their primary language at home than French in Montréal then we can create a filter to obtain rows where the value of most_at_home is greater than 2,669,195. filter(official_langs, most_at_home &gt; 2669195) ## # A tibble: 5 x 5 ## category language region most_at_home most_at_work ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Official languages English Toronto 3836770 3218725 ## 2 Official languages English Montréal 620510 412120 ## 3 Official languages French Toronto 29800 11940 ## 4 Official languages French Vancouver 8630 3245 ## 5 Official languages French Calgary 8630 2140 When we look at the table we got from filter above, it isn’t quite doing what we’d expect. We see numbers in the table in the most_at_home column that are less than 2,669,195. What’s going on here? 3.7 Using mutate to modify or add columns 3.7.1 Using mutate to modify columns Notice in the table above the word &lt;chr&gt; appears beneath each of the column names. The word under the column name indicates the data type of each column. Here all of our variables are “character” data types. Recall, a character data type is a letter or a number surrounded by quotes. In the previous example in section 3.4.4, the most_at_home and most_at_work variables were &lt;dbl&gt; (double) (you can verify this by looking at the tables in the previous sections), which is a numeric data type. This change is due to the delimiter “/” when we read in this messy data set. R read the columns in as character types, and it stayed that way after we separated the columns. Here it makes sense for region, category, and language to be stored as a character type. However, if we want to apply any functions that treat the most_at_home and most_at_work columns as a number (e.g. finding rows that are above a numeric threshold of a column), it won’t be possible to do if the variable is stored as a character. R has a variety of data types, but here we will use the function mutate to convert these two columns to a “numeric” data type. mutate is a function that will allow us to create a new variable or transform existing ones in our data set. We specify the data set in the first argument, and in the proceeding arguments, we specify the function we want to apply (as.numeric) to which columns (most_at_home, most_at_work). Then we give the mutated variable a new name. Here we are naming the columns the same names (“most_at_home,” “most_at_work”), but you can call these mutated variables anything you’d like. mutate’s general syntax is detailed in Figure 3.14. Figure 3.14: Syntax for the mutate function Below we use mutate to convert the columns most_at_home and most_at_home to numeric data types in the official_langs data set as described in figure 3.14: official_langs &lt;- mutate(official_langs, most_at_home = as.numeric(most_at_home), most_at_work = as.numeric(most_at_work) ) official_langs ## # A tibble: 10 x 5 ## category language region most_at_home most_at_work ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Official languages English Toronto 3836770 3218725 ## 2 Official languages English Montréal 620510 412120 ## 3 Official languages English Vancouver 1622735 1330555 ## 4 Official languages English Calgary 1065070 844740 ## 5 Official languages English Edmonton 1050410 792700 ## 6 Official languages French Toronto 29800 11940 ## 7 Official languages French Montréal 2669195 1607550 ## 8 Official languages French Vancouver 8630 3245 ## 9 Official languages French Calgary 8630 2140 ## 10 Official languages French Edmonton 10950 2520 Now we see &lt;dbl&gt; appears under our columns, most_at_home and most_at_work, indicating they are double data types (which is one of the sub-types of numeric)! Now when we filter we get what we expect: filter(official_langs, most_at_home &gt; 2669195) ## # A tibble: 1 x 5 ## category language region most_at_home most_at_work ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Official languages English Toronto 3836770 3218725 In this case, we see that filter returns a data frame with only one row; this indicates that only English in Toronto is reported by more people as their primary language at home than French in Montréal. 3.7.2 Using mutate to create new columns We can see in the table that 3,836,770 people reported speaking English in Toronto as their primary language at home according to the 2016 Canadian census. What does this number mean to us? To understand this number, we need context. In particular, how many people were in Toronto when this data was collected? From the 2016 Canadian census profile, the population of Toronto was reported to be 5,928,040 people. The number of people who report that English as their primary language at home is much more meaningful when we report it in this context. We can even go a step further and transform this count to a relative frequency or proportion. We can do this by dividing the number of people reporting a given language as their primary language at home by the number of people who live in Toronto. For example, the proportion of people who reported that their primary language at home was English in the 2016 Canadian census was 0.65 in Toronto. We can create a vector of the city populations. Then we use mutate to calculate the proportion of people reporting a given language as their primary language at home for all the languages in the official_langs data set by dividing the column by our vector of city populations. We give the mutated column a new name most_at_home_proportion, which will create a new column in our data set. city_populations &lt;- c(5928040, 4098927, 2463431, 1392609, 1321426) official_langs &lt;- mutate(official_langs, most_at_home_proportion = most_at_home / city_populations ) official_langs ## # A tibble: 10 x 6 ## category language region most_at_home most_at_work most_at_home_propo… ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Official lan… English Toronto 3836770 3218725 0.647 ## 2 Official lan… English Montréal 620510 412120 0.151 ## 3 Official lan… English Vancouv… 1622735 1330555 0.659 ## 4 Official lan… English Calgary 1065070 844740 0.765 ## 5 Official lan… English Edmonton 1050410 792700 0.795 ## 6 Official lan… French Toronto 29800 11940 0.00503 ## 7 Official lan… French Montréal 2669195 1607550 0.651 ## 8 Official lan… French Vancouv… 8630 3245 0.00350 ## 9 Official lan… French Calgary 8630 2140 0.00620 ## 10 Official lan… French Edmonton 10950 2520 0.00829 Notice that we divided a vector with 10 elements (most_at_home) by a vector with only 5 elements (city_populations). What gives? Arithmetic operations of vectors are performed memberwise meaning the first element of most_at_home will be divided by the first element of city_populations, the second element of most_at_home will be divided by the second element of city_populations and so on. When two vectors have different lengths, the shorter one will be recycled to match the longer one. Here the elements of city_populations will be repeated once to match the length of most_at_home. You can do this in R, however we can very easily introduce errors using this method without realizing. For example, suppose we only listed four city populations, but still had ten elements of five cities in the most_at_home column. Then the first two city populations listed in the city_populations vector (Toronto and Montréal) would be repeated at the end and our calculated proportions would be wrong in our final data table. Instead, let’s use the case_when function. When used inside the mutate function, case_when will allow us to mutate a variable when a particular condition is met. We write the condition on the left side of the tilde (~) symbol and the mutation on the right side. Let’s consider just Toronto to start. We want to take the most_at_home value for English in Toronto (3,836,770) and divide it by Toronto’s population (5,928,040). We also want to divide the most_at_home value for French in Toronto (29,800) and divide it by Toronto’s population. So when region == Toronto (the condition), we want to apply most_at_home/5928040 (the mutation). We do the same for the other cities, which is what you see in the code below. mutate(official_langs, most_at_home_proportion = case_when( region == &quot;Toronto&quot; ~ most_at_home / 5928040, region == &quot;Montréal&quot; ~ most_at_home / 4098927, region == &quot;Vancouver&quot; ~ most_at_home / 2463431, region == &quot;Calgary&quot; ~ most_at_home / 1392609, region == &quot;Edmonton&quot; ~ most_at_home / 1321426 ) ) ## # A tibble: 10 x 6 ## category language region most_at_home most_at_work most_at_home_propo… ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Official lan… English Toronto 3836770 3218725 0.647 ## 2 Official lan… English Montréal 620510 412120 0.151 ## 3 Official lan… English Vancouv… 1622735 1330555 0.659 ## 4 Official lan… English Calgary 1065070 844740 0.765 ## 5 Official lan… English Edmonton 1050410 792700 0.795 ## 6 Official lan… French Toronto 29800 11940 0.00503 ## 7 Official lan… French Montréal 2669195 1607550 0.651 ## 8 Official lan… French Vancouv… 8630 3245 0.00350 ## 9 Official lan… French Calgary 8630 2140 0.00620 ## 10 Official lan… French Edmonton 10950 2520 0.00829 If you compare the proportions in these two tables, we get the same values with both methods. That is the beauty (and sometimes downfall) of R – there are usually multiple ways to accomplish the same thing! 3.8 Creating a visualization with tidy data Now that we have cleaned and wrangled our data, we can visualize how many people speak each of Canada’s two official languages (English and French) as their primary language at home in these 5 regions. We can use ggplot to create our data visualization. Here we create a bar chart to represent the proportions for each region, and colour the proportions by language. ggplot( official_langs, aes( x = most_at_home_proportion, y = region, fill = language ) ) + geom_bar(stat = &quot;identity&quot;) + xlab(&quot;Proportion of residents reporting their primary language at home&quot;) + ylab(&quot;Region&quot;) Don’t worry too much about the code to make this plot, we will talk more in detail about visualizations in chapter 4. From this visualization, we can see that in Calgary, Edmonton, Toronto and Vancouver, English was reported as the most common primary language used at home compared to French. However, in Montréal, French was reported as the most common primary language used at home over English. 3.9 Combining functions using the pipe operator, |&gt; In R, we often have to call multiple functions in a sequence to process a data frame. The basic ways of doing this can become quickly unreadable if there are many steps. For example, suppose we need to perform three operations on a data frame data: add a new column new_col that is double another old_col filter for rows where another column, other_col, is more than 5, and select only the new column new_col for those rows. One way of doing is to just write multiple lines of code, storing temporary objects as you go: output_1 &lt;- mutate(data, new_col = old_col * 2) output_2 &lt;- filter(output_1, other_col &gt; 5) output &lt;- select(output_2, new_col) This is difficult to understand for multiple reasons. The reader may be tricked into thinking the named output_1 and output_2 objects are important for some reason, while they are just temporary intermediate computations. Further, the reader has to look through and find where output_1 and output_2 are used in each subsequent line. Another option for doing this would be to compose the functions: output &lt;- select(filter(mutate(data, new_col = old_col * 2), other_col &gt; 5), new_col) Code like this can also be difficult to understand. Functions compose (reading from left to right) in the opposite order in which they are computed by R (above, mutate happens first, then filter, then select). It is also just a really long line of code to read in one go. The pipe operator |&gt; solves this problem, resulting in cleaner and easier-to-follow code. |&gt; in built into R so you don’t need to load any packages to use it. The code below accomplishes the same thing as the previous two code blocks: output &lt;- data |&gt; mutate(new_col = old_col * 2) |&gt; filter(other_col &gt; 5) |&gt; select(new_col) Note: The |&gt; pipe operator was inspired by a previous version of the pipe operator, %&gt;%. The %&gt;% pipe operator is not built into R and needs to be loaded via an external R package. There are some other drawbacks to using %&gt;%, which are beyond the scope of this textbook. However, be aware that %&gt;% exists since you may see it used in some books or other sources. In this textbook, we will be using the base R pipe operator syntax, |&gt;. You can think of the pipe as a physical pipe. It takes the output from the function on the left-hand side of the pipe, and passes it as the first argument to the function on the right-hand side of the pipe. Note here that we have again split the code across multiple lines for readability; R is fine with this, since it knows that a line ending in a pipe |&gt; is continued on the next line. Similarly, you see that after the first pipe, the remaining lines are indented until the end of the pipeline. This is not required for the R code to work, but again is used to aid in improving code readability. Next, let’s learn about the details of using the pipe, and look at some examples of how to use it in data analysis. 3.9.1 Using |&gt; to combine filter and select Let’s work with our tidy lang_home_tidy data set from above, which contains the number of Canadians reporting their primary language at home and work for five major cities (Toronto, Montréal, Vancouver, Calgary and Edmonton): lang_home_tidy ## # A tibble: 1,070 x 5 ## region category language most_at_home most_at_work ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Montréal Aboriginal languages Aboriginal langua… 15 0 ## 2 Toronto Aboriginal languages Aboriginal langua… 50 0 ## 3 Calgary Aboriginal languages Aboriginal langua… 5 0 ## 4 Edmonton Aboriginal languages Aboriginal langua… 10 0 ## 5 Vancouv… Aboriginal languages Aboriginal langua… 15 0 ## 6 Montréal Non-Official &amp; Non-Abo… Afrikaans 10 0 ## 7 Toronto Non-Official &amp; Non-Abo… Afrikaans 265 0 ## 8 Calgary Non-Official &amp; Non-Abo… Afrikaans 505 15 ## 9 Edmonton Non-Official &amp; Non-Abo… Afrikaans 300 0 ## 10 Vancouv… Non-Official &amp; Non-Abo… Afrikaans 520 10 ## # … with 1,060 more rows Suppose we want to create a subset of the data with only the languages and counts of each language spoken most at home for the city of Vancouver. To do this, we can use the functions filter and select. First, we use filter to create a data frame called van_data that contains only values for Vancouver. We then use select on this data frame to keep only the variables we want: van_data &lt;- filter(lang_home_tidy, region == &quot;Vancouver&quot;) van_data ## # A tibble: 214 x 5 ## region category language most_at_home most_at_work ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Vancouv… Aboriginal languages Aboriginal langua… 15 0 ## 2 Vancouv… Non-Official &amp; Non-Abo… Afrikaans 520 10 ## 3 Vancouv… Non-Official &amp; Non-Abo… Afro-Asiatic lang… 10 0 ## 4 Vancouv… Non-Official &amp; Non-Abo… Akan (Twi) 125 10 ## 5 Vancouv… Non-Official &amp; Non-Abo… Albanian 530 10 ## 6 Vancouv… Aboriginal languages Algonquian langua… 0 0 ## 7 Vancouv… Aboriginal languages Algonquin 0 0 ## 8 Vancouv… Non-Official &amp; Non-Abo… American Sign Lan… 300 140 ## 9 Vancouv… Non-Official &amp; Non-Abo… Amharic 540 10 ## 10 Vancouv… Non-Official &amp; Non-Abo… Arabic 8680 275 ## # … with 204 more rows van_data_selected &lt;- select(van_data, language, most_at_home) van_data_selected ## # A tibble: 214 x 2 ## language most_at_home ## &lt;chr&gt; &lt;dbl&gt; ## 1 Aboriginal languages, n.o.s. 15 ## 2 Afrikaans 520 ## 3 Afro-Asiatic languages, n.i.e. 10 ## 4 Akan (Twi) 125 ## 5 Albanian 530 ## 6 Algonquian languages, n.i.e. 0 ## 7 Algonquin 0 ## 8 American Sign Language 300 ## 9 Amharic 540 ## 10 Arabic 8680 ## # … with 204 more rows Although this is valid code, there is a more readable approach we could take by using the pipe, |&gt;. With the pipe, we do not need to create an intermediate object to store the output from filter. Instead we can directly send the output of filter to the input of select: van_data_selected &lt;- filter(lang_home_tidy, region == &quot;Vancouver&quot;) |&gt; select(language, most_at_home) van_data_selected ## # A tibble: 214 x 2 ## language most_at_home ## &lt;chr&gt; &lt;dbl&gt; ## 1 Aboriginal languages, n.o.s. 15 ## 2 Afrikaans 520 ## 3 Afro-Asiatic languages, n.i.e. 10 ## 4 Akan (Twi) 125 ## 5 Albanian 530 ## 6 Algonquian languages, n.i.e. 0 ## 7 Algonquin 0 ## 8 American Sign Language 300 ## 9 Amharic 540 ## 10 Arabic 8680 ## # … with 204 more rows But wait - why does our select function call look different in these two examples? When you use the pipe, the output of the function on the left is automatically provided as the first argument for the function on the right, and thus you do not specify that argument in that function call. In the code above, the first argument of select is the data frame we are select-ing from, which is provided by the output of filter. As you can see, both of these approaches give us the same output, but the second approach is more clear and readable. 3.9.2 Using |&gt; with more than two functions The |&gt; can be used with any function in R. Additionally, we can pipe together more than two functions. For example, we can pipe together three functions to order the rows by counts of the language most spoken at home for only the counts that are more than 10,000 and only include the region, language and count of Canadians reporting their primary language at home in our table. To order by counts of the language most spoken at home we will use the tidyverse function, arrange. Recall this function takes column names as input and orders the rows in the data frame in ascending order based on the values in the columns. Here we use only one column for sorting (most_at_home), but more than one can also be used. To do this, list additional columns separated by commas. The order they are listed in indicates the order in which they will be used for sorting. This is much like how an English dictionary sorts words: first by the first letter, then by the second letter, and so on. Remember: If you want to sort in reverse order, you can pair a function called desc with arrange (e.g., arrange(desc(column_name))). large_region_lang &lt;- filter(lang_home_tidy, most_at_home &gt; 10000) |&gt; select(region, language, most_at_home) |&gt; arrange(most_at_home) large_region_lang ## # A tibble: 67 x 3 ## region language most_at_home ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Edmonton Arabic 10590 ## 2 Montréal Tamil 10670 ## 3 Vancouver Russian 10795 ## 4 Edmonton Spanish 10880 ## 5 Edmonton French 10950 ## 6 Calgary Arabic 11010 ## 7 Calgary Urdu 11060 ## 8 Vancouver Hindi 11235 ## 9 Montréal Armenian 11835 ## 10 Toronto Romanian 12200 ## # … with 57 more rows Note: You might also have noticed that we split the function calls across lines after the pipe, similar as to when we did this earlier in the chapter for long function calls. Again this is allowed and recommeded, especially when the piped function calls would create a long line of code. Doing this makes your code more readable. When you do this it is important to end each line with the pipe operator |&gt; to tell R that your code is continuing onto the next line. 3.10 Iterating over data with group_by + summarize 3.10.1 Calculating summary statistics: As a part of many data analyses, we need to calculate a summary value for the data (a summary statistic). A useful dplyr function for doing this is summarize. Examples of summary statistics we might want to calculate are the number of observations, the average/mean value for a column, the minimum value for a column, etc. Below we show how to use the summarize function to calculate the minimum and maximum number of Canadians reporting a particular language as their primary language at home: lang_summary &lt;- summarize(lang_home_tidy, min_most_at_home = min(most_at_home), most_most_at_home = max(most_at_home) ) lang_summary ## # A tibble: 1 x 2 ## min_most_at_home most_most_at_home ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 3836770 From this we see that there are some languages in the data set the no one speaks as their primary language at home, as well as that the most commonly spoken primary language at home is spoken by 3,836,770 people. 3.10.2 Calculating group summary statistics: A common pairing with summarize is group_by. Pairing these functions together can let you summarize values for subgroups within a data set. For example, here, we can use group_by to group the regions and then calculate the minimum and maximum number of Canadians reporting the language as the primary language at home for each of the groups. The group_by function takes at least two arguments. The first is the data frame that will be grouped, and the second and onwards are columns to use in the grouping. Here we use only one column for grouping (region), but more than one can also be used. To do this, list additional columns separated by commas. lang_summary_by_region &lt;- group_by(lang_home_tidy, region) |&gt; summarize( min_most_at_home = min(most_at_home), max_most_at_home = max(most_at_home) ) lang_summary_by_region ## # A tibble: 5 x 3 ## region min_most_at_home max_most_at_home ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Calgary 0 1065070 ## 2 Edmonton 0 1050410 ## 3 Montréal 0 2669195 ## 4 Toronto 0 3836770 ## 5 Vancouver 0 1622735 3.10.3 Additional reading on the dplyr functions As we briefly mentioned earlier in a note, the tidyverse is actually a meta R package: it installs and loads a collection of R packages that all follow the tidy data philosophy we discussed above. One of the tidyverse packages is dplyr - a data wrangling workhorse. You have already met six of the dplyr function (select, filter, mutate, arrange, summarize, and group_by). To learn more about those six and meet a few more useful functions, we recommend you checkout this chapter of the Stat 545 book. 3.11 Using purrr’s map* functions to iterate Where should you turn when you discover the next step in your data wrangling/cleaning process requires you to apply a function to each column in a data frame? For example, if you wanted to know the maximum value of each column in a data frame? Well, you could use summarize as discussed above. However, this becomes inconvenient when you have many columns, as summarize requires you to type out a column name and a data transformation for each summary statistic you want to calculate. In cases like this, where you want to apply the same data transformation to all columns, it is more efficient to use purrr’s map function to apply it to each column. For example, let’s find the maximum value of each column of the region_lang data frame by using map with the max function. First, let’s peak at the data to familiarize ourselves with it: region_lang &lt;- read_csv(&quot;data/region_lang.csv&quot;) region_lang ## # A tibble: 7,490 x 7 ## region category language mother_tongue most_at_home most_at_work lang_known ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 St. Jo… Aborigi… Aborigin… 5 0 0 0 ## 2 Halifax Aborigi… Aborigin… 5 0 0 0 ## 3 Moncton Aborigi… Aborigin… 0 0 0 0 ## 4 Saint … Aborigi… Aborigin… 0 0 0 0 ## 5 Saguen… Aborigi… Aborigin… 5 5 0 0 ## 6 Québec Aborigi… Aborigin… 0 5 0 20 ## 7 Sherbr… Aborigi… Aborigin… 0 0 0 0 ## 8 Trois-… Aborigi… Aborigin… 0 0 0 0 ## 9 Montré… Aborigi… Aborigin… 30 15 0 10 ## 10 Kingst… Aborigi… Aborigin… 0 0 0 0 ## # … with 7,480 more rows Next, we will select only the numeric columns of the data frame: region_lang_numeric &lt;- region_lang |&gt; select(mother_tongue:lang_known) region_lang_numeric ## # A tibble: 7,490 x 4 ## mother_tongue most_at_home most_at_work lang_known ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5 0 0 0 ## 2 5 0 0 0 ## 3 0 0 0 0 ## 4 0 0 0 0 ## 5 5 5 0 0 ## 6 0 5 0 20 ## 7 0 0 0 0 ## 8 0 0 0 0 ## 9 30 15 0 10 ## 10 0 0 0 0 ## # … with 7,480 more rows Next, we use map to apply the max function to each column. map takes two arguments, an object (a vector, data frame or list) that you want to apply the function to, and the function that you would like to apply. Here our arguments will be region_lang_numeric and max: max_of_columns &lt;- map(region_lang_numeric, max) max_of_columns ## $mother_tongue ## [1] 3061820 ## ## $most_at_home ## [1] 3836770 ## ## $most_at_work ## [1] 3218725 ## ## $lang_known ## [1] 5600480 Note: purrr is part of the tidyverse, and so like the dplyr and ggplot functions, once we call library(tidyverse) we do not need to load the purrr package separately. Our output looks a bit weird… we passed in a data frame, but our output doesn’t look like a data frame. As it so happens, it is not a data frame, but rather a plain vanilla list: typeof(max_of_columns) ## [1] &quot;list&quot; So what do we do? Should we convert this to a data frame? We could, but a simpler alternative is to just use a different map_* function from the purrr package. There are quite a few to choose from, they all work similarly, and their name reflects the type of output you want from the mapping operation: map function Output map() list map_lgl() logical vector map_int() integer vector map_dbl() double vector map_chr() character vector map_df() data frame Let’s get the columns’ maximums again, but this time use the map_df function to return the output as a data frame: max_of_columns &lt;- map_df(region_lang_numeric, max) max_of_columns ## # A tibble: 1 x 4 ## mother_tongue most_at_home most_at_work lang_known ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3061820 3836770 3218725 5600480 Which map_* function you choose depends on what you want to do with the output; you don’t always have to pick map_df! What if you need to add other arguments to the functions you want to map? For example, what if there were NA values in our columns that we wanted to know the maximum of? region_with_nas &lt;- read_csv(&quot;data/region_lang_with_nas.csv&quot;) region_with_nas ## # A tibble: 7,491 x 4 ## mother_tongue most_at_home most_at_work lang_known ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5 5 NA NA ## 2 5 0 0 0 ## 3 5 0 0 0 ## 4 0 0 0 0 ## 5 0 0 0 0 ## 6 5 5 0 0 ## 7 0 0 0 0 ## 8 0 0 0 0 ## 9 0 0 0 0 ## 10 0 0 0 0 ## # … with 7,481 more rows map_df(region_with_nas, max) ## # A tibble: 1 x 4 ## mother_tongue most_at_home most_at_work lang_known ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3061820 3836770 NA NA Notice map_df() returns NA for the most_at_work and lang_known variables since those columns contained NAs in the data frame. Thus, we also need to add the argument na.rm = TRUE to the max function so that we get a more useful value than NA returned (remember that is what happens with many of the built-in R statistical functions when NA’s are present…). What we need to do in that case is add these additional arguments to the end of our call to to map and they will be passed to the function that we are mapping. An example of this is shown below: map_df(region_with_nas, max, na.rm = TRUE) ## # A tibble: 1 x 4 ## mother_tongue most_at_home most_at_work lang_known ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3061820 3836770 3218725 5600480 Now map_df() returns the maximum count for each column ignoring the NAs in the data set! The map_* functions are generally quite useful for solving problems involving iteration/repetition. Additionally, their use is not limited to columns of a data frame; map_* functions can be used to apply functions to elements of a vector or list, and even to lists of data frames, or nested data frames. 3.12 Additional resources The dplyr page on the tidyverse website is where you should look if you want to learn more about the functions in this chapter, the full set of arguments you can use, and other related functions. The site also provides a very nice cheat sheet that summarizes many of the data wrangling functions from this chapter. R for Data Science has a few chapters related to data wrangling that go into more depth than this book. For example, the tidy data chapter covers tidy data, pivot_longer/pivot_wider and separate, but also covers missing values and additional wrangling functions (like unite). The data transformation chapter covers select, filter, arrange, mutate, and summarize. And the map_* functions chapter provides more about the map_* functions. You will occasionally encounter a case where you need to iterate over items in a data frame, but none of the above functions are flexible enough to do what you want. In that case, you may consider using a for loop. References "],["viz.html", "Chapter 4 Effective data visualization 4.1 Overview 4.2 Chapter learning objectives 4.3 Choosing the visualization 4.4 Refining the visualization 4.5 Creating visualizations with ggplot2 4.6 Explaining the visualization 4.7 Saving the visualization 4.8 Additional resources", " Chapter 4 Effective data visualization 4.1 Overview This chapter will introduce concepts and tools relating to data visualization beyond what we have seen and practiced so far. We will focus on guiding principles for effective data visualization and explaining visualizations independent of any particular tool or programming language. In the process, we will cover some specifics of creating visualizations (scatter plots, bar charts, line graphs, and histograms) for data using R. 4.2 Chapter learning objectives Describe when to use the following kinds of visualizations to answer specific questions using a dataset: scatter plots line plots bar plots histogram plots Given a data set and a question, select from the above plot types and use R to create a visualization that best answers the question Given a visualization and a question, evaluate the effectiveness of the visualization and suggest improvements to better answer the question Referring to the visualization, communicate the conclusions in non-technical terms Identify rules of thumb for creating effective visualizations Define the three key aspects of ggplot objects: aesthetic mappings geometric objects scales Use the ggplot2 package in R to create and refine the above visualizations using: geometric objects: geom_point, geom_line, geom_histogram, geom_bar, geom_vline, geom_hline scales: scale_x_continuous, scale_y_continuous aesthetic mappings: x, y, fill, colour, shape labelling: xlab, ylab, labs font control and legend positioning: theme flipping axes: coord_flip subplots: facet_grid Describe the difference in raster and vector output formats Use ggsave to save visualizations in .png and .svg format 4.3 Choosing the visualization 4.3.0.1 Ask a question, and answer it The purpose of a visualization is to answer a question about a data set of interest. So naturally, the first thing to do before creating a visualization is to formulate the question about the data you are trying to answer. A good visualization will clearly answer your question without distraction; a great visualization will suggest even what the question was itself without additional explanation. Imagine your visualization as part of a poster presentation for a project; even if you aren’t standing at the poster explaining things, an effective visualization will convey your message to the audience. Recall the different data analysis questions from the first chapter. With the visualizations we will cover in this chapter, we will be able to answer only descriptive and exploratory questions. Be careful not to try to answer any predictive, inferential, causal or mechanistic questions, as we have not learned the tools necessary to do that properly just yet. As with most coding tasks, it is totally fine (and quite common) to make mistakes and iterate a few times before you find the right visualization for your data and question. There are many different kinds of plotting graphics available to use. For the kinds we will introduce in this book, the general rules of thumb are: line plots visualize trends with respect to an independent, ordered quantity (e.g. time) histograms visualize the distribution of one quantitative variable (i.e., all its possible values and how often they occur) scatter plots visualize the distribution / relationship of two quantitative variables bar plots visualize comparisons of amounts All types of visualization have their (mis)uses, but three kinds are usually hard to understand or are easily replaced with an oft-better alternative. In particular, you should avoid pie charts; it is generally better to use bars, as it is easier to compare bar heights than pie slice sizes. You should also not use 3-D visualizations, as they are typically hard to understand when converted to a static 2-D image format. Finally, do not use tables to make numerical comparisons; humans are much better at quickly processing visual information than text and math. Bar plots are again typically a better alternative. 4.4 Refining the visualization 4.4.0.1 Convey the message, minimize noise Just being able to make a visualization in R with ggplot2 (or any other tool for that matter) doesn’t mean that it effectively communicates your message to others. Once you have selected a broad type of visualization to use, you will have to refine it to suit your particular need. Some rules of thumb for doing this are listed below. They generally fall into two classes: you want to make your visualization convey your message, and you want to reduce visual noise as much as possible. Humans have limited cognitive ability to process information; both of these types of refinement aim to reduce the mental load on your audience when viewing your visualization, making it easier for them to understand and remember your message quickly. Convey the message Make sure the visualization answers the question you have asked most simply and plainly as possible. Use legends and labels so that your visualization is understandable without reading the surrounding text. Ensure the text, symbols, lines, etc., on your visualization are big enough to be easily read. Ensure the data are clearly visible; don’t hide the shape/distribution of the data behind other objects (e.g. a bar). Make sure to use colour schemes that are understandable by those with colourblindness (a surprisingly large fraction of the overall population). For example, colorbrewer.org and the RColorBrewer R package provide the ability to pick such colour schemes, and you can check your visualizations after you have created them by uploading to online tools such as the colour blindness simulator. Redundancy can be helpful; sometimes conveying the same message in multiple ways reinforces it for the audience. Minimize noise Use colours sparingly. Too many different colours can be distracting, create false patterns, and detract from the message. Be wary of overplotting. If your plot has too many dots or lines and starts to look like a mess, you need to do something different. Only make the plot area (where the dots, lines, bars are) as big as needed. Simple plots can be made small. Don’t adjust the axes to zoom in on small differences. If the difference is small, show that it’s small! 4.5 Creating visualizations with ggplot2 4.5.0.1 Build the visualization iteratively This section will cover examples of how to choose and refine a visualization given a data set and a question that you want to answer, and then how to create the visualization in R using ggplot2. To use the ggplot2 package, we need to load the tidyverse metapackage. library(tidyverse) 4.5.1 The Mauna Loa CO2 data set The Mauna Loa CO2 data set, curated by Dr. Pieter Tans, NOAA/GML and Dr. Ralph Keeling, Scripps Institution of Oceanography records the atmospheric concentration of carbon dioxide (CO2, in parts per million) at the Mauna Loa research station in Hawaii from 1959 onwards (Tans and Keeling 2020). Question: Does the concentration of atmospheric CO2 change over time, and are there any interesting patterns to note? # mauna loa carbon dioxide data co2_df &lt;- read_csv(&quot;data/mauna_loa.csv&quot;) %&gt;% filter(ppm &gt; 0, date_decimal &lt; 2000) co2_df ## # A tibble: 495 x 4 ## year month date_decimal ppm ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1958 3 1958. 316. ## 2 1958 4 1958. 317. ## 3 1958 5 1958. 318. ## 4 1958 7 1959. 316. ## 5 1958 8 1959. 315. ## 6 1958 9 1959. 313. ## 7 1958 11 1959. 313. ## 8 1958 12 1959. 315. ## 9 1959 1 1959. 316. ## 10 1959 2 1959. 316. ## # … with 485 more rows Since we are investigating a relationship between two variables (CO2 concentration and date), a scatter plot is a good place to start. Scatter plots show the data as individual points with x (horizontal axis) and y (vertical axis) coordinates. Here, we will use the decimal date as the x coordinate and CO2 concentration as the y coordinate. When using the ggplot2 package, we create the plot object with the ggplot function; there are a few basic aspects of a plot that we need to specify: the data: the name of the data frame object that we would like to visualize here, we specify the co2_df data frame the aesthetic mapping: tells ggplot how the columns in the data frame map to properties of the visualization to create an aesthetic mapping, we use the aes function here, we set the plot x axis to the date_decimal variable, and the plot y axis to the ppm variable the geometric object: specifies how the mapped data should be displayed to create a geometric object, we use a geom_* function (see the ggplot reference for a list of geometric objects) here, we use the geom_point function to visualize our data as a scatterplot We can see in Figure 4.1 how each of these aspects map to our code for creating a basic scatter plot for our co2_df data. Figure 4.1: Creating a scatterplot with the ggplot function We could pass many other possible arguments to the aesthetic mapping and geometric object to change how the plot looks. For the purposes of quickly testing things out to see what they look like, though, we can just go with the default settings: co2_scatter &lt;- ggplot(co2_df, aes(x = date_decimal, y = ppm)) + geom_point() co2_scatter Figure 4.2: Scatter plot of atmospheric concentration of CO2 over time Certainly, the visualization in Figure 4.2 shows a clear upward trend in the atmospheric concentration of CO2 over time. This plot answers the first part of our question in the affirmative, but that appears to be the only conclusion one can make from the scatter visualization. However, since time is an ordered quantity, we can try using a line plot instead using the geom_line function. Line plots require that their x coordinate orders the data, and connect the sequence of x and y coordinates with line segments. Let’s again try this with just the default arguments: co2_line &lt;- ggplot(co2_df, aes(x = date_decimal, y = ppm)) + geom_line() co2_line Figure 4.3: Line plot of atmospheric concentration of CO2 over time Aha! Figure 4.3 shows us there is another interesting phenomenon in the data: in addition to increasing over time, the concentration seems to oscillate as well. Given the visualization as it is now, it is still hard to tell how fast the oscillation is, but nevertheless, the line seems to be a better choice for answering the question than the scatter plot was. The comparison between these two visualizations illustrates a common issue with scatter plots: often, the points are shown too close together or even on top of one another, muddling information that would otherwise be clear (overplotting). Now that we have settled on the rough details of the visualization, it is time to refine things. This plot is fairly straightforward, and there is not much visual noise to remove. But there are a few things we must do to improve clarity, such as adding informative axis labels and making the font a more readable size. To add axis labels, we use the xlab and ylab functions. To change the font size, we use the theme function with the text argument: co2_line &lt;- ggplot(co2_df, aes(x = date_decimal, y = ppm)) + geom_line() + xlab(&quot;Year&quot;) + ylab(&quot;Atmospheric CO2 (ppm)&quot;) + theme(text = element_text(size = 18)) co2_line Figure 4.4: Line plot of atmospheric concentration of CO2 over time with clearer axes and labels Finally, let’s see if we can better understand the oscillation by changing the visualization slightly. Note that it is totally fine to use a small number of visualizations to answer different aspects of the question you are trying to answer. We will accomplish this by using scales, another important feature of ggplot2 that easily transforms the different variables and set limits. We scale the horizontal axis using the scale_x_continuous function, and the vertical axis with the scale_y_continuous function. We can transform the axis by passing the trans argument, and set limits by passing the limits argument. In particular, here, we will use the scale_x_continuous function with the limits argument to zoom in on just five years of data (say, 1990-1995): co2_line &lt;- ggplot(co2_df, aes(x = date_decimal, y = ppm)) + geom_line() + xlab(&quot;Year&quot;) + ylab(&quot;Atmospheric CO2 (ppm)&quot;) + scale_x_continuous(limits = c(1990, 1995)) + theme(text = element_text(size = 18)) co2_line Figure 4.5: Line plot of atmospheric concentration of CO2 from 1990 to 1995 only Interesting! It seems that each year, the atmospheric CO2 increases until it reaches its peak somewhere around April, decreases until around late September, and finally increases again until the end of the year. In Hawaii, there are two seasons: summer from May through October, and winter from November through April. Therefore, the oscillating pattern in CO2 matches up fairly closely with the two seasons. 4.5.2 The island landmass data set The islands.csv data set contains a list of Earth’s landmasses as well as their area (in thousands of square miles) (McNeil 1977). Question: Are the continents (North / South America, Africa, Europe, Asia, Australia, Antarctica) Earth’s seven largest landmasses? If so, what are the next few largest landmasses after those? # islands data islands_df &lt;- read_csv(&quot;data/islands.csv&quot;) islands_df ## # A tibble: 48 x 2 ## landmass size ## &lt;chr&gt; &lt;dbl&gt; ## 1 Africa 11506 ## 2 Antarctica 5500 ## 3 Asia 16988 ## 4 Australia 2968 ## 5 Axel Heiberg 16 ## 6 Baffin 184 ## 7 Banks 23 ## 8 Borneo 280 ## 9 Britain 84 ## 10 Celebes 73 ## # … with 38 more rows Here, we have a list of Earth’s landmasses, and are trying to compare their sizes. The right type of visualization to answer this question is a bar plot, specified by the geom_bar function in ggplot2. However, by default, geom_bar sets the heights of bars to the number of times a value appears in a data frame (its count); here, we want to plot exactly the values in the data frame, i.e., the landmass sizes. So we have to pass the stat = \"identity\" argument to geom_bar: islands_bar &lt;- ggplot(islands_df, aes(x = landmass, y = size)) + geom_bar(stat = &quot;identity&quot;) islands_bar Figure 4.6: Bar plot of all Earth’s landmasses’ size with squished labels Alright, not bad! The plot in Figure 4.6 is definitely the right kind of visualization, as we can clearly see and compare sizes of landmasses. The major issues are that the smaller landmasses’ sizes are hard to distinguish, and the names of the landmasses are obscuring each other as they have been squished into too little space. But remember that the question we asked was only about the largest landmasses; let’s make the plot a little bit clearer by keeping only the largest 12 landmasses. We do this using the top_n function. Then to help us make sure the labels have enough space, we’ll use horizontal bars instead of vertical ones. We do this using the coord_flip function, which swaps the x and y coordinate axes: islands_top12 &lt;- top_n(islands_df, 12, size) islands_bar &lt;- ggplot(islands_top12, aes(x = landmass, y = size)) + geom_bar(stat = &quot;identity&quot;) + coord_flip() islands_bar Figure 4.7: Bar plot of size for Earth’s largest 12 landmasses The plot in Figure 4.7 is definitely clearer now, and allows us to answer our question (“are the top 7 largest landmasses continents?”) in the affirmative. But the question could be made clearer from the plot by organizing the bars not by alphabetical order but by size, and to colour them based on whether they are a continent. To do this, we use mutate to add a column to the data regarding whether or not the landmass is a continent: islands_top12 &lt;- top_n(islands_df, 12, size) continents &lt;- c(&quot;Africa&quot;, &quot;Antarctica&quot;, &quot;Asia&quot;, &quot;Australia&quot;, &quot;Europe&quot;, &quot;North America&quot;, &quot;South America&quot;) islands_ct &lt;- mutate(islands_top12, is_continent = ifelse(landmass %in% continents, &quot;Continent&quot;, &quot;Other&quot;)) islands_ct ## # A tibble: 12 x 3 ## landmass size is_continent ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Africa 11506 Continent ## 2 Antarctica 5500 Continent ## 3 Asia 16988 Continent ## 4 Australia 2968 Continent ## 5 Baffin 184 Other ## 6 Borneo 280 Other ## 7 Europe 3745 Continent ## 8 Greenland 840 Other ## 9 Madagascar 227 Other ## 10 New Guinea 306 Other ## 11 North America 9390 Continent ## 12 South America 6795 Continent In order to colour the bars, we add the fill argument to the aesthetic mapping. Then we use the reorder function in the aesthetic mapping to organize the landmasses by their size variable. Finally, we use the labs and theme functions to add labels, change the font size, and position the legend: islands_bar &lt;- ggplot(islands_ct, aes(x = reorder(landmass, size), y = size, fill = is_continent)) + geom_bar(stat = &quot;identity&quot;) + labs(x = &quot;Landmass&quot;, y = &quot;Size (1000 square mi)&quot;, fill = &quot;Type&quot;) + coord_flip() + theme(text = element_text(size = 18), legend.position = c(0.75, 0.45)) islands_bar Figure 4.8: Bar plot of size for Earth’s largest 12 landmasses coloured by whether its a continent with clearer axes and labels The plot in Figure 4.8 is now a very effective visualization for answering our original questions. Landmasses are organized by their size, and continents are coloured differently than other landmasses, making it quite clear that continents are the largest seven landmasses. 4.5.3 The Old Faithful eruption/waiting time data set The faithful data set contains measurements of the waiting time between eruptions and the subsequent eruption duration (in minutes). The faithful data set is available in base R under the name faithful so it does not need to be loaded. Question: Is there a relationship between the waiting time before an eruption to the duration of the eruption? # old faithful eruption time / wait time data faithful ## # A tibble: 272 x 2 ## eruptions waiting ## &lt;dbl&gt; &lt;dbl&gt; ## 1 3.6 79 ## 2 1.8 54 ## 3 3.33 74 ## 4 2.28 62 ## 5 4.53 85 ## 6 2.88 55 ## 7 4.7 88 ## 8 3.6 85 ## 9 1.95 51 ## 10 4.35 85 ## # … with 262 more rows Here again, we investigate the relationship between two quantitative variables (waiting time and eruption time). But if you look at the output of the data frame, you’ll notice that neither of the columns are ordered. So, in this case, let’s start again with a scatter plot: faithful_scatter &lt;- ggplot(faithful, aes(x = waiting, y = eruptions)) + geom_point() faithful_scatter Figure 4.9: Scatter plot of waiting time and eruption time We can see in Figure 4.9 the data tend to fall into two groups: one with short waiting and eruption times, and one with long waiting and eruption times. Note that in this case, there is no overplotting: the points are generally nicely visually separated, and the pattern they form is clear. In order to refine the visualization, we need only to add axis labels and make the font more readable: faithful_scatter &lt;- ggplot(faithful, aes(x = waiting, y = eruptions)) + geom_point() + labs(x = &quot;Waiting Time (mins)&quot;, y = &quot;Eruption Duration (mins)&quot;) + theme(text = element_text(size = 18)) faithful_scatter Figure 4.10: Scatter plot of waiting time and eruption time with clearer axes and labels 4.5.4 The Canadian languages data set We will return to the can_lang data set (Timbers 2020), which contains counts of languages from the 2016 Canadian census. Question: Is there a relationship between the number of people who speak a language as their mother tongue and the number of people who speak that language as their primary spoken language at home? And is there a pattern in the strength of this relationship in the higher-level language categories (Official languages, Aboriginal languages, or non-official and non-Aboriginal languages)? can_lang &lt;- read_csv(&quot;data/can_lang.csv&quot;) can_lang ## # A tibble: 214 x 6 ## category language mother_tongue most_at_home most_at_work lang_known ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Aboriginal la… Aboriginal… 590 235 30 665 ## 2 Non-Official … Afrikaans 10260 4785 85 23415 ## 3 Non-Official … Afro-Asiat… 1150 445 10 2775 ## 4 Non-Official … Akan (Twi) 13460 5985 25 22150 ## 5 Non-Official … Albanian 26895 13135 345 31930 ## 6 Aboriginal la… Algonquian… 45 10 0 120 ## 7 Aboriginal la… Algonquin 1260 370 40 2480 ## 8 Non-Official … American S… 2685 3020 1145 21930 ## 9 Non-Official … Amharic 22465 12785 200 33670 ## 10 Non-Official … Arabic 419890 223535 5585 629055 ## # … with 204 more rows We will begin with a scatter plot of the mother_tongue and most_at_home columns from our data frame. ggplot(can_lang, aes(x = most_at_home, y = mother_tongue)) + geom_point() Figure 4.11: Scatter plot of number of Canadians reporting a language as their mother tongue vs the primary language at home To improve the interpretability of the data visualization in Figure 4.11 we should replace the default axis names with a more informative labels. We can use \\n to create a line break in the axis names so the words after \\n are printed on a new line. This will make the axes labels on the plots more readable. ggplot(can_lang, aes(x = most_at_home, y = mother_tongue)) + geom_point() + xlab(&quot;Language spoken most at home \\n (number of Canadian residents)&quot;) + ylab(&quot;Mother tongue \\n (number of Canadian residents)&quot;) Figure 4.12: Scatter plot of number of Canadians reporting a language as their mother tongue vs the primary language at home with x and y labels Most of the data points from the 214 observations in this data set are bunched up in the lower left-handside of the visualization in Figure 4.12. The data is clumped because many more people in Canada speak the two languages represented by the points in the upper right corner. By filtering the data, we can confirm these two points correspond to the two official languages (English and French): can_lang %&gt;% filter(language == &quot;English&quot; | language == &quot;French&quot;) ## # A tibble: 2 x 6 ## category language mother_tongue most_at_home most_at_work lang_known ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Official languages English 19460850 22162865 15265335 29748265 ## 2 Official languages French 7166700 6943800 3825215 10242945 To answer our question, we will need to adjust the scale of the x and y axes so that they are on a log scale. Doing this can help make patterns in the data more interpretable. ggplot(can_lang, aes(x = most_at_home, y = mother_tongue)) + geom_point() + xlab(&quot;Language spoken most at home \\n (number of Canadian residents)&quot;) + ylab(&quot;Mother tongue \\n (number of Canadian residents)&quot;) + scale_x_log10(labels = scales::comma) + scale_y_log10(labels = scales::comma) Figure 4.13: Scatter plot of number of Canadians reporting a language as their mother tongue vs the primary language at home with log adjusted x and y axes Here we are adjusting the scale of the x and y axes on a \\(log\\) scale with base 10. Notice that \\(log_{10}(10) = 1\\), \\(log_{10}(100) = 2\\), and \\(log_{10}(1000) = 3\\). On the logarithmic scale, the values 10, 100 and 1000 are all the same distance apart so we see that applying this function is moving big values closer together and moving small values farther apart. Therefore by adjusting the scale in this way we are highlighting the differences between small values, which allows us to better see the relationship in Figure 4.13. Log transformations are not the only way you can transform the data! Notice if you write log(0) in R you get -Inf so if you have lots of zeros in your data you will want to do something else, which is beyond the scope of the book. From the visualization in Figure 4.13, we see that for the 214 languages in this data set, as the number of people who have a language as their mother tongue increases, so does the number of people who speak that language at home. When we see two variables do this, we call this a positive relationship. Because the points are fairly close together, we can say that the relationship is strong. Because drawing a straight line through these points would fit the pattern we observe quite well, we say that it’s linear. Similar to our example in chapter 1, we can convert the counts to percentages to make them more understandable. Changing the units allows us to better interpret the numbers in our data set. We can do this by dividing the number of people reporting a given language as their mother tongue or primary language at home by the number of people who live in Canada and multiplying by 100%. For example, the percentage of people who reported that their mother tongue was English in the 2016 Canadian census was 19,460,850 / 35,151,728 \\(\\times\\) 100 % = 55.36%. Below we use mutate to calculate the percentage of people reporting a given language as their mother tongue and primary language at home for all the languages in the can_lang data set. Since the new columns are appended to the end of the data table, we selected the new columns after the transformation so you can clearly see the mutated output from the table. can_lang &lt;- can_lang %&gt;% mutate( mother_tongue_percent = (mother_tongue / 35151728)*100, most_at_home_percent = (most_at_home / 35151728)*100 ) can_lang %&gt;% select(mother_tongue_percent, most_at_home_percent) ## # A tibble: 214 x 2 ## mother_tongue_percent most_at_home_percent ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.00168 0.000669 ## 2 0.0292 0.0136 ## 3 0.00327 0.00127 ## 4 0.0383 0.0170 ## 5 0.0765 0.0374 ## 6 0.000128 0.0000284 ## 7 0.00358 0.00105 ## 8 0.00764 0.00859 ## 9 0.0639 0.0364 ## 10 1.19 0.636 ## # … with 204 more rows Finally, let’s visualize the data now that we have represented it as percentages (and change our axis labels to reflect this change in units!): ggplot(can_lang, aes(x = most_at_home_percent, y = mother_tongue_percent)) + geom_point() + xlab(&quot;Language spoken most at home \\n (percentage of Canadian residents)&quot;) + ylab(&quot;Mother tongue \\n (percentage of Canadian residents)&quot;) + scale_x_log10(labels = scales::comma) + scale_y_log10(labels = scales::comma) Figure 4.14: Scatter plot of percentage of Canadians reporting a language as their mother tongue vs the primary language at home Now we’ll move onto the second part of our exploratory data analysis question: when considering the relationship between the number of people who have a language as their mother tongue and the number of people who speak that language at home, is there a pattern in the strength of this relationship in the higher-level language categories (Official languages, Aboriginal languages, or non-official and non-Aboriginal languages)? One common way to explore this is to colour the data points on the scatter plot we have already created by group/category. For example, given that we have the higher-level language category for each language recorded in the 2016 Canadian census, we can colour the points in our previous scatter plot to represent each language’s higher-level language category. Here we want to distinguish the values according to the category group with which they belong. We can add an argument to the aes function, specifying that the category column should colour the points. Adding this argument will colour the points according to their group and add a legend at the side of the plot. ggplot(can_lang, aes(x = most_at_home_percent, y = mother_tongue_percent, color = category)) + geom_point() + xlab(&quot;Language spoken most at home \\n (percentage of Canadian residents)&quot;) + ylab(&quot;Mother tongue \\n (percentage of Canadian residents)&quot;) + scale_x_log10(labels = scales::comma) + scale_y_log10(labels = scales::comma) Figure 4.15: Scatter plot of percentage of Canadians reporting a language as their mother tongue vs the primary language at home coloured by language category In Figure ??, the points are coloured with the default ggplot2 colour palette. But what if you want to use different colours? In R, one package that provides alternative colour palettes is RColorBrewer (Neuwirth 2014). You can visualize the list of colour palettes that RColorBrewer has to offer with the display.brewer.all() function. You can also print a list of colour blind friendly palettes by adding colorblindFriendly = T to the function. library(RColorBrewer) display.brewer.all(colorblindFriendly = T) From the output above, we can choose the colour palette we want to use in our plot. To change the colour palette, we add the scale_colour_brewer() layer indicating the palette we want to use. ggplot(can_lang, aes(x = most_at_home_percent, y = mother_tongue_percent, color = category)) + geom_point() + xlab(&quot;Language spoken most at home \\n (percentage of Canadian residents)&quot;) + ylab(&quot;Mother tongue \\n (percentage of Canadian residents)&quot;) + scale_x_log10(labels = scales::comma) + scale_y_log10(labels = scales::comma) + scale_color_brewer(palette = &quot;RdYlBu&quot;) Figure 4.16: Scatter plot of percentage of Canadians reporting a language as their mother tongue vs the primary language at home coloured by language category with colour blind friendly colours You can use this color blindness simulator to check if your visualizations are colour blind friendly. From the visualization in Figure 4.16, we can now clearly see that not just a lot, but that the majority of Canadians reported one of the official languages as their mother tongue and as the language they speak most often at home. What do we see when considering the second part of our exploratory question? Do we see a difference in the pattern of the relationship between the number of people who speak a language as their mother tongue and the number of people who speak a language as their primary spoken language at home between higher-level language categories? Probably not! For each higher-level language category, there appears to be a positive relationship between the number of people who speak a language as their mother tongue and the number of people who speak a language as their primary spoken language at home. This relationship looks similar, regardless of the category. Does this mean that this relationship is positive for all languages in the world? Can we use this data visualization on its own to predict how many people have a given language as their mother tongue if we know how many people speak it as their primary language at home? The answer to both these questions is “no.” However, with exploratory data analysis, we can create new hypotheses, ideas, and questions (like the ones at the beginning of this paragraph). Answering those questions would likely involve gathering additional data and doing more complex analyses, which we will see more of later in this course. Now, we can go one step further and distinguish English and French languages with different colours in our visualization. To separate these languages, we will filter the rows where the language is either English or French and mutate the category column to equal the corresponding language. english_and_french &lt;- can_lang %&gt;% filter(language == &quot;English&quot; | language == &quot;French&quot;) %&gt;% mutate(category = language) english_and_french ## # A tibble: 2 x 8 ## category language mother_tongue most_at_home most_at_work lang_known ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 English English 19460850 22162865 15265335 29748265 ## 2 French French 7166700 6943800 3825215 10242945 ## # … with 2 more variables: mother_tongue_percent &lt;dbl&gt;, ## # most_at_home_percent &lt;dbl&gt; Next we will bind the mutated data set english_and_french that we just created with the remaining rows in the can_lang data set: can_lang &lt;- bind_rows( english_and_french, can_lang %&gt;% filter(language != &quot;English&quot; &amp; language != &quot;French&quot;) ) != in R is a logical operator that means “NOT EQUAL TO.” &amp; is a logical operator meaning “AND.” We are filtering the can_lang data frame where the rows do not equal “English” and do not equal “French.” We have added a few more layers to make the data visualization in Figure 4.17 even more effective. Specifically, we used have improved the visualizations accessibility by choosing colours that are easier to distinguish, mapped category to shape, and handled the problem of overlapping data points by making them slightly transparent. ggplot(can_lang, aes( x = most_at_home_percent, y = mother_tongue_percent, colour = category, shape = category )) + # map categories to different shapes geom_point(alpha = 0.6) + # set the transparency of the points xlab(&quot;Language spoken most at home \\n (percentage of Canadian residents)&quot;) + ylab(&quot;Mother tongue \\n (percentage of Canadian residents)&quot;) + scale_x_log10(labels = scales::comma) + scale_y_log10(labels = scales::comma) + scale_color_brewer(palette = &quot;RdYlBu&quot;) Figure 4.17: Scatter plot of percentage of Canadians reporting a language as their mother tongue vs the primary language at home coloured by language category 4.5.5 The Michelson speed of light data set The morley data set contains measurements of the speed of light (in kilometres per second with 299,000 subtracted) from the year 1879 for five experiments, each with 20 consecutive runs (Michelson 1882). This data set is available in base R under the name morley so it does not need to be loaded. Question: Given what we know now about the speed of light (299,792.458 kilometres per second), how accurate were each of the experiments? # michelson morley experimental data morley ## # A tibble: 100 x 3 ## Expt Run Speed ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 1 850 ## 2 1 2 740 ## 3 1 3 900 ## 4 1 4 1070 ## 5 1 5 930 ## 6 1 6 850 ## 7 1 7 950 ## 8 1 8 980 ## 9 1 9 980 ## 10 1 10 880 ## # … with 90 more rows In this experimental data, Michelson was trying to measure just a single quantitative number (the speed of light). The data set contains many measurements of this single quantity. To tell how accurate the experiments were, we need to visualize the distribution of the measurements (i.e., all their possible values and how often each occurs). We can do this using a histogram. A histogram helps us visualize how a particular variable is distributed in a data set by separating the data into bins, and then using vertical bars to show how many data points fell in each bin. To create a histogram in ggplot2 we will use the geom_histogram geometric object, setting the x axis to the Speed measurement variable; and as we did before, let’s use the default arguments just to see how things look: morley_hist &lt;- ggplot(morley, aes(x = Speed)) + geom_histogram() morley_hist Figure 4.18: Histogram of Michelson’s speed of light data Figure 4.18 is a great start. However, we cannot tell how accurate the measurements are using this visualization unless we can see what the true value is. In order to visualize the true speed of light, we will add a vertical line with the geom_vline function, setting the xintercept argument to the true value. There is a similar function, geom_hline, that is used for plotting horizontal lines. Note that vertical lines are used to denote quantities on the horizontal axis, while horizontal lines are used to denote quantities on the vertical axis. morley_hist &lt;- ggplot(morley, aes(x = Speed)) + geom_histogram() + geom_vline(xintercept = 792.458, linetype = &quot;dashed&quot;, size = 1.0) morley_hist Figure 4.19: Histogram of Michelson’s speed of light data with vertical line indicating true speed of light In Figure 4.19, we also still cannot tell which experiments (denoted in the Expt column) led to which measurements; perhaps some experiments were more accurate than others. To fully answer our question, we need to separate the measurements from each other visually. We can try to do this using a coloured histogram, where counts from different experiments are stacked on top of each other in different colours. We can create a histogram coloured by the Expt variable by adding it to the fill aesthetic mapping. We make sure the different colours can be seen (despite them all sitting on top of each other) by setting the alpha argument in geom_histogram to 0.5 to make the bars slightly translucent: morley_hist &lt;- ggplot(morley, aes(x = Speed, fill = Expt)) + geom_histogram(position = &quot;identity&quot;, alpha = 0.5) + geom_vline(xintercept = 792.458, linetype = &quot;dashed&quot;, size = 1.0) morley_hist Figure 4.20: Histogram of Michelson’s speed of light data coloured by experiment Wait a second, we notice that the histogram is still all the same colour! What is going on here? If we look at the printed morley data, the column Expt is an integer (we see the label &lt;int&gt; underneath the Expt column name). But, we want to treat it as a categorical variable. To fix this issue we can write factor(Expt) in the fill aesthetic mapping. By writing factor(Expt) we are ensuring that R will treat this variable as a factor and the colour will be mapped discretely. morley_hist &lt;- ggplot(morley, aes(x = Speed, fill = Expt)) + geom_histogram(position = &quot;identity&quot;, alpha = 0.5) + geom_vline(xintercept = 792.458, linetype = &quot;dashed&quot;, size = 1.0) morley_hist Figure 4.21: Histogram of Michelson’s speed of light data coloured by experiment as factor Factors impact plots in two ways: (1) ensuring a colour is mapped as discretely where appropriate (like in this example) and (2) the ordering of levels in a plot. ggplot takes into account the order of the factor levels as opposed to the order that is displayed in your data frame. Learning how to reorder your factor levels will help you with reordering the labels of a factor on a plot. Unfortunately, the attempt to separate out the experiment number visually has created a bit of a mess. All of the colours in Figure 4.20 are blending together, and although it is possible to derive some insight from this (e.g., experiments 1 and 3 had some of the most incorrect measurements), it isn’t the clearest way to convey our message and answer the question. Let’s try a different strategy of creating multiple separate histograms on top of one another. In order to create a plot in ggplot2 that has multiple subplots arranged in a grid, we use the facet_grid function. The argument to facet_grid specifies the variable(s) used to split the plot into subplots. It has the syntax vertical_variable ~ horizontal_variable, where veritcal_variable is used to split the plot vertically, horizontal_variable is used to split horizontally, and . is used if there should be no split along that axis. In our case, we only want to split vertically along the Expt variable, so we use Expt ~ . as the argument to facet_grid. morley_hist &lt;- ggplot(morley, aes(x = Speed, fill = factor(Expt))) + geom_histogram(position = &quot;identity&quot;) + facet_grid(Expt ~ .) + geom_vline(xintercept = 792.458, linetype = &quot;dashed&quot;, size = 1.0) morley_hist Figure 4.22: Histogram of Michelson’s speed of light data split vertically by experiment The visualization in Figure 4.22 now makes it quite clear how accurate the different experiments were with respect to one another. There are two finishing touches to make this visualization even clearer. First and foremost, we need to add informative axis labels using the labs function, and increase the font size to make it readable using the theme function. Second, and perhaps more subtly, even though it is easy to compare the experiments on this plot to one another, it is hard to get a sense for just how accurate all the experiments were overall. For example, how accurate is the value 800 on the plot, relative to the true speed of light? To answer this question, we’ll use the mutate function to transform our data into a relative measure of accuracy rather than absolute measurements: morley_rel &lt;- mutate(morley, relative_accuracy = 100 * ((299000 + Speed) - 299792.458) / (299792.458)) morley_hist &lt;- ggplot(morley_rel, aes(x = relative_accuracy, fill = factor(Expt))) + geom_histogram(position = &quot;identity&quot;) + facet_grid(Expt ~ .) + geom_vline(xintercept = 0, linetype = &quot;dashed&quot;, size = 1.0) + labs(x = &quot;Relative Accuracy (%)&quot;, y = &quot;# Measurements&quot;, fill = &quot;Experiment ID&quot;) + theme(text = element_text(size = 18)) morley_hist Figure 4.23: Histogram of relative accuracy split vertically by experiment with clearer axes and labels Wow, impressive! These measurements of the speed of light from 1879 had errors around 0.05% of the true speed. Figure 4.23 shows you that even though experiments 2 and 5 were perhaps the most accurate, all of the experiments did quite an admirable job given the technology available at the time. 4.6 Explaining the visualization 4.6.0.1 Tell a story Typically, your visualization will not be shown entirely on its own, but rather it will be part of a larger presentation. Further, visualizations can provide supporting information for any aspect of a presentation, from opening to conclusion. For example, you could use an exploratory visualization in the opening of the presentation to motivate your choice of a more detailed data analysis / model, a visualization of the results of your analysis to show what your analysis has uncovered, or even one at the end of a presentation to help suggest directions for future work. Regardless of where it appears, a good way to discuss your visualization is as a story: Establish the setting and scope, and motivate why you did what you did. Pose the question that your visualization answers. Justify why the question is important to answer. Answer the question using your visualization. Make sure you describe all aspects of the visualization (including describing the axes). But you can emphasize different aspects based on what is important to answer your question: trends (lines): Does a line describe the trend well? If so, the trend is linear, and if not, the trend is nonlinear. Is the trend increasing, decreasing, or neither? Is there a periodic oscillation (wiggle) in the trend? Is the trend noisy (does the line “jump around” a lot) or smooth? distributions (scatters, histograms): How spread out are the data? Where are they centered, roughly? Are there any obvious “clusters” or “subgroups,” which would be visible as multiple bumps in the histogram? distributions of two variables (scatters): is there a clear / strong relationship between the variables (points fall in a distinct pattern), a weak one (points fall in a pattern but there is some noise), or no discernible relationship (the data are too noisy to make any conclusion)? amounts (bars): How large are the bars relative to one another? Are there patterns in different groups of bars? Summarize your findings, and use them to motivate whatever you will discuss next. Below are two examples of how one might take these four steps in describing the example visualizations that appeared earlier in this chapter. Each of the steps is denoted by its numeral in parentheses, e.g. (3). Mauna Loa Atmospheric CO2 Measurements: (1) Many current forms of energy generation and conversion—from automotive engines to natural gas power plants—rely on burning fossil fuels and produce greenhouse gases, typically primarily carbon dioxide (CO2), as a byproduct. Too much of these gases in the Earth’s atmosphere will cause it to trap more heat from the sun, leading to global warming. (2) In order to assess how quickly the atmospheric concentration of CO2 is increasing over time, we (3) used a data set from the Mauna Loa observatory from Hawaii, consisting of CO2 measurements from 1959 to the present. We plotted the measured concentration of CO2 (on the vertical axis) over time (on the horizontal axis). From this plot, you can see a clear, increasing, and generally linear trend over time. There is also a periodic oscillation that occurs once per year and aligns with Hawaii’s seasons, with an amplitude that is small relative to the growth in the overall trend. This shows that atmospheric CO2 is clearly increasing over time, and (4) it is perhaps worth investigating more into the causes. Michelson Light Speed Experiments: (1) Our modern understanding of the physics of light has advanced significantly from the late 1800s when Michelson and Morley’s experiments first demonstrated that it had a finite speed. We now know based on modern experiments that it moves at roughly 299792.458 kilometres per second. (2) But how accurately were we first able to measure this fundamental physical constant, and did certain experiments produce more accurate results than others? (3) To better understand this we plotted data from 5 experiments by Michelson in 1879, each with 20 trials, as histograms stacked on top of one another. The horizontal axis shows the accuracy of the measurements relative to the true speed of light as we know it today, expressed as a percentage. From this visualization, you can see that most results had relative errors of at most 0.05%. You can also see that experiments 1 and 3 had measurements that were the farthest from the true value, and experiment 5 tended to provide the most consistently accurate result. (4) It would be worth further investigating the differences between these experiments to see why they produced different results. 4.7 Saving the visualization 4.7.0.1 Choose the right output format for your needs Just as there are many ways to store data sets, there are many ways to store visualizations and images. Which one you choose can depend on several factors, such as file size/type limitations (e.g., if you are submitting your visualization as part of a conference paper or to a poster printing shop) and where it will be displayed (e.g., online, in a paper, on a poster, on a billboard, in talk slides). Generally speaking, images come in two flavours: bitmap (or raster) formats and vector (or scalable graphics) formats. Bitmap / Raster images are represented as a 2-D grid of square pixels, each with its own colour. Raster images are often compressed before storing so they take up less space. A compressed format is lossy if the image cannot be perfectly recreated when loading and displaying, with the hope that the change is not noticeable. Lossless formats, on the other hand, allow a perfect display of the original image. Common file types: JPEG (.jpg, .jpeg): lossy, usually used for photographs PNG (.png): lossless, usually used for plots / line drawings BMP (.bmp): lossless, raw image data, no compression (rarely used) TIFF (.tif, .tiff): typically lossless, no compression, used mostly in graphic arts, publishing Open-source software: GIMP Vector / Scalable Graphics images are represented as a collection of mathematical objects (lines, surfaces, shapes, curves). When the computer displays the image, it redraws all of the elements using their mathematical formulas. Common file types: SVG (.svg): general-purpose use EPS (.eps), general-purpose use (rarely used) Open-source software: Inkscape Raster and vector images have opposing advantages and disadvantages. A raster image of a fixed width / height takes the same amount of space and time to load regardless of what the image shows (caveat: the compression algorithms may shrink the image more or run faster for certain images). A vector image takes space and time to load corresponding to how complex the image is, since the computer has to draw all the elements each time it is displayed. For example, if you have a scatter plot with 1 million points stored as an SVG file, it may take your computer some time to open the image. On the other hand, you can zoom into / scale up vector graphics as much as you like without the image looking bad, while raster images eventually start to look “pixellated.” PDF files: The portable document format PDF (.pdf) is commonly used to store both raster and vector graphics formats. If you try to open a PDF and it’s taking a long time to load, it may be because there is a complicated vector graphics image that your computer is rendering. Let’s investigate how different image file formats behave with a scatter plot of the Old Faithful data set (Hardle 1991): library(svglite) # we need this to save SVG files faithful_plot &lt;- ggplot(data = faithful, aes(x = waiting, y = eruptions)) + geom_point() faithful_plot Figure 4.24: Scatter plot of waiting time and eruption time ggsave(&quot;faithful_plot.png&quot;, faithful_plot) ggsave(&quot;faithful_plot.jpg&quot;, faithful_plot) ggsave(&quot;faithful_plot.bmp&quot;, faithful_plot) ggsave(&quot;faithful_plot.tiff&quot;, faithful_plot) ggsave(&quot;faithful_plot.svg&quot;, faithful_plot) print(paste(&quot;PNG filesize: &quot;, file.info(&quot;faithful_plot.png&quot;)[&quot;size&quot;] / 1000000, &quot;MB&quot;)) ## [1] &quot;PNG filesize: 0.07861 MB&quot; print(paste(&quot;JPG filesize: &quot;, file.info(&quot;faithful_plot.jpg&quot;)[&quot;size&quot;] / 1000000, &quot;MB&quot;)) ## [1] &quot;JPG filesize: 0.139187 MB&quot; print(paste(&quot;BMP filesize: &quot;, file.info(&quot;faithful_plot.bmp&quot;)[&quot;size&quot;] / 1000000, &quot;MB&quot;)) ## [1] &quot;BMP filesize: 3.148978 MB&quot; print(paste(&quot;TIFF filesize: &quot;, file.info(&quot;faithful_plot.tiff&quot;)[&quot;size&quot;] / 1000000, &quot;MB&quot;)) ## [1] &quot;TIFF filesize: 9.443892 MB&quot; print(paste(&quot;SVG filesize: &quot;, file.info(&quot;faithful_plot.svg&quot;)[&quot;size&quot;] / 1000000, &quot;MB&quot;)) ## [1] &quot;SVG filesize: 0.030047 MB&quot; Wow, that’s quite a difference! Notice that for such a simple plot with few graphical elements (points), the vector graphics format (SVG) is over 100 times smaller than the uncompressed raster images (BMP, TIFF). Also, note that the JPG format is twice as large as the PNG format since the JPG compression algorithm is designed for natural images (not plots). Below, we also show what the images look like when we zoom in to a rectangle with only 3 data points. You can see why vector graphics formats are so useful: because they’re just based on mathematical formulas, vector graphics can be scaled up to arbitrary sizes. This makes them great for presentation media of all sizes, from papers to posters to billboards. Figure 4.25: Zoomed in faithful, raster (PNG, left) and vector (SVG, right) formats 4.8 Additional resources The ggplot2 page on the tidyverse website is where you should look if you want to learn more about the functions in this chapter, the full set of arguments you can use, and other related functions. The site also provides a very nice cheat sheet that summarizes many of the data wrangling functions from this chapter. The Fundamentals of Data Visualization has a wealth of information on designing effective visualizations. It is not specific to any particular programming language or library. If you want to improve your visualization skills, this is the next place to look. R for Data Science has a chapter on creating visualizations using ggplot2. This reference is specific to R and ggplot2, but provides a much more detailed introduction to the full set of tools that ggplot2 provides. This chapter is where you should look if you want to learn how to make more intricate visualizations in ggplot2 than what is included in this chapter. References "],["version-control.html", "Chapter 5 Collaboration with version control 5.1 Overview 5.2 Chapter learning objectives 5.3 What is version control, and why should I use it? 5.4 Creating a space for your project online 5.5 Creating and editing files on GitHub 5.6 Cloning your repository on JupyterHub 5.7 Working in a cloned repository on JupyterHub 5.8 Collaboration 5.9 Additional resources", " Chapter 5 Collaboration with version control 5.1 Overview This chapter will introduce the concept of using version control systems to track changes to a project over its lifespan, to share and edit code in a collaborative team, and to distribute the finished project to its intended audience. This chapter also demonstrates how to implement these ideas effectively in practice using Git, GitHub, and JupyterHub. 5.2 Chapter learning objectives By the end of the chapter, students will be able to: Describe what version control is and why data analysis projects can benefit from it Create a remote version control repository on GitHub Move changes to files from GitHub to JupyterHub, and from JupyterHub to GitHub Give collaborators access to the repository Resolve conflicting edits made by multiple collaborators Communicate with collaborators using issues Use best practices when collaborating on a project with others 5.3 What is version control, and why should I use it? Data analysis projects often require iteration and revision to move from an initial idea to a finished product that is ready for the intended audience. Without deliberate and conscious effort towards tracking changes made to the analysis, projects tend to become messy, with mystery results files that cannot be reproduced, temporary files with snippets of ideas, mind-boggling filenames like document_final_draft_v5_final.txt, large blocks of commented code “saved for later,” and more. Additionally, the iterative nature of data analysis projects makes it important to be able to examine earlier versions of code and writing. Finally, data analyses are typically completed by a team of people rather than a single person. This means that files need to be shared across multiple computers, and multiple people often end up editing the project simultaneously. In such a situation, determining who has the latest version of the project—and how to resolve conflicting edits—can be a real challenge. Version control helps solve these challenges by tracking changes to the files in the analysis (code, writing, data, etc) over the lifespan of the project, including when the changes were made and who made them. This provides the means both to view earlier versions of the project and to revert changes. Version control also facilitates collaboration via tools to share edits with others and resolve conflicting edits. But even if you’re working on a project alone, you should still use version control. It helps you keep track of what you’ve done, when you did it, and what you’re planning to do next! You mostly collaborate with yourself, and me-from-two-months-ago never responds to email. –Mark T. Holder In order to version control a project, you generally need two things: a version control system and a repository hosting service. The version control system is the software that is responsible for tracking changes, sharing changes you make with others, obtaining changes others have made, and resolving conflicting edits. The repository hosting service is responsible for storing a copy of the version controlled project online (a repository), where you and your collaborators can access it remotely, discuss issues and bugs, and distribute your final product. For both of these items, there is a wide variety of choices; some of the more popular ones are: Version control systems: Git Mercurial Subversion Repository hosting services: GitHub GitLab BitBucket In this textbook we’ll use Git for version control, and GitHub for repository hosting, because both are currently the most widely-used platforms. Note: technically you don’t have to use a repository hosting service. You can, for example, use Git to version control a project that is stored only in a folder on your computer. But using a repository hosting service provides a few big benefits, including managing collaborator access permissions, tools to discuss and track bugs, and the ability to have external collaborators contribute work, not to mention the safety of having your work backed up in the cloud. Since most repository hosting services now offer free accounts, there are not many situations in which you wouldn’t want to use one for your project. 5.4 Creating a space for your project online Before you can create repositories, you will need a GitHub account; you can sign up for a free account at https://github.com/. Once you have logged into your account, you can create a new repository to host your project by clicking on the “+” icon in the upper right hand corner, and then on “New Repository” as shown below: On the next page, do the following: Enter the name for your project repository. In the example below, we use canadian_languages. Most repositories follow this naming convention, which involves lowercase letter words separated by either underscores or hyphens. Choose an option for the privacy of your repository If you select “Public,” your repository may be viewed by anyone, but only you and collaborators you designate will be able to modify it. If you select “Private,” only you and your collaborators can view or modify it. Select “Add a README file.” This creates a template README.md file in your repository’s root folder. When you are happy with your repository name and configuration, click on the green “Create Repository” button. Now you should have a repository that looks something like this: 5.5 Creating and editing files on GitHub There are several ways to use the GitHub interface to add files to your repository and to edit them. Below we cover how to use the pen tool to edit existing files, and how to use the “Add file” drop down to create a new file or upload files from your computer. These techniques are useful for handling simple plaintext files, for example, the README.md file that is already present in the repository. 5.5.1 The pen tool The pen tool can be used to edit existing plaintext files. Click on the pen tool: Use the text box to make your changes: Finally, commit your changes. When you commit a file in a repository, the version control system takes a snapshot of what the file looks like. As you continue working on the project, over time you will possibly make many commits to a single file; this generates a useful version history for that file. On GitHub, if you click the green “Commit changes” button, it will save the file and then make a commit. Do this now: 5.5.2 The “Add file” menu The “Add file” menu can be used to create new plaintext files and upload files from your computer. To create a new plaintext file, click the “Add file” drop down menu and select the “Create new file” option: A page will open with a small text box for the file name to be entered, and a larger text box where the desired file content text can be entered. Note the two tabs, “Edit new file” and “Preview.” Toggling between them lets you enter and edit text and view what the text will look like when rendered, respectively. Note that GitHub understands and renders .md files using a markdown syntax very similar to Jupyter notebooks, so the “Preview” tab is especially helpful for checking markdown code correctness. Save and commit your changes by click the green “Commit changes” button at the bottom of the page. You can also upload files that you have created on your local machine by using the “Add file” drop down menu and selecting “Upload files”: To select the files from your local computer to upload, you can either drag and drop them into the grey box area shown below, or click the “choose your files” link to access a file browser dialog. Once the files you want to upload have been selected, click the green “Commit changes” button at the bottom of the page. Note that Git and GitHub are designed to track changes in individual files. Do not upload your whole project in an archive file (e.g. .zip), because then Git can only keep track of changes to the entire .zip file—that wouldn’t be very useful if you’re trying to see the history of changes to a single code file in your project! 5.6 Cloning your repository on JupyterHub Although there are several ways to create and edit files on GitHub, they are not quite powerful enough for efficiently creating and editing complex files, or files that need to be executed to assess whether they work (e.g., files containing code). Thus, it is useful to be able to connect the project repository that was created on GitHub to a coding environment. This can be done on your local computer, or using a JupyterHub; below we show how to do this using a JupyterHub. We need to clone our project’s Git repository to our JupyterHub—i.e., make a copy that knows where it was obtained from so that it knows where send/receive new committed edits. In order to do this, first copy the url from the HTTPS tab of the Code drop down menu on GitHub: Then open JupyterHub, and click the Git+ icon on the file browser tab: Paste the url of the GitHub project repository you created and click the blue “CLONE” button: On the file browser tab, you will now see a folder for your project’s repository (and inside it will be all the files that existed on GitHub): 5.7 Working in a cloned repository on JupyterHub Now that you have cloned your repository on JupyterHub, you can get to work editing, creating, and deleting files. Once you reach a point that you want Git to keep a record of the current version, you need to commit (i.e., snapshot) your changes. Then once you have made commits that you want to share with your collaborators, you need to push (i.e., send) those commits back to GitHub. Again, we can use the JupyterLab Git extension tool to do all of this. In particular, your workflow on JupyterHub should look like this: You edit, create, and delete files in your cloned repository on JupyterHub. Once you want a record of the current version, you specify which files to “add” to Git’s staging area. You can think of files in the staging area as those modified files for which you want a snapshot. You commit those flagged files to your repository, and include a helpful commit message to tell your collaborators about the changes you made. Note: here you are only committing to your cloned repository stored on JupyterHub. The repository on GitHub has not changed, and your collaborators cannot see your work yet. Go back to step 1. and keep working! When you want to store your commits (that only exist in your cloned repository right now) on the cloud where they can be shared with your collaborators, you push them back to the hosted repository on GitHub. Below we walk through how to use the Jupyter Git extension tool to do each of the steps outlined above. 5.7.1 Specifying files to commit Below we created and saved a new file (named eda.ipynb) that we would like to send back to the project repository on GitHub. To “add” this modified file to the staging area (i.e., flag that this is a file whose changes we would like to commit), we click the Jupyter Git extension icon on the far left-hand side of JupyterLab: This opens the Jupyter Git graphical user interface pane, and then we click the plus sign beside the file that we want to “add.” Note: because this is the first change for this file that we want to add, it falls under the “Untracked” heading. However, next time we edit this file and want to add the changes we made, we will find it under the “Changed” heading. Note: do not add the eda-checkpoint.ipynb file (sometimes called .ipynb_checkpoints). This file is automatically created by Jupyter when you work on eda.ipynb. You generally do not add auto-generated files to Git repositories, only add the files you directly create and edit. This moves the file from the “Untracked” heading to the “Staged” heading, flagging this file so that Git knows we want a snapshot of its current state as a commit. Now we are ready to “commit” the changes. Make sure to include a (clear and helpful!) message about what was changed so that your collaborators (and future you) know what happened in this commit. 5.7.2 Making the commit To snapshot the changes with an associated commit message, we put the message in the text box at the bottom of the Git pane and click on the blue “Commit” button. It is highly recommended to write useful and meaningful messages about what was changed. These commit messages, and the datetime stamp for a given commit, are the primary means to navigate through the project’s histry in the event that we need to view or retrieve a past version of a file, or revert our project to an earlier state. When you click the “Commit” button for the first time, you will be prompted to enter your name and email. This only needs to be done once for each machine you use Git on. After “commiting” the file(s), you will see there there are 0 “Staged” files and we are now ready to push our changes (and the attached commit message) to our project repository on GitHub: 5.7.3 Pushing the commits to GitHub To send the committed changes back to the project repository on GitHub, we need to push them. To do this we click on the cloud icon with the up arrow on the Jupyter Git tab: We will then be prompted to enter our GitHub username and password, and click the blue “OK” button: If the files were successfully pushed to our project repository on GitHub we will be given the success message shown below. Click “Dismiss” to continue working in Jupyter. You will see that the changes now exist there! 5.8 Collaboration 5.8.1 Giving collaborators access to your project As mentioned earlier, GitHub allows you to control who has access to your project. The default of both public and private projects are that only the person who created the GitHub repository has permissions to create, edit and delete files (write access). To give your collaborators write access to the projects, navigate to the “Settings” tab: Then click “Manage access”: Click the green “Invite a collaborator” button: Type in the collaborator’s GitHub username and select their name when it appears: Finally, click the green “Add to this repository” button: After this you should see your newly added collaborator listed under the “Manage access” tab. They should receive an email invitation to join the GitHub repository as a collaborator. They need to accept this invitation to enable write access. 5.8.2 Pulling changes from GitHub If your collaborators send their own commits to the GitHub repository, you will need to pull those changes to your own cloned copy on JupyterHub before you’re allowed to push any more changes yourself. By pulling their changes, you sync your local repository to what is present on GitHub. Note: you can still work on your own cloned repository and commit changes even if collaborators have pushed changes to the GitHub repository. It is only when you try to push your changes back to GitHub that Git will make sure nobody else has pushed any work in the meantime. You can do this using the Jupyter Git tab by clicking on the cloud icon with the down arrow: Once the files are successfully pulled from GitHub, you need to click “Dismiss” to keep working: And then when you open (or refresh) the files whose changes you just pulled, you should be able to see them: It can be very useful to review the history of the changes to your project. You can do this directly on the JupyterHub by clicking “History” in the Git tab. It is good practice to pull any changes at the start of every work session before you start working on your local copy. If you do not do this, and your collaborators have pushed some changes to the project to GitHub, then you will be unable to push your changes to GitHub until you pull. This situation can be recognized by this error message: Usually, getting out of this situation is not too troublesome. First you need to pull the changes that exist on GitHub that you do not yet have on your machine. Usually when this happens, Git can automatically merge the changes for you, even if you and your collaborators were working on different parts of the same file! If however, you and your collaborators made changes to the same line of the same file, Git will not be able to automatically merge the changes—it will not know whether to keep your version of the line(s), your collaborators version of the line(s), or some blend of the two. When this happens, Git will tell you that you have a merge conflict and that it needs human intervention (you!), and which file(s) this occurs in. 5.8.3 Handling merge conflicts To fix the merge conflict we need to open the file that had the merge conflict in a plain text editor and look for special marks that Git puts in the file to tell you where the merge conflict occurred. The beginning of the merge conflict is preceded by &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD and the end of the merge conflict is marked by &gt;&gt;&gt;&gt;&gt;&gt;&gt;. Between these markings, Git also inserts a separator (=======). The version of the change before the separator is your change, and the version that follows the separator was the change that existed on GitHub. Once you have decided which version of the change (or what combination!) to keep, you need to use the plain text editor to remove the special marks that Git added. The file must be saved, added to the staging area, and then committed before you will be able to push your changes to GitHub. 5.8.4 Communicating using GitHub issues When working on a project in a team, you don’t just want a historical record of who changed what file and when in the project—you also want a record of decisions that were made, ideas that were floated, problems that were identified and addressed, and all other communication surrounding the project. Email and messaging apps are both very popular for general communication, but are not designed for project-specific communication: they both generally do not have facilities for organizing conversations by project subtopics, searching for conversations related to particular bugs or software versions, etc. GitHub issues are an alternative written communication medium to email and messaging apps, and were designed specifically to facilitate project-specific communication. Issues are opened from the “Issues” tab on the project’s GitHub page, and they persist there even after the conversation is over and the issue is closed (in contrast to email, issues are not usually deleted). One issue thread is usually created per topic, and they are easily searchable using GitHub’s search tools. All issues are accessible to all project collaborators, so no one is left out of the conversation. Finally, issues can be setup so that team members get email notifications when a new issue is created or a new post is made in an issue thread. Replying to issues from email is also possible. Given all of these advantages, we highly recommend the use of issues for project-related communication. To open a GitHub issue, first click on the “Issues” tab: Next click the “New issue” button: Add an issue title (which acts like an email subject line), and then put the body of the message in the larger text box. Finally click “Submit new issue” to post the issue to share with others: You can reply to an issue that someone opened by adding your written response to the large text box and clicking comment: When a conversation is resolved, you can click “Close issue.” The closed issue can be later viewed be clicking the “Closed” header link in the “Issue” tab: 5.9 Additional resources This chapter focussed primarily on version control, but that is just one aspect of effective collaboration. If you want a more general guide for strategies on how to collaborate on a scientific project, read good enough practices in scientific computing (2014), which covers data management, project organization, collaborative writing, and more. After reading this chapter, you should have just enough knowledge of version control that you will find it to be a valuable tool in your projects. But we have just barely scratched the surface of what is possible with these tools. You can continue learning about version control using GitHub’s guides website, GitHub’s YouTube channel, and BitBucket’s tutorials. References "],["classification.html", "Chapter 6 Classification I: training &amp; predicting 6.1 Overview 6.2 Chapter learning objectives 6.3 The classification problem 6.4 Exploring a data set 6.5 Classification with K-nearest neighbours 6.6 K-nearest neighbours with tidymodels 6.7 Data preprocessing with tidymodels 6.8 Putting it together in a workflow", " Chapter 6 Classification I: training &amp; predicting 6.1 Overview In previous chapters, we focused solely on descriptive and exploratory data analysis questions. This chapter and the next together serve as our first foray into answering predictive questions about data. In particular, we will focus on classification, i.e., using one or more variables to predict the value of a categorical variable of interest. This chapter will cover the basics of classification, how to preprocess data to make it suitable for use in a classifier, and how to use our observed data to make predictions. The next will focus on how to evaluate how accurate the predictions from our classifier are, as well as how to improve our classifier (where possible) to maximize its accuracy. 6.2 Chapter learning objectives By the end of the chapter, students will be able to: Recognize situations where a classifier would be appropriate for making predictions Describe what a training data set is and how it is used in classification Interpret the output of a classifier Compute, by hand, the straight-line (Euclidean) distance between points on a graph when there are two predictor variables Explain the K-nearest neighbour classification algorithm Perform K-nearest neighbour classification in R using tidymodels Use a recipe to preprocess data to be centered, scaled, and balanced Combine preprocessing and model training using a Tidymodels workflow 6.3 The classification problem In many situations, we want to make predictions based on the current situation as well as past experiences. For instance, a doctor may want to diagnose a patient as either diseased or healthy based on their symptoms and the doctor’s past experience with patients; an email provider might want to tag a given email as “spam” or “not spam” based on the email’s text and past email text data; or a credit card company may want to predict whether a purchase is fraudulent based on the current purchase item, amount, and location as well as past purchases. These tasks are all examples of classification, i.e., predicting a categorical class (sometimes called a label) for an observation given its other variables (sometimes called features). Generally, a classifier assigns an observation without a known class (e.g. a new patient) to a class (e.g. diseased or healthy) on the basis of how similar it is to other observations for which we do know the class (e.g. previous patients with known diseases and symptoms). These observations with known classes that we use as a basis for prediction are called a training set; this name comes from the fact that we use these data to train, or teach, our classifier. Once taught, we can use the classifier to make predictions on new data for which we do not know the class. There are many possible methods that we could use to predict a categorical class/label for an observation. In this book we will focus on the simple, widely used K-nearest neighbours algorithm. In your future studies, you might encounter decision trees, support vector machines (SVMs), logistic regression, neural networks, and more; see the additional resources section at the end of the next chapter for where to begin learning more about these other methods. It is also worth mentioning that there are many variations on the basic classification problem. For example, we focus on the setting of binary classification where only two classes are involved (e.g., a diagnosis of either healthy or diseased), but you may also run into multiclass classification problems with more than two categories (e.g., a diagnosis of healthy, bronchitis, pneumonia, or a common cold). 6.4 Exploring a data set In this chapter and the next, we will study a data set of digitized breast cancer image features, created by Dr. William H. Wolberg, W. Nick Street, and Olvi L. Mangasarian at the University of Wisconsin, Madison. Each row in the data set represents an image of a tumour sample—such as the one shown in Figure 6.1—including the diagnosis (benign or malignant) and several other measurements (e.g., nucleus texture, perimeter, area, etc.). Diagnosis for each image was conducted by physicians. As with all data analyses, we first need to formulate a precise question that we want to answer. Here, the question is predictive: can we use the tumour image measurements available to us to predict whether a future tumour image (with unknown diagnosis) shows a benign or malignant tumour? Answering this question is important because traditional, non-data-driven methods for tumour diagnosis are quite subjective and dependent upon how skilled and experienced the diagnosing physician is. Furthermore, benign tumours are not normally dangerous; the cells stay in the same place and the tumour stops growing before it gets very large. By contrast, in malignant tumours, the cells invade the surrounding tissue and spread into nearby organs where they can cause serious damage (learn more about cancer here). Thus, it is important to quickly and accurately diagnose the tumour type to guide patient treatment. Figure 6.1: A malignant breast fine needle aspiration image. Source Loading the data Our first step is to load, wrangle, and explore the data using visualizations in order to better understand the data we are working with. We start by loading the necessary packages for our analysis. Below you’ll see (in addition to the usual tidyverse) a new package: forcats. The forcats package enables us to easily manipulate factors in R; factors are a special categorical type of variable in R that are often used for class label data. library(tidyverse) library(forcats) In this case, the file containing the breast cancer data set is a simple .csv file with headers. We’ll use the read_csv function with no additional arguments, and then inspect its contents: cancer &lt;- read_csv(&quot;data/wdbc.csv&quot;) cancer ## # A tibble: 569 x 12 ## ID Class Radius Texture Perimeter Area Smoothness Compactness Concavity ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 8.42e5 M 1.10 -2.07 1.27 0.984 1.57 3.28 2.65 ## 2 8.43e5 M 1.83 -0.353 1.68 1.91 -0.826 -0.487 -0.0238 ## 3 8.43e7 M 1.58 0.456 1.57 1.56 0.941 1.05 1.36 ## 4 8.43e7 M -0.768 0.254 -0.592 -0.764 3.28 3.40 1.91 ## 5 8.44e7 M 1.75 -1.15 1.78 1.82 0.280 0.539 1.37 ## 6 8.44e5 M -0.476 -0.835 -0.387 -0.505 2.24 1.24 0.866 ## 7 8.44e5 M 1.17 0.161 1.14 1.09 -0.123 0.0882 0.300 ## 8 8.45e7 M -0.118 0.358 -0.0728 -0.219 1.60 1.14 0.0610 ## 9 8.45e5 M -0.320 0.588 -0.184 -0.384 2.20 1.68 1.22 ## 10 8.45e7 M -0.473 1.10 -0.329 -0.509 1.58 2.56 1.74 ## # … with 559 more rows, and 3 more variables: Concave_Points &lt;dbl&gt;, ## # Symmetry &lt;dbl&gt;, Fractal_Dimension &lt;dbl&gt; Variable descriptions Breast tumours can be diagnosed by performing a biopsy, a process where tissue is removed from the body and examined for the presence of disease. Traditionally these procedures were quite invasive; modern methods such as fine needle asipiration, used to collect the present data set, extract only a small amount of tissue and are less invasive. After taking a digital image of the breast tissue sample—such as the one in Figure 6.1—10 different variables (numbers 3-12 below) were measured for each cell nucleus in the image, and then the mean for each variable across the nuclei was recorded. As part of the data preparation, these values have been scaled; we will discuss what this means and why we do it later in this chapter. Each image additionally was given a unique ID and a diagnosis for malignance by a physician. Therefore, the total set of variables per image in this data set is: ID number Class: the diagnosis of Malignant or Benign Radius: the mean of distances from center to points on the perimeter Texture: the standard deviation of gray-scale values Perimeter: the length of the surrounding contour Area: the area inside the contour Smoothness: the local variation in radius lengths Compactness: the ratio of squared perimeter and area Concavity: severity of concave portions of the contour Concave Points: the number of concave portions of the contour Symmetry: how similar the nucleus is when mirrored Fractal Dimension: a measurement of how “rough” the perimeter is Below we use glimpse to preview the data frame. This function can make it easier to inspect the data when we have a lot of columns: glimpse(cancer) ## Rows: 569 ## Columns: 12 ## $ ID &lt;dbl&gt; 842302, 842517, 84300903, 84348301, 84358402, 843786… ## $ Class &lt;chr&gt; &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M… ## $ Radius &lt;dbl&gt; 1.0960995, 1.8282120, 1.5784992, -0.7682333, 1.74875… ## $ Texture &lt;dbl&gt; -2.0715123, -0.3533215, 0.4557859, 0.2535091, -1.150… ## $ Perimeter &lt;dbl&gt; 1.26881726, 1.68447255, 1.56512598, -0.59216612, 1.7… ## $ Area &lt;dbl&gt; 0.98350952, 1.90703027, 1.55751319, -0.76379174, 1.8… ## $ Smoothness &lt;dbl&gt; 1.56708746, -0.82623545, 0.94138212, 3.28066684, 0.2… ## $ Compactness &lt;dbl&gt; 3.28062806, -0.48664348, 1.05199990, 3.39991742, 0.5… ## $ Concavity &lt;dbl&gt; 2.65054179, -0.02382489, 1.36227979, 1.91421287, 1.3… ## $ Concave_Points &lt;dbl&gt; 2.53024886, 0.54766227, 2.03543978, 1.45043113, 1.42… ## $ Symmetry &lt;dbl&gt; 2.215565542, 0.001391139, 0.938858720, 2.864862154, … ## $ Fractal_Dimension &lt;dbl&gt; 2.25376381, -0.86788881, -0.39765801, 4.90660199, -0… We can see from the summary of the data above that Class is of type character (denoted by &lt;chr&gt;). Since we are going to be working with Class as a categorical statistical variable, we will convert it to factor using the function as_factor. cancer &lt;- cancer %&gt;% mutate(Class = as_factor(Class)) glimpse(cancer) ## Rows: 569 ## Columns: 12 ## $ ID &lt;dbl&gt; 842302, 842517, 84300903, 84348301, 84358402, 843786… ## $ Class &lt;fct&gt; M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M… ## $ Radius &lt;dbl&gt; 1.0960995, 1.8282120, 1.5784992, -0.7682333, 1.74875… ## $ Texture &lt;dbl&gt; -2.0715123, -0.3533215, 0.4557859, 0.2535091, -1.150… ## $ Perimeter &lt;dbl&gt; 1.26881726, 1.68447255, 1.56512598, -0.59216612, 1.7… ## $ Area &lt;dbl&gt; 0.98350952, 1.90703027, 1.55751319, -0.76379174, 1.8… ## $ Smoothness &lt;dbl&gt; 1.56708746, -0.82623545, 0.94138212, 3.28066684, 0.2… ## $ Compactness &lt;dbl&gt; 3.28062806, -0.48664348, 1.05199990, 3.39991742, 0.5… ## $ Concavity &lt;dbl&gt; 2.65054179, -0.02382489, 1.36227979, 1.91421287, 1.3… ## $ Concave_Points &lt;dbl&gt; 2.53024886, 0.54766227, 2.03543978, 1.45043113, 1.42… ## $ Symmetry &lt;dbl&gt; 2.215565542, 0.001391139, 0.938858720, 2.864862154, … ## $ Fractal_Dimension &lt;dbl&gt; 2.25376381, -0.86788881, -0.39765801, 4.90660199, -0… Factors have what are called “levels,” which you can think of as categories. We can ask for the levels from the Class column by using the levels function. This function should return the name of each category in that column. Given that we only have 2 different values in our Class column (B and M), we only expect to get two names back. Note that the levels function requires a vector argument, while the select function outputs a data frame; so we use the pull function, which converts a single column of a data frame into a vector. cancer %&gt;% select(Class) %&gt;% pull() %&gt;% # turns a data frame into a vector levels() ## [1] &quot;M&quot; &quot;B&quot; Exploring the data Before we start doing any modelling, let’s explore our data set. Below we use the group_by and summarize functions we used before to see that we have 357 (63%) benign and 212 (37%) malignant tumour observations. num_obs &lt;- nrow(cancer) cancer %&gt;% group_by(Class) %&gt;% summarize( n = n(), percentage = n() / num_obs * 100 ) ## # A tibble: 2 x 3 ## Class n percentage ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 M 212 37.3 ## 2 B 357 62.7 Next, let’s draw a scatter plot to visualize the relationship between the perimeter and concavity variables. Rather than use ggplot's default palette, we define our own two colours here using hexadecimal colour codes—E69F00 for orange and 56B4E9 for light blue—and pass them as the values argument to the scale_color_manual function. We also make the category labels (“B” and “M”) more readable by changing them to “Benign” and “Malignant” using the labels argument. perim_concav &lt;- cancer %&gt;% ggplot(aes(x = Perimeter, y = Concavity, color = Class)) + geom_point(alpha = 0.5) + labs(color = &quot;Diagnosis&quot;) + scale_color_manual(labels = c(&quot;Malignant&quot;, &quot;Benign&quot;), values = c(&quot;#E69F00&quot;, &quot;#56B4E9&quot;)) perim_concav Figure 6.2: Scatter plot of concavity versus perimeter coloured by diagnosis label In Figure 6.2, we can see that malignant observations typically fall in the the upper right-hand corner of the plot area. By contrast, benign observations typically fall in lower left-hand corner of the plot. Suppose we obtain a new observation not in the current data set that has all the variables measured except the label (i.e., an image without the physician’s diagnosis for the tumour class), and we find a perimeter of 1 and concavity of 1. Could we use this information to classify that observation as benign or malignant? What about another new observation with perimeter value of -1 and concavity value of -0.5? And another with values of 0 and 1? It seems like the prediction of an unobserved label might be possible, based on our visualization. 6.5 Classification with K-nearest neighbours In order to actually make predictions for new observations in practice, we will need a classification algorithm. In this book we will use the K-nearest neighbours classification algorithm. To predict the label of a new observation (here, classify it as either benign or malignant), the K-nearest neighbours classifier generally finds the \\(K\\) “nearest” or “most similar” observations in our training set, and then uses their diagnoses to make a prediction for the new observation’s diagnosis. \\(K\\) is a number that we must choose in advance; for now, we will assume that someone has chosen \\(K\\) for us. We will cover how to choose \\(K\\) ourselves in the next chapter. To illustrate the concept of K-nearest neighbours classification, we will walk through an example. Suppose we have a new observation, with perimeter of 2 and concavity of 4, whose diagnosis “Class” is unknown. This new observation is depicted by the red scatter point in Figure 6.3. Figure 6.3: Scatter plot of concavity versus perimeter with new observation labelled in red Figure 6.4 shows that the nearest point to this new observation is malignant and located at the coordinates (2.1, 3.6). The idea here is that if a point is close to another in the scatter plot, then the perimeter and concavity values are similar, and so we may expect that they would have the same diagnosis. Figure 6.4: Scatter plot of concavity versus perimeter, with malignant nearest neighbour to a new observation highlighted Suppose we have another new observation with perimeter 0.2 and concavity of 3.3. Looking at the scatter plot in Figure 6.5, how would you classify this red observation? The nearest neighbour to this new point is a benign observation at (0.2, 2.7). Does this seem like the right prediction to make for the red observation? Probably not, if you consider the other nearby points… Figure 6.5: Scatter plot of concavity versus perimeter, with benign nearest neighbour to a new observation highlighted To improve the prediction we can consider several neighbouring points, say \\(K = 3\\), that are closest to the new red observation to predict its diagnosis class. Among those 3 closest points, we use the majority class as our prediction for the new observation. As shown in Figure 6.6, we see that the diagnoses of 2 of the 3 nearest neighbours to our new observation are malignant. Therefore we take majority vote and classify our new red observation as malignant. Figure 6.6: Scatter plot of concavity versus perimeter with three nearest neighbours Here we chose the \\(K=3\\) nearest observations, but there is nothing special about \\(K=3\\). We could have used \\(K=4, 5\\) or more (though we may want to choose an odd number to avoid ties). We will discuss more about choosing \\(K\\) in the next chapter. Distance between points We decide which points are the \\(K\\) “nearest” to our new observation using the straight-line distance (we will often just refer to this as distance). Suppose we have two observations \\(a\\) and \\(b\\), each having two predictor variables, \\(x\\) and \\(y\\). Denote \\(a_x\\) and \\(a_y\\) to be the values of variables \\(x\\) and \\(y\\) for observation \\(a\\); \\(b_x\\) and \\(b_y\\) have similar definitions for observaiton \\(b\\). Then the straight-line distance between observation \\(a\\) and \\(b\\) on the x-y plane can be computed using the following formula: \\[\\mathrm{Distance} = \\sqrt{(a_x -b_x)^2 + (a_y - b_y)^2}\\] To find the \\(K\\) nearest neighbours to our new observation, we compute the distance from that new observation to each observation in our training data, and select the \\(K\\) observations corresponding to the \\(K\\) smallest distance values. For example, suppose we want to use \\(K=5\\) neighbours to classify a new observation with perimeter of 0 and concavity of 3.5, shown in red in Figure 6.7. Let’s calculate the distances between our new point and each of the observations in the training set to find the \\(K=5\\) neighbours that are nearest to our new point. You will see in the mutate step below, we compute the straight-line distance using the formula above: we square the differences between the two observations’ perimeter and concavity coordinates, add the squared differences, and then take the square root. Figure 6.7: Scatter plot of concavity versus perimeter with new observation labelled in red new_obs_Perimeter &lt;- 0 new_obs_Concavity &lt;- 3.5 cancer %&gt;% select(ID, Perimeter, Concavity, Class) %&gt;% mutate(dist_from_new = sqrt((Perimeter - new_obs_Perimeter)^2 + (Concavity - new_obs_Concavity)^2)) %&gt;% arrange(dist_from_new) %&gt;% slice(1:5) # subset the first 5 rows ## # A tibble: 5 x 5 ## ID Perimeter Concavity Class dist_from_new ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 86409 0.241 2.65 B 0.881 ## 2 887181 0.750 2.87 M 0.980 ## 3 899667 0.623 2.54 M 1.14 ## 4 907914 0.417 2.31 M 1.26 ## 5 8710441 -1.16 4.04 B 1.28 In Table 6.1 we show in mathematical detail how the mutate step was used to compute the dist_from_new variable (the distance to the new observation) for each of the 5 nearest neighbours in the training data. Table 6.1: Evaluating the distances from the new observation to each of its 5 nearest neighbours Perimeter Concavity Distance Class 0.24 2.65 \\(\\sqrt{(0 - 0.24)^2 + (3.5 - 2.65)^2} = 0.88\\) B 0.75 2.87 \\(\\sqrt{(0 - 0.75)^2 + (3.5 - 2.87)^2} = 0.98\\) M 0.62 2.54 \\(\\sqrt{(0 - 0.62)^2 + (3.5 - 2.54)^2} = 1.14\\) M 0.42 2.31 \\(\\sqrt{(0 - 0.42)^2 + (3.5 - 2.31)^2} = 1.26\\) M -1.16 4.04 \\(\\sqrt{(0 - (-1.16))^2 + (3.5 - 4.04)^2} = 1.28\\) B The result of this computation shows that 3 of the 5 nearest neighbours to our new observation are malignant (M); since this is the majority, we classify our new observation as malignant. These 5 neighbours are circled in Figure 6.8. Figure 6.8: Scatter plot of concavity versus perimeter with 5 nearest neighbours circled More than two predictor variables Although the above description is directed toward two predictor variables, exactly the same K-nearest neighbour algorithm applies when you have a higher number of predictor variables (i.e., a higher-dimensional predictor space). Each predictor variable may give us new information to help create our classifier. The only difference is the formula for the distance between points. In particular, let’s say we have \\(m\\) predictor variables for two observations \\(a\\) and \\(b\\), i.e., \\(a = (a_{1}, a_{2}, \\dots, a_{m})\\) and \\(b = (b_{1}, b_{2}, \\dots, b_{m})\\). Previously, when we had two variables, we added up the squared difference between each of our (two) variables, and then took the square root. Now we will do the same, except for all of our \\(m\\) variables. In other words, the distance formula becomes \\[\\mathrm{Distance} = \\sqrt{(a_{1} -b_{1})^2 + (a_{2} - b_{2})^2 + \\dots + (a_{m} - b_{m})^2}.\\] This formula still corresponds to a straight-line distance, just in a space with more dimensions. For example, suppose we decide to use 3 predictor variables (so, a 3-dimensional space): perimeter, concavity, and symmetry. Figure 6.9 shows what the data look like when we visualize them as a 3-dimensional scatter. In this case, the formula above is just the straight line distance in this 3-dimensional space. Figure 6.9: 3D scatter plot of the symmetry, concavity, and perimeter variables. Click and drag the plot above to rotate it, and scroll to zoom. Note that in general we recommend against using 3D visualizations; here we show the data in 3D only to illustrate what “higher dimensions” and “nearest neighbours” look like, for learning purposes. Summary In order to classify a new observation using a K-nearest neighbour classifier, we have to: Compute the distance between the new observation and each observation in the training set. Sort the data table in ascending order according to the distances. Choose the top \\(K\\) rows of the sorted table. Classify the new observation based on a majority vote of the neighbour classes. 6.6 K-nearest neighbours with tidymodels Coding the K-nearest neighbours algorithm in R ourselves can get complicated, especially if we want to handle multiple classes, more than two variables, or predicting the class for multiple new observations. Thankfully, in R, the K-nearest neighbours algorithm is implemented in the parsnip package included in the tidymodels meta package, along with many other models that you will encounter in this and future classes. The tidymodels collection provides tools to help make and use models, such as classifiers. Using the packages in this collection will help keep our code simple, readable and accurate; the less we have to code ourselves, the fewer mistakes we are likely to make. We start off by loading tidymodels. library(tidymodels) Let’s walk through how to use tidymodels to perform K-nearest neighbours classification. We will use the cancer data set from above, using perimeter and concavity as predictors and \\(K = 5\\) neighbours to build our classifier. Then we will use the classifier to predict the diagnosis label for a new observation with perimeter 0, concavity 3.5, and an unknown diagnosis label. Let’s pick out our 2 desired predictor variables and class label and store it as a new dataset named cancer_train: cancer_train &lt;- cancer %&gt;% select(Class, Perimeter, Concavity) cancer_train ## # A tibble: 569 x 3 ## Class Perimeter Concavity ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 M 1.27 2.65 ## 2 M 1.68 -0.0238 ## 3 M 1.57 1.36 ## 4 M -0.592 1.91 ## 5 M 1.78 1.37 ## 6 M -0.387 0.866 ## 7 M 1.14 0.300 ## 8 M -0.0728 0.0610 ## 9 M -0.184 1.22 ## 10 M -0.329 1.74 ## # … with 559 more rows Next, we create a model specification for K-nearest neighbours classification by calling the nearest_neighbor function, specifying that we want to use \\(K = 5\\) neighbours (we will discuss how to choose \\(K\\) in the next chapter) and the straight-line distance (weight_func = \"rectangular\"). The weight_func argument controls how neighbours vote when classifying a new observation; by setting it to \"rectangular\", each of the \\(K\\) nearest neighbours gets exactly 1 vote as described above. Other choices, which weight each neighbour’s vote differently, can be found on the tidymodels website. We specify the particular computational engine (in this case, the kknn engine) for training the model with the set_engine function. Finally we specify that this is a classification problem with the set_mode function. knn_spec &lt;- nearest_neighbor(weight_func = &quot;rectangular&quot;, neighbors = 5) %&gt;% set_engine(&quot;kknn&quot;) %&gt;% set_mode(&quot;classification&quot;) knn_spec ## K-Nearest Neighbor Model Specification (classification) ## ## Main Arguments: ## neighbors = 5 ## weight_func = rectangular ## ## Computational engine: kknn In order to fit the model on the breast cancer data, we need to pass the model specification and the dataset to the fit function. We also need to specify what variables to use as predictors and what variable to use as the target. Below, the Class ~ Perimeter + Concavity argument specifies that Class is the target variable (the one we want to predict), and both Perimeter and Concavity are to be used as the predictors. knn_fit &lt;- knn_spec %&gt;% fit(Class ~ Perimeter + Concavity, data = cancer_train) We can also use a convenient shorthand syntax using a period, Class ~ ., to indicate that we want to use every variable except Class as a predictor in the model. In this particular setup, since Concavity and Perimeter are the only two predictors in the cancer_train data frame, Class ~ Perimeter + Concavity and Class ~ . are equivalent. In general, you can choose individual predictors using the + symbol, or you can specify to use all predictors using the . symbol. knn_fit &lt;- knn_spec %&gt;% fit(Class ~ ., data = cancer_train) knn_fit ## parsnip model object ## ## Fit time: 15ms ## ## Call: ## kknn::train.kknn(formula = Class ~ ., data = data, ks = min_rows(5, data, 5), kernel = ~&quot;rectangular&quot;) ## ## Type of response variable: nominal ## Minimal misclassification: 0.07557118 ## Best kernel: rectangular ## Best k: 5 Here you can see the final trained model summary. It confirms that the computational engine used to train the model was kknn::train.kknn. It also shows the fraction of errors made by the nearest neighbour model, but we will ignore this for now and discuss it in more detail in the next chapter. Finally it shows (somewhat confusingly) that the “best” weight function was “rectangular” and “best” setting of \\(K\\) was 5; but since we specified these earlier, R is just repeating those settings to us here. In the next chapter, we will actually let R tune the model for us. Finally, we make the prediction on the new observation by calling the predict function, passing both the fit object we just created and the new observation itself. As above when we ran the K-nearest neighbours classification algorithm manually, the knn_fit object classifies the new observation as malignant (“M”). Note that the predict function outputs a data frame with a single variable named .pred_class. new_obs &lt;- tibble(Perimeter = 0, Concavity = 3.5) predict(knn_fit, new_obs) ## # A tibble: 1 x 1 ## .pred_class ## &lt;fct&gt; ## 1 M 6.7 Data preprocessing with tidymodels 6.7.1 Centering and scaling When using K-nearest neighbour classification, the scale of each variable (i.e., its size and range of values) matters. Since the classifier predicts classes by identifying observations that are nearest to it, any variables that have a large scale will have a much larger effect than variables with a small scale. But just because a variable has a large scale doesn’t mean that it is more important for making accurate predictions. For example, suppose you have a data set with two attributes, salary (in dollars) and years of education, and you want to predict the corresponding type of job. When we compute the neighbour distances, a difference of $1000 is huge compared to a difference of 10 years of education. But for our conceptual understanding and answering of the problem, it’s the opposite; 10 years of education is huge compared to a difference of $1000 in yearly salary! In many other predictive models, the center of each variable (e.g., its mean) matters as well. For example, if we had a data set with a temperature variable measured in degrees Kelvin, and the same data set with temperature measured in degrees Celcius, the two variables would differ by a constant shift of 273 (even though they contain exactly the same information). Likewise in our hypothetical job classification example, we would likely see that the center of the salary variable is in the tens of thousands, while the center of the years of education variable is in the single digits. Although this doesn’t affect the K-nearest neighbour classification algorithm, this large shift can change the outcome of using many other predictive models. When all variables in a data set have a mean (center) of 0 and a standard deviation (scale) of 1, we say that the data have been standardized. To illustrate the effect that standardization can have on the K-nearest neighbour algorithm, we will read in the original, unscaled Wisconsin breast cancer data set; we have been using a standardized version of the data set up until now. To keep things simple, we will just use the Area, Smoothness, and Class variables: unscaled_cancer &lt;- read_csv(&quot;data/unscaled_wdbc.csv&quot;) %&gt;% mutate(Class = as_factor(Class)) %&gt;% select(Class, Area, Smoothness) unscaled_cancer ## # A tibble: 569 x 3 ## Class Area Smoothness ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 M 1001 0.118 ## 2 M 1326 0.0847 ## 3 M 1203 0.110 ## 4 M 386. 0.142 ## 5 M 1297 0.100 ## 6 M 477. 0.128 ## 7 M 1040 0.0946 ## 8 M 578. 0.119 ## 9 M 520. 0.127 ## 10 M 476. 0.119 ## # … with 559 more rows Looking at the unscaled / uncentered data above, you can see that the difference between the values for area measurements are much larger than those for smoothness, and the mean appears to be much larger too. Will this affect predictions? In order to find out, we will create a scatter plot of these two predictors (coloured by diagnosis) for both the unstandardized data we just loaded, and the standardized version of that same data. In the tidymodels framework, all data preprocessing happens using a recipe. Here we will initialize a recipe for the unscaled_cancer data above, specifying that the Class variable is the target, and all other variables are predictors: uc_recipe &lt;- recipe(Class ~ ., data = unscaled_cancer) print(uc_recipe) ## Data Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 2 So far, there is not much in the recipe; just a statement about the number of targets and predictors. Let’s add scaling (step_scale) and centering (step_center) steps for all of the predictors so that they each have a mean of 0 and standard deviation of 1. Note that tidyverse actually provides step_normalize, which does both centering and scaling in a single recipe step; in this book we will keep step_scale and step_center separate to emphasize conceptually that there are two steps happening. The prep function finalizes the recipe by using the data (here, unscaled_cancer) to compute anything necessary to run the recipe (in this case, the column means and standard deviations): uc_recipe &lt;- uc_recipe %&gt;% step_scale(all_predictors()) %&gt;% step_center(all_predictors()) %&gt;% prep() uc_recipe ## Data Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 2 ## ## Training data contained 569 data points and no missing data. ## ## Operations: ## ## Scaling for Area, Smoothness [trained] ## Centering for Area, Smoothness [trained] You can now see that the recipe includes a scaling and centering step for all predictor variables. Note that when you add a step to a recipe, you must specify what columns to apply the step to. Here we used the all_predictors() function to specify that each step should be applied to all predictor variables. However, there are a number of different arguments one could use here, as well as naming particular columns with the same syntax as the select function. For example: all_nominal() and all_numeric(): specify all categorical or all numeric variables all_predictors() and all_outcomes(): specify all predictor or all target variables Area, Smoothness: specify both the Area and Smoothness variable -Class: specify everything except the Class variable You can find a full set of all the steps and variable selection functions on the recipes home page. We finally use the bake function to apply the recipe. scaled_cancer &lt;- bake(uc_recipe, unscaled_cancer) scaled_cancer ## # A tibble: 569 x 3 ## Area Smoothness Class ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0.984 1.57 M ## 2 1.91 -0.826 M ## 3 1.56 0.941 M ## 4 -0.764 3.28 M ## 5 1.82 0.280 M ## 6 -0.505 2.24 M ## 7 1.09 -0.123 M ## 8 -0.219 1.60 M ## 9 -0.384 2.20 M ## 10 -0.509 1.58 M ## # … with 559 more rows At this point, you may wonder why we are doing so much work just to center and scale our variables. Can’t we just manually scale and center the Area and Smoothness variables ourselves before building our KNN model? Well, technically yes; but doing so is error-prone. In particular, we might accidentally forget to apply the same centering / scaling when making predictions, or accidentally apply a different centering / scaling than what we used while training. Proper use of a recipe helps keep our code simple, readable, and error-free. Furthermore, note that using prep and bake is required only when you want to inspect the result of the preprocessing steps yourself. You will see further on in Section 6.8 that tidymodels provides tools to automatically apply prep and bake as necessary without additional coding effort. Figure 6.10 shows the two scatter plots side-by-side—one for unscaled_cancer and one for scaled_cancer. Each has the same new observation annotated with its \\(K=3\\) nearest neighbours. In the plot for the nonstandardized original data, you can see some odd choices for the three nearest neighbours. In particular, the “neighbours” are visually well within the cloud of benign observations, and the neighbours are all nearly vertically aligned with the new observation (which is why it looks like there is only one black line on this plot). Here the computation of nearest neighbours is dominated by the much larger-scale area variable. On the right, the plot for standardized data shows a much more intuitively reasonable selection of nearest neighbours. Thus, standardizing the data can change things in an important way when we are using predictive algorithms. As a rule of thumb, standardizing your data should be a part of the preprocessing you do before any predictive modelling / analysis. Figure 6.10: Comparison of K = 3 nearest neighbours with standardized and unstandardized data 6.7.2 Balancing Another potential issue in a data set for a classifier is class imbalance, i.e., when one label is much more common than another. Since classifiers like the K-nearest neighbour algorithm use the labels of nearby points to predict the label of a new point, if there are many more data points with one label overall, the algorithm is more likely to pick that label in general (even if the “pattern” of data suggests otherwise). Class imbalance is actually quite a common and important problem: from rare disease diagnosis to malicious email detection, there are many cases in which the “important” class to identify (presence of disease, malicious email) is much rarer than the “unimportant” class (no disease, normal email). To better illustrate the problem, let’s revisit the breast cancer data; except now we will remove many of the observations of malignant tumours, simulating what the data would look like if the cancer was rare. We will do this by picking only 3 observations randomly from the malignant group, and keeping all of the benign observations. The new imbalanced data is shown in Figure 6.11. set.seed(3) rare_cancer &lt;- bind_rows( filter(cancer, Class == &quot;B&quot;), cancer %&gt;% filter(Class == &quot;M&quot;) %&gt;% sample_n(3) ) %&gt;% select(Class, Perimeter, Concavity) rare_plot &lt;- rare_cancer %&gt;% ggplot(aes(x = Perimeter, y = Concavity, color = Class)) + geom_point(alpha = 0.5) + labs(color = &quot;Diagnosis&quot;) + scale_color_manual(labels = c(&quot;Malignant&quot;, &quot;Benign&quot;), values = c(&quot;#E69F00&quot;, &quot;#56B4E9&quot;)) rare_plot Figure 6.11: Imbalanced data Note: You will see in the code above that we use the set.seed function. This is because we are using sample_n to artificially pick only 3 of the malignant tumour observations, which uses random sampling to choose which rows will be in the training set. In order to make the code reproducible, we use set.seed to specify where the random number generator starts for this process, which then guarantees the same result, i.e., the same choice of 3 observations, each time the code is run. In general, when your code involves random numbers, if you want the same result each time, you should use set.seed; if you want a different result each time, you should not. Suppose we now decided to use \\(K = 7\\) in K-nearest neighbour classification. With only 3 observations of malignant tumours, the classifier will always predict that the tumour is benign, no matter what its concavity and perimeter are! This is because in a majority vote of 7 observations, at most 3 will be malignant (we only have 3 total malignant observations), so at least 4 must be benign, and the benign vote will always win. For example, Figure 6.12 shows what happens for a new tumour observation that is quite close to three observations in the training data that were tagged as malignant. Figure 6.12: Imbalanced data with 7 nearest neighbours to a new observation highlighted Figure 6.13 shows what happens if we set the background colour of each area of the plot to the decision the K-nearest neighbour classifier would make. We can see that the decision is always “benign,” corresponding to the blue colour. Figure 6.13: Imbalanced data with background colour indicating the decision of the classifier Despite the simplicity of the problem, solving it in a statistically sound manner is actually fairly nuanced, and a careful treatment would require a lot more detail and mathematics than we will cover in this textbook. For the present purposes, it will suffice to rebalance the data by oversampling the rare class. In other words, we will replicate rare observations multiple times in our data set to give them more voting power in the K-nearest neighbour algorithm. In order to do this, we will add an oversampling step to the earlier uc_recipe recipe with the step_upsample function. We show below how to do this, and also use the group_by and summarize functions to see that our classes are now balanced: ups_recipe &lt;- recipe(Class ~ ., data = rare_cancer) %&gt;% step_upsample(Class, over_ratio = 1, skip = FALSE) %&gt;% prep() ups_recipe ## Data Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 2 ## ## Training data contained 360 data points and no missing data. ## ## Operations: ## ## Up-sampling based on Class [trained] upsampled_cancer &lt;- bake(ups_recipe, rare_cancer) upsampled_cancer %&gt;% group_by(Class) %&gt;% summarize(n = n()) ## # A tibble: 2 x 2 ## Class n ## &lt;fct&gt; &lt;int&gt; ## 1 M 357 ## 2 B 357 Now suppose we train our K-nearest neighbour classifier with \\(K=7\\) on this balanced data. Figure 6.14 shows what happens now when we set the background colour of each area of our scatter plot to the decision the K-nearest neighbour classifier would make. We can see that the decision is more reasonable; when the points are close to those labelled malignant, the classifier predicts a malignant tumour, and vice versa when they are closer to the benign tumour observations. Figure 6.14: Upsampled data with background colour indicating the decision of the classifier 6.8 Putting it together in a workflow The tidymodels package collection also provides the workflow, a simple way to chain together multiple data analysis steps without a lot of otherwise necessary code for intermediate steps. To illustrate the whole pipeline, let’s start from scratch with the unscaled_wdbc.csv data. First we will load the data, create a model, and specify a recipe for how the data should be preprocessed: # load the unscaled cancer data and make sure the target Class variable is a factor unscaled_cancer &lt;- read_csv(&quot;data/unscaled_wdbc.csv&quot;) %&gt;% mutate(Class = as_factor(Class)) # create the KNN model knn_spec &lt;- nearest_neighbor(weight_func = &quot;rectangular&quot;, neighbors = 7) %&gt;% set_engine(&quot;kknn&quot;) %&gt;% set_mode(&quot;classification&quot;) # create the centering / scaling recipe uc_recipe &lt;- recipe(Class ~ Area + Smoothness, data = unscaled_cancer) %&gt;% step_scale(all_predictors()) %&gt;% step_center(all_predictors()) Note that each of these steps is exactly the same as earlier, except for one major difference: we did not use the select function to extract the relevant variables from the data frame, and instead simply specified the relevant variables to use via the formula Class ~ Area + Smoothness (instead of Class ~ .) in the recipe. You will also notice that we did not call prep() on the recipe; this is unnecssary when it is placed in a workflow. We will now place these steps in a workflow using the add_recipe and add_model functions, and finally we will use the fit function to run the whole workflow on the unscaled_cancer data. Note another difference from earlier here: we do not include a formula in the fit function. This is again because we included the formula in the recipe, so there is no need to respecify it: knn_fit &lt;- workflow() %&gt;% add_recipe(uc_recipe) %&gt;% add_model(knn_spec) %&gt;% fit(data = unscaled_cancer) knn_fit ## ══ Workflow [trained] ══════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: nearest_neighbor() ## ## ── Preprocessor ──────────────────────────────────────────────────────────────── ## 2 Recipe Steps ## ## • step_scale() ## • step_center() ## ## ── Model ─────────────────────────────────────────────────────────────────────── ## ## Call: ## kknn::train.kknn(formula = ..y ~ ., data = data, ks = min_rows(7, data, 5), kernel = ~&quot;rectangular&quot;) ## ## Type of response variable: nominal ## Minimal misclassification: 0.112478 ## Best kernel: rectangular ## Best k: 7 As before, the fit object lists the function that trains the model as well as the “best” settings for the number of neighbours and weight function (for now, these are just the values we chose manually when we created knn_spec above). But now the fit object also includes information about the overall workflow, including the centering and scaling preprocessing steps. In other words, when we use the predict function with the knn_fit object to make a prediction for a new observation, it will first apply the same recipe steps to the new observation. As an example, we will predict the class label of a two new observations: one with Area = 500 and Smoothness = 0.075, and one with Area = 1500 and Smoothness = 0.1. new_observation &lt;- tibble(Area = c(500, 1500), Smoothness = c(0.075, 0.1)) prediction &lt;- predict(knn_fit, new_observation) prediction ## # A tibble: 2 x 1 ## .pred_class ## &lt;fct&gt; ## 1 B ## 2 M The classifier predicts that the first observation is benign (“B”), while the second is malignant (“M”). Figure 6.15 visualizes the predictions that this trained K-nearest neighbour model will make on a large range of new observations. Although you have seen coloured prediction map visualizations like this a few times now, we have not included the code to generate them, as it is a little bit complicated. For the interested reader who wants a learning challenge, we now include it below. The basic idea is to create a grid of synthetic new observations using the expand.grid function, predict the label of each, and visualize the predictions with a coloured scatter having a very high transparency (low alpha value) and large point radius. See if you can figure out what each line is doing! Understanding this code is not required for the remainder of the textbook. It is included for those readers who would like to use similar visualizations in their own data analyses. # create the grid of area/smoothness vals, and arrange in a data frame are_grid &lt;- seq(min(unscaled_cancer$Area), max(unscaled_cancer$Area), length.out = 100) smo_grid &lt;- seq(min(unscaled_cancer$Smoothness), max(unscaled_cancer$Smoothness), length.out = 100) asgrid &lt;- as_tibble(expand.grid(Area = are_grid, Smoothness = smo_grid)) # use the fit workflow to make predictions at the grid points knnPredGrid &lt;- predict(knn_fit, asgrid) # bind the predictions as a new column with the grid points prediction_table &lt;- bind_cols(knnPredGrid, asgrid) %&gt;% rename(Class = .pred_class) # plot: # 1. the coloured scatter of the original data # 2. the faded coloured scatter for the grid points wkflw_plot &lt;- ggplot() + geom_point(data = unscaled_cancer, mapping = aes(x = Area, y = Smoothness, color = Class), alpha = 0.75) + geom_point(data = prediction_table, mapping = aes(x = Area, y = Smoothness, color = Class), alpha = 0.02, size = 5.) + labs(color = &quot;Diagnosis&quot;) + scale_color_manual(labels = c(&quot;Malignant&quot;, &quot;Benign&quot;), values = c(&quot;#E69F00&quot;, &quot;#56B4E9&quot;)) Figure 6.15: Scatter plot of smoothness versus area where background colour indicates the decision of the classifier "],["classification-continued.html", "Chapter 7 Classification II: evaluation &amp; tuning 7.1 Overview 7.2 Chapter learning objectives 7.3 Evaluating accuracy 7.4 Tuning the classifier 7.5 Summary 7.6 Predictor variable selection 7.7 Additional resources", " Chapter 7 Classification II: evaluation &amp; tuning 7.1 Overview This chapter continues the introduction to predictive modelling through classification. While the previous chapter covered training and data preprocessing, this chapter focuses on how to evaluate the accuracy of a classifier, as well as how to improve the classifier (where possible) to maximize its accuracy. 7.2 Chapter learning objectives By the end of the chapter, students will be able to: Describe what training, validation, and test data sets are and how they are used in classification Split data into training, validation, and test data sets Evaluate classification accuracy in R using a validation data set and appropriate metrics Execute cross-validation in R to choose the number of neighbours in a K-nearest neighbour classifier Describe advantages and disadvantages of the K-nearest neighbour classification algorithm 7.3 Evaluating accuracy Sometimes our classifier might make the wrong prediction. A classifier does not need to be right 100% of the time to be useful, though we don’t want the classifier to make too many wrong predictions. How do we measure how “good” our classifier is? Let’s revisit the breast cancer images example and think about how our classifier will be used in practice. A biopsy will be performed on a new patient’s tumour, the resulting image will be analyzed, and the classifier will be asked to decide whether the tumour is benign or malignant. The key word here is new: our classifier is “good” if it provides accurate predictions on data not seen during training. But then how can we evaluate our classifier without having to visit the hospital to collect more tumour images? The trick is to split the data into a training set and test set (Figure 7.1) and use only the training set when building the classifier. Then to evaluate the accuracy of the classifier, we first set aside the true labels from the test set, and then use the classifier to predict the labels in the test set. If our predictions match the true labels for the observations in the test set, then we have some confidence that our classifier might also accurately predict the class labels for new observations without known class labels. Note: if there were a golden rule of machine learning, it might be this: you cannot use the test data to build the model! If you do, the model gets to “see” the test data in advance, making it look more accurate than it really is. Imagine how bad it would be to overestimate your classifier’s accuracy when predicting whether a patient’s tumour is malignant or benign! Figure 7.1: Splitting the data into training and testing sets How exactly can we assess how well our predictions match the true labels for the observations in the test set? One way we can do this is to calculate the prediction accuracy. This is the fraction of examples for which the classifier made the correct prediction. To calculate this we divide the number of correct predictions by the number of predictions made. This process is illustrated in Figure 7.2. Note that there are other measures for how well classifiers perform, such as precision and recall; these will not be discussed here, but you will likely encounter them in other more advanced courses on this topic. Figure 7.2: Process for splitting the data and finding the prediction accuracy In R, we can use the tidymodels library collection not only to perform K-nearest neighbour classification, but also to assess how well our classification worked. Let’s work through an example of this process using the breast cancer dataset. We start by loading the necessary libraries, reading in the breast cancer data from the previous chapter, and making a quick scatter plot visualization of tumour cell concavity versus smoothness coloured by diagnosis in Figure 7.3. # load packages library(tidyverse) library(tidymodels) # load data cancer &lt;- read_csv(&quot;data/unscaled_wdbc.csv&quot;) %&gt;% mutate(Class = as_factor(Class)) # convert the character Class variable to the factor datatype # create scatter plot of tumour cell concavity versus smoothness, # labelling the points be diagnosis class perim_concav &lt;- cancer %&gt;% ggplot(aes(x = Smoothness, y = Concavity, color = Class)) + geom_point(alpha = 0.5) + labs(color = &quot;Diagnosis&quot;) + scale_color_manual(labels = c(&quot;Malignant&quot;, &quot;Benign&quot;), values = c(&quot;#E69F00&quot;, &quot;#56B4E9&quot;)) perim_concav Figure 7.3: Scatterplot of tumour cell concavity versus smoothness coloured by diagnosis label Create the train / test split Once we have decided on a predictive question to answer and done some preliminary exploration, the very next thing to do is to split the data into the training and test sets. Typically, the training set is between 50 - 95% of the data, while the test set is the remaining 5 - 50%; the intuition is that you want to trade off between training an accurate model (by using a larger training data set) and getting an accurate evaluation of its performance (by using a larger test data set). Here, we will use 75% of the data for training, and 25% for testing. The initial_split function from tidymodels handles the procedure of splitting the data for us. It also takes two very important steps when splitting to ensure that the accuracy estimates from the test data are reasonable. First, it shuffles the data before splitting. This ensures that any ordering present in the data does not influence the data that ends up in the training and testing sets. Second, it stratifies the data by the class label, to ensure that roughly the same proportion of each class ends up in both the training and testing sets. For example, if roughly 65% of the observations are from the benign class (B) and 35% are from the malignant class (M), then we would hope that roughly 65% of the training data are benign, 35% are malignant, and the same proportions exist in the testing data. Let’s use the initial_split function to create the training and testing sets. We will specify that prop = 0.75 so that 75% of our original data set ends up in the training set. We will also set the strata argument to the categorical label variable (here, Class) to ensure that the training and validation subsets contain the right proportions of each category of observation. The training and testing functions then extract the training and testing data sets into two separate data frames. set.seed(1) cancer_split &lt;- initial_split(cancer, prop = 0.75, strata = Class) cancer_train &lt;- training(cancer_split) cancer_test &lt;- testing(cancer_split) Note: You will see in the code above that we use the set.seed function again, as discussed in the previous chapter. In this case it is because initial_split uses random sampling to choose which rows will be in the training set. Since we want our code to be reproducible and generate the same train/test split each time it is run, we use set.seed. glimpse(cancer_train) ## Rows: 426 ## Columns: 12 ## $ ID &lt;dbl&gt; 8510426, 8510653, 8510824, 857373, 857810, 858477, 8… ## $ Class &lt;fct&gt; B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B… ## $ Radius &lt;dbl&gt; 13.540, 13.080, 9.504, 13.640, 13.050, 8.618, 10.170… ## $ Texture &lt;dbl&gt; 14.36, 15.71, 12.44, 16.34, 19.31, 11.79, 14.88, 20.… ## $ Perimeter &lt;dbl&gt; 87.46, 85.63, 60.34, 87.21, 82.61, 54.34, 64.55, 54.… ## $ Area &lt;dbl&gt; 566.3, 520.0, 273.9, 571.8, 527.2, 224.5, 311.9, 221… ## $ Smoothness &lt;dbl&gt; 0.09779, 0.10750, 0.10240, 0.07685, 0.08060, 0.09752… ## $ Compactness &lt;dbl&gt; 0.08129, 0.12700, 0.06492, 0.06059, 0.03789, 0.05272… ## $ Concavity &lt;dbl&gt; 0.066640, 0.045680, 0.029560, 0.018570, 0.000692, 0.… ## $ Concave_Points &lt;dbl&gt; 0.047810, 0.031100, 0.020760, 0.017230, 0.004167, 0.… ## $ Symmetry &lt;dbl&gt; 0.1885, 0.1967, 0.1815, 0.1353, 0.1819, 0.1683, 0.27… ## $ Fractal_Dimension &lt;dbl&gt; 0.05766, 0.06811, 0.06905, 0.05953, 0.05501, 0.07187… glimpse(cancer_test) ## Rows: 143 ## Columns: 12 ## $ ID &lt;dbl&gt; 84501001, 846381, 84799002, 849014, 852763, 853401, … ## $ Class &lt;fct&gt; M, M, M, M, M, M, M, B, M, M, M, B, B, B, B, B, B, M… ## $ Radius &lt;dbl&gt; 12.460, 15.850, 14.540, 19.810, 14.580, 18.630, 16.7… ## $ Texture &lt;dbl&gt; 24.04, 23.95, 27.54, 22.15, 21.53, 25.11, 21.59, 18.… ## $ Perimeter &lt;dbl&gt; 83.97, 103.70, 96.73, 130.00, 97.41, 124.80, 110.10,… ## $ Area &lt;dbl&gt; 475.9, 782.7, 658.8, 1260.0, 644.8, 1088.0, 869.5, 5… ## $ Smoothness &lt;dbl&gt; 0.11860, 0.08401, 0.11390, 0.09831, 0.10540, 0.10640… ## $ Compactness &lt;dbl&gt; 0.23960, 0.10020, 0.15950, 0.10270, 0.18680, 0.18870… ## $ Concavity &lt;dbl&gt; 0.22730, 0.09938, 0.16390, 0.14790, 0.14250, 0.23190… ## $ Concave_Points &lt;dbl&gt; 0.085430, 0.053640, 0.073640, 0.094980, 0.087830, 0.… ## $ Symmetry &lt;dbl&gt; 0.2030, 0.1847, 0.2303, 0.1582, 0.2252, 0.2183, 0.18… ## $ Fractal_Dimension &lt;dbl&gt; 0.08243, 0.05338, 0.07077, 0.05395, 0.06924, 0.06197… We can see from glimpse in the code above that the training set contains 427 observations, while the test set contains 142 observations. This corresponds to a train / test split of 75% / 25%, as desired. Pre-process the data As we mentioned last chapter, KNN is sensitive to the scale of the predictors, and so we should perform some preprocessing to standardize them. An additional consideration we need to take when doing this is that we should create the standardization preprocessor using only the training data. This ensures that our test data does not influence any aspect of our model training. Once we have created the standardization preprocessor, we can then apply it separately to both the training and test data sets. Fortunately, the recipe framework from tidymodels makes it simple to handle this properly. Below we construct and prepare the recipe using only the training data (due to data = cancer_train in the first line). cancer_recipe &lt;- recipe(Class ~ Smoothness + Concavity, data = cancer_train) %&gt;% step_scale(all_predictors()) %&gt;% step_center(all_predictors()) Train the classifier Now that we have split our original data set into training and test sets, we can create our K-nearest neighbour classifier with only the training set using the technique we learned in the previous chapter. For now, we will just choose the number \\(K\\) of neighbours to be 3, and use concavity and smoothness as the predictors. As before we need to create a model specification, combine the model specification and recipe into a workflow, and then finally use fit with the training data cancer_train to build the classifier. set.seed(1) knn_spec &lt;- nearest_neighbor(weight_func = &quot;rectangular&quot;, neighbors = 3) %&gt;% set_engine(&quot;kknn&quot;) %&gt;% set_mode(&quot;classification&quot;) knn_fit &lt;- workflow() %&gt;% add_recipe(cancer_recipe) %&gt;% add_model(knn_spec) %&gt;% fit(data = cancer_train) knn_fit ## ══ Workflow [trained] ══════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: nearest_neighbor() ## ## ── Preprocessor ──────────────────────────────────────────────────────────────── ## 2 Recipe Steps ## ## • step_scale() ## • step_center() ## ## ── Model ─────────────────────────────────────────────────────────────────────── ## ## Call: ## kknn::train.kknn(formula = ..y ~ ., data = data, ks = min_rows(3, data, 5), kernel = ~&quot;rectangular&quot;) ## ## Type of response variable: nominal ## Minimal misclassification: 0.1150235 ## Best kernel: rectangular ## Best k: 3 Note: Here again you see the set.seed function. In the K-nearest neighbour algorithm, there is a tie for the majority neighbour class, the winner is randomly selected. Although there is no chance of a tie when \\(K\\) is odd (here \\(K=3\\)), it is possible that the code may be changed in the future to have an even value of \\(K\\). Thus, to prevent potential issues with reproducibility, we have set the seed. Note that in your own code, you only have to set the seed once at the beginning of your analysis. Predict the labels in the test set Now that we have a K-nearest neighbour classifier object, we can use it to predict the class labels for our test set. We use the bind_cols to add the column of predictions to the original test data, creating the cancer_test_predictions data frame. The Class variable contains the true diagnoses, while the .pred_class contains the predicted diagnoses from the classifier. cancer_test_predictions &lt;- predict(knn_fit, cancer_test) %&gt;% bind_cols(cancer_test) cancer_test_predictions ## # A tibble: 143 x 13 ## .pred_class ID Class Radius Texture Perimeter Area Smoothness ## &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 M 84501001 M 12.5 24.0 84.0 476. 0.119 ## 2 B 846381 M 15.8 24.0 104. 783. 0.0840 ## 3 M 84799002 M 14.5 27.5 96.7 659. 0.114 ## 4 M 849014 M 19.8 22.2 130 1260 0.0983 ## 5 M 852763 M 14.6 21.5 97.4 645. 0.105 ## 6 M 853401 M 18.6 25.1 125. 1088 0.106 ## 7 B 854253 M 16.7 21.6 110. 870. 0.0961 ## 8 B 854941 B 13.0 18.4 82.6 524. 0.0898 ## 9 M 855138 M 13.5 20.8 88.4 559. 0.102 ## 10 B 855167 M 13.4 21.6 86.2 563 0.0816 ## # … with 133 more rows, and 5 more variables: Compactness &lt;dbl&gt;, ## # Concavity &lt;dbl&gt;, Concave_Points &lt;dbl&gt;, Symmetry &lt;dbl&gt;, ## # Fractal_Dimension &lt;dbl&gt; Compute the accuracy Finally we can assess our classifier’s accuracy. To do this we use the metrics function from tidymodels to get the statistics about the quality of our model, specifying the truth and estimate arguments: cancer_test_predictions %&gt;% metrics(truth = Class, estimate = .pred_class) ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.860 ## 2 kap binary 0.691 In the metrics data frame we are interested in the accuracy row; looking at the value of the .estimate variable shows that the estimated accuracy of the classifier on the test data was 86%. The other entries involve more advanced metrics that are beyond the scope of this book. We can also look at the confusion matrix for the classifier, which shows the table of predicted labels and correct labels, using the conf_mat function: confusion &lt;- cancer_test_predictions %&gt;% conf_mat(truth = Class, estimate = .pred_class) confusion ## Truth ## Prediction M B ## M 39 6 ## B 14 84 This table shows that the classifier labelled 39+84 = 123 observations correctly. It also shows that the classifier made some mistakes; in particular, it classified 14 observations as benign when they were truly malignant, and 6 observations as malignant when they were truly benign. Critically analyze performance We now know that the classifier was 86% accurate on the test dataset. That sounds pretty good!…Wait, is it good? Or do we need something higher? In general, what a good value for accuracy is depends on the application. On a task of predicting whether a tumour is benign or malignant for a type of tumour that is benign 99% of the time, it is very easy to obtain a 99% accuracy just by guessing benign for every observation. In this case, 99% accuracy is probably not good enough. And beyond just accuracy, sometimes the kind of mistake the classifier makes is important as well. In the previous example, it might be very bad for the classifier to predict “benign” when the true class is “malignant,” as this might result in a patient not receiving appropriate medical attention. On the other hand, it might be less bad for the classifier to guess “malignant” when the true class is “benign,” as the patient will then likely see a doctor who can provide an expert diagnosis. This is why it is important not only to look at accuracy, but also the confusion matrix. However, there is always an easy baseline that you can compare to for any classification problem: the majority classifier. The majority classifier always guesses the majority class label from the training data, regardless of what values the predictor variables take. It helps to give you a sense for scale when considering accuracies. If the majority classifier obtains a 90% accuracy on a problem, then you might hope for your K-nearest neighbours classifier to do better than that. If your classifier provides a significant improvement upon the majority classifier, this means that at least your method is extracting some useful information from your predictor variables. Be careful though: improving on the majority classifier does not necessarily mean the classifier is working well enough for your application. As an example, in the breast cancer data, the proportions of benign and malignant observations in the training data are as follows: cancer_proportions &lt;- cancer_train %&gt;% group_by(Class) %&gt;% summarize(n = n()) %&gt;% mutate(percent = 100*n/nrow(cancer_train)) cancer_proportions ## # A tibble: 2 x 3 ## Class n percent ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 M 159 37.3 ## 2 B 267 62.7 Since the benign class represents the majority of the training data, the majority classifier would always predict that a new observation is benign. The estimated accuracy of the majority classifier is usually fairly close to the majority class proportion in the training data. In this case, we would suspect that the majority classifier will have an accuracy of around 63%. The K-nearest neighbours classifier we built does quite a bit better than this, with an accuracy of 86%. This means that from the perspective of accuracy, the K-nearest neighbours classifier improved quite a bit on the basic majority classifier. Hooray! But we still need to be cautious; in this application, it is likely very important not to miss-diagnose any malignant tumours to avoid missing patients who actually need medical care. The confusion matrix above shows that the classifier does indeed miss-diagnose a significant number of malignant tumours as benign (14 out of 53 malignant tumours, or 26%!). Therefore, even though the accuracy improved upon the majority classifier, our critical analysis suggests that this classifier may not have appropriate performance for the application. 7.4 Tuning the classifier The vast majority of predictive models in statistics and machine learning have parameters. A parameter is a number that you have to pick in advance that determines some aspect of how the model behaves. For example, in the K-nearest neighbours classification algorithm, \\(K\\) is a parameter that we have to pick that determines how many neighbours participate in the class vote. By picking different values of \\(K\\), we create different classifiers that make different predictions. So then how do we pick the best value of \\(K\\), i.e., tune the model? And is it possible to make this selection in a principled way? Ideally what we want is to somehow maximize the performance of our classifier on data it hasn’t seen yet. But we cannot use our test data set in the process of building our model. So we will play the same trick we did before when evaluating our classifier: we’ll split our training data itself into two subsets, use one to train the model, and then use the other to evaluate it. In this section we will cover the details of this procedure, as well as how to use it to help you pick good a parameter value for your classifier. Remember: don’t touch the test set during the tuning process. Tuning is a part of model training! 7.4.1 Cross-validation The first step in choosing the parameter \\(K\\) is to be able to evaluate the classifier using only the training data. If this is possible, then we can compare the classifier’s performance for different values of \\(K\\)—and pick the best—using only the training data. As suggested at the beginning of this section, we will accomplish this by splitting the training data, training on one subset, and evaluating on the other. The subset of training data used for evaluation is often called the validation set. There is, however, one key difference from the train/test split that we performed earlier. In particular, we were forced to make only a single split of the data. This is because at the end of the day, we have to produce a single classifier. If we had multiple different splits of the data into training and testing data, we would produce multiple different classifiers. But while we are tuning the classifier, we are free to create multiple classifiers based on multiple splits of the training data, evaluate them, and then choose a parameter value based on all of the different results. If we just split our overall training data once, our best parameter choice will depend strongly on whatever data was lucky enough to end up in the validation set. Perhaps using multiple different train/validation splits, we’ll get a better estimate of accuracy, which will lead to a better choice of the number of neighbours \\(K\\) for the overall set of training data. Let’s investigate this idea in R! In particular, we will use different seed values in the set.seed function to generate five different train/validation splits of our overall training data, train five different K-nearest neighbour models, and evaluate their accuracy. We will start with just a single split generated by using set.seed(1). set.seed(1) # choose a random split of training data # create the 25/75 split of the training data into training and validation cancer_split &lt;- initial_split(cancer_train, prop = 0.75, strata = Class) cancer_subtrain &lt;- training(cancer_split) cancer_validation &lt;- testing(cancer_split) # recreate the standardization recipe from before (since it must be based on the training data) cancer_recipe &lt;- recipe(Class ~ Smoothness + Concavity, data = cancer_subtrain) %&gt;% step_scale(all_predictors()) %&gt;% step_center(all_predictors()) # fit the knn model (we can reuse the old knn_spec model from before) knn_fit &lt;- workflow() %&gt;% add_recipe(cancer_recipe) %&gt;% add_model(knn_spec) %&gt;% fit(data = cancer_subtrain) # get predictions on the validation data validation_predicted &lt;- predict(knn_fit, cancer_validation) %&gt;% bind_cols(cancer_validation) # compute the accuracy acc &lt;- validation_predicted %&gt;% metrics(truth = Class, estimate = .pred_class) %&gt;% filter(.metric == &quot;accuracy&quot;) %&gt;% select(.estimate) %&gt;% pull() acc ## [1] 0.8878505 The accuracy estimate using the split based on set.seed(1) is 88.8%. Now we repeat the above code 4 more times, using set.seed(i) for i = 2, 3, 4, 5. With five different seeds, we get five different shuffles of the data, and therefore five different values for accuracy: 88.8%, 86.9%, 83.2%, 88.8%, 87.9%. None of these is necessarily “more correct” than any other; they’re just five estimates of the true, underlying accuracy of our classifier built using our overall training data. We can combine the estimates by taking their average (here 87%) to try to get a single assessment of our classifier’s accuracy; this has the effect of reducing the influence of any one (un)lucky validation set on the estimate. In practice, we don’t use random splits, but rather use a more structured splitting procedure so that each observation in the data set is used in a validation set only a single time. The name for this strategy is called cross-validation. In cross-validation, we split our overall training data into \\(C\\) evenly-sized chunks, and then iteratively use \\(1\\) chunk as the validation set and combine the remaining \\(C-1\\) chunks as the training set. This procedure is shown in Figure 7.4. Here, \\(C=5\\) different chunks of the data set are used, resulting in 5 different choices for the validation set; we call this 5-fold cross-validation. Figure 7.4: 5-fold cross validation To perform 5-fold cross-validation in R with tidymodels, we use another function: vfold_cv. This function splits our training data into v folds automatically. We set the strata argument to the categorical label variable (here, Class) to ensure that the training and validation subsets contain the right proportions of each category of observation. cancer_vfold &lt;- vfold_cv(cancer_train, v = 5, strata = Class) cancer_vfold ## # 5-fold cross-validation using stratification ## # A tibble: 5 x 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [340/86]&gt; Fold1 ## 2 &lt;split [340/86]&gt; Fold2 ## 3 &lt;split [341/85]&gt; Fold3 ## 4 &lt;split [341/85]&gt; Fold4 ## 5 &lt;split [342/84]&gt; Fold5 Then, when we create our data analysis workflow, we use the fit_resamples function instead of the fit function for training. This runs cross-validation on each train/validation split. Note: we set the seed when we call train not only because of the potential for ties, but also because we are doing cross-validation. Cross-validation uses a random process to select how to partition the training data. set.seed(1) # recreate the standardization recipe from before (since it must be based on the training data) cancer_recipe &lt;- recipe(Class ~ Smoothness + Concavity, data = cancer_train) %&gt;% step_scale(all_predictors()) %&gt;% step_center(all_predictors()) # fit the knn model (we can reuse the old knn_spec model from before) knn_fit &lt;- workflow() %&gt;% add_recipe(cancer_recipe) %&gt;% add_model(knn_spec) %&gt;% fit_resamples(resamples = cancer_vfold) knn_fit ## # Resampling results ## # 5-fold cross-validation using stratification ## # A tibble: 5 x 4 ## splits id .metrics .notes ## &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [340/86]&gt; Fold1 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 1]&gt; ## 2 &lt;split [340/86]&gt; Fold2 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 1]&gt; ## 3 &lt;split [341/85]&gt; Fold3 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 1]&gt; ## 4 &lt;split [341/85]&gt; Fold4 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 1]&gt; ## 5 &lt;split [342/84]&gt; Fold5 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 1]&gt; The collect_metrics function is used to aggregate the mean and standard error of the classifier’s validation accuracy across the folds. You will find results related to the accuracy in the row with accuracy listed under the .metric column. You should consider the mean (mean) to be the estimated accuracy, while the standard error (std_err) is a measure of how uncertain we are in the mean value. A detailed treatment of this is beyond the scope of this chapter; but roughly, if your estimated mean is 0.88 and standard error is 0.02, you can expect the true average accuracy of the classifier to be somewhere roughly between 86% and 90% (although it may fall outside this range). You may ignore the other columns in the metrics data frame, as they do not provide any additional insight. You can also ignore the entire second row with roc_auc in the .metric column, as it is beyond the scope of this book. knn_fit %&gt;% collect_metrics() ## # A tibble: 2 x 6 ## .metric .estimator mean n std_err .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 accuracy binary 0.883 5 0.0174 Preprocessor1_Model1 ## 2 roc_auc binary 0.924 5 0.0198 Preprocessor1_Model1 We can choose any number of folds, and typically the more we use the better our accuracy estimate will be (lower standard error). However, we are limited by computational power: the more folds we choose, the more computation it takes, and hence the more time it takes to run the analysis. So when you do cross-validation, you need to consider the size of the data, and the speed of the algorithm (e.g., K-nearest neighbour) and the speed of your computer. In practice, this is a trial and error process, but typically \\(C\\) is chosen to be either 5 or 10. Here we show how the standard error decreases when we use 10-fold cross validation rather than 5-fold: cancer_vfold &lt;- vfold_cv(cancer_train, v = 10, strata = Class) vfold_metrics &lt;- workflow() %&gt;% add_recipe(cancer_recipe) %&gt;% add_model(knn_spec) %&gt;% fit_resamples(resamples = cancer_vfold) %&gt;% collect_metrics() vfold_metrics ## # A tibble: 2 x 6 ## .metric .estimator mean n std_err .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 accuracy binary 0.882 10 0.0131 Preprocessor1_Model1 ## 2 roc_auc binary 0.919 10 0.0109 Preprocessor1_Model1 7.4.2 Parameter value selection Using 5- and 10-fold cross-validation, we have estimated that the prediction accuracy of our classifier is somewhere around 88%. Whether that is good or not depends entirely on the downstream application of the data analysis. In the present situation, we are trying to predict a tumour diagnosis, with expensive, damaging chemo/radiation therapy or patient death as potential consequences of misprediction. Hence, we might like to do better than 88% for this application. In order to improve our classifier, we have one choice of parameter: the number of neighbours, \\(K\\). Since cross-validation helps us evaluate the accuracy of our classifier, we can use cross-validation to calculate an accuracy for each value of \\(K\\) in a reasonable range, and then pick the value of \\(K\\) that gives us the best accuracy. The tidymodels package collection provides a very simple syntax for tuning models: each parameter in the model to be tuned should be specified as tune() in the model specification rather than given a particular value. knn_spec &lt;- nearest_neighbor(weight_func = &quot;rectangular&quot;, neighbors = tune()) %&gt;% set_engine(&quot;kknn&quot;) %&gt;% set_mode(&quot;classification&quot;) Then instead of using fit or fit_resamples, we will use the tune_grid function to fit the model for each value in a range of parameter values. In particular, we first create a data frame with a neighbors variable that contains the sequence of values of \\(K\\) to try; below we create the k_vals data frame with the neighbors variable containing each value from \\(K=1\\) to \\(K=15\\) using the seq function. Then we pass that data frame to the grid argument of tune_grid. We set the seed prior to tuning to ensure results are reproducible: set.seed(1) k_vals &lt;- tibble(neighbors = seq(from = 1, to = 15, by = 1)) knn_results &lt;- workflow() %&gt;% add_recipe(cancer_recipe) %&gt;% add_model(knn_spec) %&gt;% tune_grid(resamples = cancer_vfold, grid = k_vals) %&gt;% collect_metrics() knn_results ## # A tibble: 30 x 7 ## neighbors .metric .estimator mean n std_err .config ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 accuracy binary 0.861 10 0.0161 Preprocessor1_Model01 ## 2 1 roc_auc binary 0.855 10 0.0172 Preprocessor1_Model01 ## 3 2 accuracy binary 0.861 10 0.0161 Preprocessor1_Model02 ## 4 2 roc_auc binary 0.894 10 0.0123 Preprocessor1_Model02 ## 5 3 accuracy binary 0.882 10 0.0131 Preprocessor1_Model03 ## 6 3 roc_auc binary 0.919 10 0.0109 Preprocessor1_Model03 ## 7 4 accuracy binary 0.882 10 0.0131 Preprocessor1_Model04 ## 8 4 roc_auc binary 0.922 10 0.0113 Preprocessor1_Model04 ## 9 5 accuracy binary 0.873 10 0.0121 Preprocessor1_Model05 ## 10 5 roc_auc binary 0.928 10 0.00893 Preprocessor1_Model05 ## # … with 20 more rows We can select the best value of the number of neighbours (i.e., the one that results in the highest classifier accuracy estimate) by plotting the accuracy versus \\(K\\). accuracies &lt;- knn_results %&gt;% filter(.metric == &quot;accuracy&quot;) accuracy_vs_k &lt;- ggplot(accuracies, aes(x = neighbors, y = mean)) + geom_point() + geom_line() + labs(x = &quot;Neighbors&quot;, y = &quot;Accuracy Estimate&quot;) accuracy_vs_k Figure 7.5: Plot of estimated accuracy versus the number of neighbours Figure 7.5 suggests that setting the number of neighbours to \\(K =\\) 11 provides the highest accuracy. But as you can see, there is no exact or perfect answer here; any selection from \\(K = 3\\) and \\(15\\) would be reasonably justified, as all of these differ in classifier accuracy by a small amount. Remember: the values you see on this plot are estimates of the true accuracy of our classifier. Although the \\(K =\\) 11 value is higher than the others on this plot, that doesn’t mean the classifier is actually more accurate with this parameter value! Generally, when selecting \\(K\\) (and other parameters for other predictive models), we are looking for a value where: we get roughly optimal accuracy, so that our model will likely be accurate changing the value to a nearby one (e.g. adding or subtracting 1) doesn’t decrease accuracy too much, so that our choice is reliable in the presence of uncertainty the cost of training the model is not prohibitive (e.g., in our situation, if \\(K\\) is too large, predicting becomes expensive!) We know that \\(K =\\) 11 provides the highest estimated accuracy. Further, Figure 7.5 shows that the estimated accuracy changes by only a small amount if we increase or decrease \\(K\\) near \\(K =\\) 11. And finally, \\(K =\\) 11 does not create a prohibitively expensive computational cost of training. Considering these three points, we would indeed select \\(K =\\) 11 for the classifier. 7.4.3 Under/Overfitting To build a bit more intuition, what happens if we keep increasing the number of neighbours \\(K\\)? In fact, the accuracy actually starts to decrease! Let’s specify a much larger range of values of \\(K\\) to try in the grid argument of tune_grid. Figure 7.6 shows a plot of estimated accuracy as we vary \\(K\\) from 1 to almost the number of observations in the data set. set.seed(1) k_lots &lt;- tibble(neighbors = seq(from = 1, to = 385, by = 10)) knn_results &lt;- workflow() %&gt;% add_recipe(cancer_recipe) %&gt;% add_model(knn_spec) %&gt;% tune_grid(resamples = cancer_vfold, grid = k_lots) %&gt;% collect_metrics() accuracies &lt;- knn_results %&gt;% filter(.metric == &quot;accuracy&quot;) accuracy_vs_k_lots &lt;- ggplot(accuracies, aes(x = neighbors, y = mean)) + geom_point() + geom_line() + labs(x = &quot;Neighbors&quot;, y = &quot;Accuracy Estimate&quot;) accuracy_vs_k_lots Figure 7.6: Plot of accuracy estimate versus number of neighbours for many K values Underfitting: What is actually happening to our classifier that causes this? As we increase the number of neighbours, more and more of the training observations (and those that are farther and farther away from the point) get a “say” in what the class of a new observation is. This causes a sort of “averaging effect” to take place, making the boundary between where our classifier would predict a tumour to be malignant versus benign to smooth out and become simpler. If you take this to the extreme, setting \\(K\\) to the total training data set size, then the classifier will always predict the same label regardless of what the new observation looks like. In general, if the model isn’t influenced enough by the training data, it is said to underfit the data. Overfitting: In contrast, when we decrease the number of neighbours, each individual data point has a stronger and stronger vote regarding nearby points. Since the data themselves are noisy, this causes a more “jagged” boundary corresponding to a less simple model. If you take this case to the extreme, setting \\(K = 1\\), then the classifier is essentially just matching each new observation to its closest neighbour in the training data set. This is just as problematic as the large \\(K\\) case, because the classifier becomes unreliable on new data: if we had a different training set, the predictions would be completely different. In general, if the model is influenced too much by the training data, it is said to overfit the data. You can see these two effects in Figure 7.7, which shows how the classifier changes as we set the number of neighbours \\(K\\) to 1, 7, 20, and 300. Figure 7.7: Effect of K in overfitting and underfitting 7.5 Summary Classification algorithms use one or more quantitative variables to predict the value of another, categorical variable. The K-nearest neighbour algorithm in particular does this by first finding the \\(K\\) points in the training data nearest to the new observation, and then returning the majority class vote from those training observations. We can evaluate a classifier by splitting the data randomly into a training and test data set, using the training set to build the classifier, and using the test set to estimate its accuracy. Finally, we can tune the classifier (e.g., select the number of neighbours \\(K\\) in KNN) by maximizing estimated accuracy via cross-validation. This process is summarized in Figure 7.8. Figure 7.8: Overview of KNN classification The overall workflow for performing K-nearest neighbour classification using tidymodels is as follows: Use the initial_split function to split the data into a training and test set. Set the strata argument to the class label variable. Put the test set aside for now. Use the vfold_cv function to split up the training data for cross validation. Create a recipe that specifies the class label and predictors, as well as preprocessing steps for all variables. Pass the training data as the data argument of the recipe. Create a nearest_neighbors model specification, with neighbors = tune(). Add the recipe and model specification to a workflow(), and use the tune_grid function on the train/validation splits to estimate the classifier accuracy for a range of \\(K\\) values. Pick a value of \\(K\\) that yields a high accuracy estimate that doesn’t change much if you change \\(K\\) to a nearby value. Make a new model specification for the best parameter value (i.e., \\(K\\)), and retrain the classifier using the fit function. Evaluate the estimated accuracy of the classifier on the test set using the predict function. All algorithms have strengths and weaknesses. We summarize these for the K-nearest neighbours algorithm here. Strengths: K-nearest neighbours classification is a simple, intuitive algorithm requires few assumptions about what the data must look like works for binary (two-class) and multi-class (more than 2 classes) classification problems Weaknesses: K-nearest neighbours classification becomes very slow as the training data gets larger may not perform well with a large number of predictors may not perform well when classes are imbalanced 7.6 Predictor variable selection This section is not required reading for the remainder of the textbook. It is included for those readers interested in learning how irrelevant variables can influence the performance of a classifier, and how to pick a subset of useful variables to include as predictors. Another potentially important part of tuning your classifier is to choose which variables from your data will be treated as predictor variables. Technically, you can choose anything from using a single predictor variable to using every variable in your data; the K-nearest neighbours algorithm accepts any number of predictors. However, it is not the case that using more predictors always yields better predictions! In fact, sometimes including irrelevant predictors can actually negatively affect classifier performance. The effect of irrelevant predictors Let’s take a look at an example where K-nearest neighbours performs worse when given more predictors to work with. In this example we have modified the breast cancer data to have only the Smoothness, Concavity, and Perimeter variables from the original data, and then added irrelevant variables that we created ourselves using a random number generator. The irrelevant variables each take a value of 0 or 1 with equal probability for each observation, regardless of what the value Class variable takes. In other words, the irrelevant variables have no meaningful relationship with the Class variable. cancer_irrelevant %&gt;% select(Class, Smoothness, Concavity, Perimeter, Irrelevant1, Irrelevant2) ## # A tibble: 569 x 6 ## Class Smoothness Concavity Perimeter Irrelevant1 Irrelevant2 ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 M 0.118 0.300 123. 1 0 ## 2 M 0.0847 0.0869 133. 0 1 ## 3 M 0.110 0.197 130 1 0 ## 4 M 0.142 0.241 77.6 1 0 ## 5 M 0.100 0.198 135. 1 0 ## 6 M 0.128 0.158 82.6 0 0 ## 7 M 0.0946 0.113 120. 1 1 ## 8 M 0.119 0.0937 90.2 0 0 ## 9 M 0.127 0.186 87.5 0 0 ## 10 M 0.119 0.227 84.0 1 1 ## # … with 559 more rows Next, we build a sequence of KNN classifiers that include Smoothness, Concavity, and Perimeter as predictor variables, but also increasingly many irrelevant variables. In particular we create 6 datasets with 0, 5, 10, 15, 20, and 40 irrelevant predictors. Then we build a model, tuned via 5-fold cross-validation, for each data set. Figure 7.9 shows the estimated cross-validation accuracy versus the number of irrelevant predictors. As we add more irrelevant predictor variables, the estimated accuracy of our classifier decreases. This is because the irrelevant variables add a random amount to the distance between each pair of observations; the more irrelevant variables there are, the more (random) influence they have, and the more they corrupt the set of nearest neighbours that vote on the class of the new observation to predict. Figure 7.9: Effect of inclusion of irrelevant predictors Although the accuracy decreases as expected, one surprising thing about Figure 7.9 is that it shows that the method still outperforms the baseline majority classifier (with about 63% accuracy) even with 40 irrelevant variables. How could that be? Figure 7.10 provides the answer: the tuning procedure for the K-nearest neighbours classifier combats the extra randomness from the irrelevant variables by increasing the number of neighbours. Of course, because of all the extra noise in the data from the irrelevant variables, the number of neighbours does not increase smoothly; but the general trend is increasing. Figure 7.11 corroborates this evidence; if we fix the number of neighbours to \\(K=3\\), the accuracy falls off more quickly. Figure 7.10: Tuned number of neighbours for varying number of irrelevant predictors Figure 7.11: Accuracy versus number of irrelevant predictors for tuned and untuned number of neighbours Finding a good subset of predictors So then if it is not ideal to use all of our variables as predictors without consideration, how do we choose which variables we should use? A simple method is to rely on your scientific understanding of the data to tell you which variables are not likely to be useful predictors. For example, in the cancer data that we have been studying, the ID variable is just a unique identifier for the observation. As it is not related to any measured property of the cells, the ID variable should therefore not be used as a predictor. That is, of course, a very clear-cut case. But the decision for the remaining variables is less obvious, as all seem like reasonable candidates. It is not clear which subset of them will create the best classifier. One could use visualizations and other exploratory analyses to try to help understand which variables are potentially relevant, but this process is both time-consuming and error-prone when there are many variables to consider. We therefore need a more systematic and programmatic way of choosing variables. This is a very difficult problem to solve in general, and there are a number of methods that have been developed that apply in particular cases of interest. Here we will discuss two basic selection methods as an introduction to the topic. See the additional resources at the end of this chapter to find out where you can learn more about variable selection, including more advanced methods. The first idea you might think of for a systematic way to select predictors is to try all possible subsets of predictors and then pick the set that results in the “best” classifier. This procedure is indeed a well-known variable selection method referred to as best subset selection. In particular, you create a separate model for every possible subset of predictors tune each one using cross validation pick the subset of predictors that gives you the highest cross-validation accuracy Best subset selection is applicable to any classification method (KNN or otherwise). However, it becomes very slow when you have even a moderate number of predictors to choose from (say, around 10). This is because the number of possible predictor subsets grows very quickly with the number of predictors, and you have to train the model (itself a slow process!) for each one. For example, if we have \\(2\\) predictors—let’s call them A and B—then we have 3 variable sets to try: A alone, B alone, and finally A and B together. If we have \\(3\\) predictors—A, B, and C—then we have 7 to try: A, B, C, AB, BC, AC, and ABC. In general, the number of models we have to train for \\(m\\) predictors is \\(2^m-1\\); in other words, when we get to \\(10\\) predictors we have over one thousand models to train, and at \\(20\\) predictors we have over one million models to train! So although it is a simple method, best subset selection is usually too computationally expensive to use in practice. Another idea is to iteratively build up a model by adding one predictor variable at a time. This method—known as forward selection—is also widely applicable and fairly straightforward. It involves the following steps: start with a model having no predictors run the following 3 steps until you run out of predictors: for each unused predictor, add it to the model to form a candidate model tune all of the candidate models update the model to be the candidate model with the highest cross-validation accuracy select the model that provides the best trade-off between accuracy and simplicity Say you have \\(m\\) total predictors to work with. In the first iteration, you have to make \\(m\\) candidate models, each with 1 predictor. Then in the second iteration, you have to make \\(m-1\\) candidate models, each with 2 predictors (the one you chose before and a new one). This pattern continues for as many iterations as you want. If you run the method all the way until you run out of predictors to choose, you will end up training \\(\\frac{1}{2}m(m+1)\\) separate models. This is a big improvement from the \\(2^m-1\\) models that best subset selection requires you to train! For example, while best subset selection requires training over 1000 candidate models with \\(m=10\\) predictors, forward selection requires training only 55 candidate models. Therefore we will continue the rest of this section using forward selection. One word of caution before we move on. Every additional model that you train increases the likelihood that you will get unlucky and stumble on a model that has a high cross-validation accuracy estimate, but a low true accuracy on the test data and other future observations. Since forward selection involves training a lot of models, you run a fairly high risk of this happening. To keep this risk low, only use forward selection when you have a large amount of data and a relatively small total number of predictors. More advanced methods do not suffer from this problem as much; see the additional resources at the end of this chapter for where to learn more about advanced predictor selection methods. Forward selection in R We now turn to implementing forward selection in R. Unfortunately there is no built-in way to do this using the tidymodels framework, so we will have to code it ourselves. First we will use the select function to extract the “total” set of predictors that we are willing to work with. Here we will load the modified version of the cancer data with irrelevant predictors, and select Smoothness, Concavity, Perimeter, Irrelevant1, Irrelevant2, and Irrelevant3 as potential predictors, and the Class variable as the label. We will also extract the column names for the full set of predictor variables. set.seed(1) cancer_subset &lt;- cancer_irrelevant %&gt;% select(Class, Smoothness, Concavity, Perimeter, Irrelevant1, Irrelevant2, Irrelevant3) names &lt;- colnames(cancer_subset %&gt;% select(-Class)) cancer_subset ## # A tibble: 569 x 7 ## Class Smoothness Concavity Perimeter Irrelevant1 Irrelevant2 Irrelevant3 ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 M 0.118 0.300 123. 1 0 1 ## 2 M 0.0847 0.0869 133. 0 1 1 ## 3 M 0.110 0.197 130 1 0 0 ## 4 M 0.142 0.241 77.6 1 0 0 ## 5 M 0.100 0.198 135. 1 0 0 ## 6 M 0.128 0.158 82.6 0 0 0 ## 7 M 0.0946 0.113 120. 1 1 1 ## 8 M 0.119 0.0937 90.2 0 0 0 ## 9 M 0.127 0.186 87.5 0 0 1 ## 10 M 0.119 0.227 84.0 1 1 0 ## # … with 559 more rows The key idea of the forward selection code is to use the paste function (which concatenates strings separated by spaces) to create a model formula for each subset of predictors for which we want to build a model. The collapse argument tells paste what to put between the items in the list; to make a formula, we need to put a + symbol between each variable. As an example, let’s make a model formula for all the predictors, which should output something like Class ~ Smoothness + Concavity + Perimeter + Irrelevant1 + Irrelevant2 + Irrelevant3: example_formula &lt;- paste(&quot;Class&quot;, &quot;~&quot;, paste(names, collapse=&quot;+&quot;)) example_formula ## [1] &quot;Class ~ Smoothness+Concavity+Perimeter+Irrelevant1+Irrelevant2+Irrelevant3&quot; Finally, we need to write some code that performs the task of sequentially finding the best predictor to add to the model. If you recall the end of the wrangling chapter, we mentioned that sometimes one needs more flexible forms of iteration than what we have used earlier, and in these cases one typically resorts to a for loop. This is one of those cases! Here we will use two for loops: one over increasing predictor set sizes (where you see for (i in 1:length(names)) below), and another to check which predictor to add in each round (where you see for (j in 1:length(names)) below). For each set of predictors to try, we construct a model formula, pass it into a recipe, build a workflow that tunes a KNN classifier using 5-fold cross-validation, and finally records the estimated accuracy. set.seed(1) # create an empty tibble to store the results accuracies &lt;- tibble(size = integer(), model_string = character(), accuracy = numeric()) # create a model specification knn_spec &lt;- nearest_neighbor(weight_func = &quot;rectangular&quot;, neighbors = tune()) %&gt;% set_engine(&quot;kknn&quot;) %&gt;% set_mode(&quot;classification&quot;) # create a 5-fold cross-validation object cancer_vfold &lt;- vfold_cv(cancer_subset, v = 5, strata = Class) # store the total number of predictors n_total &lt;- length(names) # stores selected predictors selected &lt;- c() # for every size from 1 to the total number of predictors for (i in 1:n_total) { # for every predictor still not added yet accs &lt;- list() models &lt;- list() for (j in 1:length(names)) { # create a model string for this combination of predictors preds_new &lt;- c(selected, names[[j]]) model_string &lt;- paste(&quot;Class&quot;, &quot;~&quot;, paste(preds_new, collapse=&quot;+&quot;)) # create a recipe from the model string cancer_recipe &lt;- recipe(as.formula(model_string), data = cancer_subset) %&gt;% step_scale(all_predictors()) %&gt;% step_center(all_predictors()) # tune the KNN classifier with these predictors, and collect the accuracy for the best K acc &lt;- workflow() %&gt;% add_recipe(cancer_recipe) %&gt;% add_model(knn_spec) %&gt;% tune_grid(resamples = cancer_vfold, grid = 10) %&gt;% collect_metrics() %&gt;% filter(.metric == &quot;accuracy&quot;) %&gt;% summarize(mx = max(mean)) acc &lt;- acc$mx %&gt;% unlist() # add this result to the dataframe accs[[j]] &lt;- acc models[[j]] &lt;- model_string } jstar &lt;- which.max(unlist(accs)) accuracies &lt;- accuracies %&gt;% add_row(size = i, model_string = models[[jstar]], accuracy = accs[[jstar]]) selected &lt;- c(selected, names[[jstar]]) names &lt;- names[-jstar] } accuracies ## # A tibble: 6 x 3 ## size model_string accuracy ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Class ~ Perimeter 0.887 ## 2 2 Class ~ Perimeter+Concavity 0.912 ## 3 3 Class ~ Perimeter+Concavity+Smoothness 0.930 ## 4 4 Class ~ Perimeter+Concavity+Smoothness+Irrelevant1 0.932 ## 5 5 Class ~ Perimeter+Concavity+Smoothness+Irrelevant1+Irrelevant3 0.916 ## 6 6 Class ~ Perimeter+Concavity+Smoothness+Irrelevant1+Irrelevant3… 0.907 Interesting! The forward selection procedure first added the three meaningful variables Perimeter, Concavity, and Smoothness, followed by the irrelevant variables. Figure 7.12 visualizes the accuracy versus the number of predictors in the model. You can see that as meaningful predictors are added, the estimated accuracy increases substantially; and as you add irrelevant variables, the accuracy either exhibits small fluctuations or decreases as the model attempts to tune the number of neighbours to account for the extra noise. In order to pick the right model from the sequence, you have to balance high accuracy and model simplicity (i.e., having fewer predictors and a lower chance of overfitting). The way to find that balance is to look for the elbow in Figure 7.12, i.e., the place on the plot where the accuracy stops increasing dramatically and levels off or begins to decrease. The elbow in Figure 7.12 appears to occur at the model with 3 predictors; after that point the accuracy levels off. So here the right trade-off of accuracy and number of predictors occurs with 3 variables: Class ~ Perimeter + Concavity + Smoothness. In other words, we have successfully removed irrelevant predictors from the model! It is always worth remembering, however, that what cross validation gives you is an estimate of the true accuracy; you have to use your judgement when looking at this plot to decide where the elbow occurs, and whether adding a variable provides a meaningful increase in accuracy. Remember: since the choice of which variables to include as predictors is part of tuning your classifier, you cannot use your test data for this process! Figure 7.12: Estimated accuracy versus the number of predictors for the sequence of models built using forward selection 7.7 Additional resources The tidymodels website is an excellent reference for more details on, and advanced usage of, the functions and packages in the past two chapters. Aside from that, it also has a nice beginner’s tutorial and an extensive list of more advanced examples that you can use to continue learning beyond the scope of this book. It’s worth noting that the tidymodels package does a lot more than just classification, and so the examples on the website similarly go beyond classification as well. In the next two chapters, you’ll learn about another kind of predictive modelling setting, so it might be worth visiting the website only after reading through those chapters. An Introduction to Statistical Learning (2013) provides a great next stop in the process of learning about classification. Chapter 4 discusses additional basic techniques for classification that we do not cover, such as logistic regression, linear discriminant analysis, and naive Bayes. Chapter 5 goes into much more detail about cross-validation. Chapters 8 and 9 cover decision trees and support vector machines, two very popular but more advanced classification methods. Finally, Chapter 6 covers a number of methods for selecting predictor variables. Note that while this book is still a very accessible introductory text, it requires a bit more mathematical background than we require. References "],["regression1.html", "Chapter 8 Regression I: K-nearest neighbours 8.1 Overview 8.2 Chapter learning objectives 8.3 The regression problem 8.4 Exploring a data set 8.5 K-nearest neighbours regression 8.6 Training, evaluating, and tuning the model 8.7 Underfitting and overfitting 8.8 Evaluating on the test set 8.9 Strengths and limitations of KNN regression 8.10 Multivariate KNN regression", " Chapter 8 Regression I: K-nearest neighbours 8.1 Overview This chapter continues our foray into answering predictive questions. Here we will focus on regression, which is the process of predicting numerical variables. This is unlike the past two chapters, which focused on predicting categorical variables via classification. However, regression does have many similarities to classification: for example, just as in the case of classification, we will split our data into training, validation, and test sets, we will use tidymodels workflows, we will use a K-nearest neighbours (KNN) approach to make predictions, and we will use cross-validation to choose K. Because of how similar these procedures are, make sure to read Chapters 6 and 7 before reading this one—we will move a little bit faster here with the concepts that have already been covered. This chapter will primarily focus on the case where there is a single predictor, but the end of the chapter shows how to perform regression with more than one predictor variable, i.e., multivariate regression. 8.2 Chapter learning objectives By the end of the chapter, students will be able to: Recognize situations where a simple regression analysis would be appropriate for making predictions. Explain the K-nearest neighbour (KNN) regression algorithm and describe how it differs from KNN classification. Interpret the output of a KNN regression. In a dataset with two or more variables, perform K-nearest neighbour regression in R using a tidymodels workflow Execute cross-validation in R to choose the number of neighbours. Evaluate KNN regression prediction accuracy in R using a test data set and an appropriate metric (e.g., root means square prediction error). In the context of KNN regression, compare and contrast goodness of fit and prediction properties (namely RMSE vs RMSPE). Describe the advantages and disadvantages of the K-nearest neighbour regression approach. 8.3 The regression problem Regression, like classification, is a predictive problem setting where we want to use past information to predict future observations. But in the case of regression, the goal is to predict numerical values instead of categorical values. The variable that you want to predict is often called the response variable. For example, we could try to use the number of hours a person spends on exercise each week to predict their race time in the annual Boston marathon. As another example, we could try to use the size of a house to predict its sale price. Both of these response variables—race time and sale price—are numerical, and so predicting them given past data is considered a regression problem. Just like in the classification setting, there are many possible methods that we can use to predict numerical response variables. In this chapter we will focus on the K-nearest neighbours algorithm, and in the next chapter we will study linear regression. In your future studies, you might encounter regression trees, splines, and general local regression methods; see the additional resources section at the end of the next chapter for where to begin learning more about these other methods. Many of the concepts from classification map over to the setting of regression. For example, a regression model predicts a new observation’s response variable based on the response variables for similar observations in the data set of past observations. When building a regression model, we first split the data into training and test sets, in order to ensure that we assess the performance of our method on observations not seen during training. And finally, we can use cross-validation to evaluate different choices of model parameters (e.g., K in a K-nearest neighbours model). The major difference is that we are now predicting numerical variables instead of categorical variables. You can usually tell whether a variable is numerical or categorical—and therefore whether you need to perform regression or classification—by taking two response variables X and Y from your data, and asking the question “is response variable X more than response variable Y?” If the variable is categorical, the question will make no sense (“is blue more than red?” or “is benign more than malignant?”). If the variable is numerical, it will make sense (“is 1.5 hours more than 2.25 hours?” or “is $500,000 more than $400,000?”). Be careful when applying this heuristic, though: sometimes a categorical variables will be encoded as numbers in your data (e.g. “1” represents “benign,” and “0” represents “malignant”). In these cases you have to ask the question about the meaning of the labels (“benign” and “malignant”), not their values (“1” and “0”). 8.4 Exploring a data set In this chapter and the next, we will study the Sacramento real estate data set. This data set contains 932 real estate transactions in Sacramento, California originally reported in the Sacramento Bee newspaper. We first need to formulate a precise question that we want to answer. In this example, our question is again predictive: can we use the size of a house in the Sacramento, CA area to predict its sale price? A rigorous, quantitative answer to this question might help a realtor advise a client as to whether the price of a particular listing is fair, or perhaps how to set the price of a new listing. We begin the analysis by loading and examining the data. library(tidyverse) library(tidymodels) library(gridExtra) set.seed(5) sacramento &lt;- read_csv(&quot;data/sacramento.csv&quot;) sacramento ## # A tibble: 932 x 9 ## city zip beds baths sqft type price latitude longitude ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 SACRAMENTO z95838 2 1 836 Residential 59222 38.6 -121. ## 2 SACRAMENTO z95823 3 1 1167 Residential 68212 38.5 -121. ## 3 SACRAMENTO z95815 2 1 796 Residential 68880 38.6 -121. ## 4 SACRAMENTO z95815 2 1 852 Residential 69307 38.6 -121. ## 5 SACRAMENTO z95824 2 1 797 Residential 81900 38.5 -121. ## 6 SACRAMENTO z95841 3 1 1122 Condo 89921 38.7 -121. ## 7 SACRAMENTO z95842 3 2 1104 Residential 90895 38.7 -121. ## 8 SACRAMENTO z95820 3 1 1177 Residential 91002 38.5 -121. ## 9 RANCHO_CORDOVA z95670 2 2 941 Condo 94905 38.6 -121. ## 10 RIO_LINDA z95673 3 2 1146 Residential 98937 38.7 -121. ## # … with 922 more rows The scientific question guides our initial exploration: the columns in the data that we are interested in are sqft (house size, in livable square feet) and price (house sale price, in US dollars (USD)). The first step is to visualize the data as a scatter plot where we place the predictor variable (house size) on the x-axis, and we place the target/response variable that we want to predict (sale price) on the y-axis. eda &lt;- ggplot(sacramento, aes(x = sqft, y = price)) + geom_point(alpha = 0.4) + xlab(&quot;House size (square feet)&quot;) + ylab(&quot;Price (USD)&quot;) + scale_y_continuous(labels = dollar_format()) eda Figure 8.1: Scatter plot of price (USD) versus house size (square feet) The plot is shown in Figure 8.1. We can see that in Sacramento, CA, as the size of a house increases, so does its sale price. Thus, we can reason that we may be able to use the size of a not-yet-sold house (for which we don’t know the sale price) to predict its final sale price. Note that we do not suggest here that a larger house size causes a higher sale price; just that house price tends to increase with house size, and that we may be able to use the latter to predict the former. 8.5 K-nearest neighbours regression Much like in the case of classification, we can use a K-nearest neighbours-based approach in regression to make predictions. Let’s take a small sample of the data above and walk through how K-nearest neighbours (knn) works in a regression context before we dive in to creating our model and assessing how well it predicts house sale price. This subsample is taken to allow us to illustrate the mechanics of KNN regression with a few data points; later in this chapter we will use all the data. To take a small random sample of size 30, we’ll use the function sample_n. This function takes two arguments: tbl (a data frame-like object to sample from) size (the number of observations/rows to be randomly selected/sampled) small_sacramento &lt;- sample_n(sacramento, size = 30) Next let’s say we come across a 2,000 square-foot house in Sacramento we are interested in purchasing, with an advertised list price of $350,000. Should we offer to pay the asking price for this house, or is it overpriced and we should offer less? Absent any other information, we can get a sense for a good answer to this question by using the data we have to predict the sale price given the sale prices we have already observed. But in Figure (fig:07-small-eda-regr), you can see that we have no observations of a house of size exactly 2,000 square feet. How can we predict the sale price? small_plot &lt;- ggplot(small_sacramento, aes(x = sqft, y = price)) + geom_point() + xlab(&quot;House size (square feet)&quot;) + ylab(&quot;Price (USD)&quot;) + scale_y_continuous(labels = dollar_format()) + geom_vline(xintercept = 2000, linetype = &quot;dotted&quot;) small_plot Figure 8.2: Scatter plot of price (USD) versus house size (square feet) with vertical line indicating 2,000 square feet on x-axis We will employ the same intuition from the classification chapter, and use the neighbouring points to the new point of interest to suggest/predict what its sale price might be. For the example above, we find and label the 5 nearest neighbours to our observation of a house that is 2,000 square feet. nearest_neighbours &lt;- small_sacramento %&gt;% mutate(diff = abs(2000 - sqft)) %&gt;% arrange(diff) %&gt;% slice(1:5) #subset the first 5 rows nearest_neighbours ## # A tibble: 5 x 10 ## city zip beds baths sqft type price latitude longitude diff ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ROSEVILLE z957… 3 2.5 1829 Resident… 306500 38.8 -121. 171 ## 2 SACRAMENTO z958… 3 2 1828 Resident… 250000 38.6 -121. 172 ## 3 RANCHO_CORD… z956… 4 3 2208 Resident… 336000 38.6 -121. 208 ## 4 PLACERVILLE z956… 3 2 1704 Resident… 475000 38.7 -121. 296 ## 5 RIO_LINDA z956… 2 2 1690 Resident… 136500 38.7 -121. 310 Figure 8.3: Scatter plot of price (USD) versus house size (square feet) with lines to 5 nearest neighbours Figure 8.3 illustrates the difference between the house sizes of the 5 nearest neighbours (in terms of house size) to our new 2,000 square-foot house of interest. Now that we have obtained these nearest neighbours, we can use their values to predict the sale price for the new home. Specifically, we can take the mean (or average) of these 5 values as our predicted value, as illustrated by the red point in Figure 8.4. prediction &lt;- nearest_neighbours %&gt;% summarise(predicted = mean(price)) prediction ## # A tibble: 1 x 1 ## predicted ## &lt;dbl&gt; ## 1 300800 Figure 8.4: Scatter plot of price (USD) versus house size (square feet) with predicted price for a 2,000 square-foot house based on 5 nearest neighbours represented as a red dot Our predicted price is $300,800 (shown as a red point in Figure 8.4), which is much less than $350,000; perhaps we might want to offer less than the list price at which the house is advertised. But this is only the very beginning of the story. We still have all the same unanswered questions here with KNN regression that we had with KNN classification: which \\(K\\) do we choose, and is our model any good at making predictions? In the next few sections, we will address these questions in the context of KNN regression. 8.6 Training, evaluating, and tuning the model As usual, we must start by putting some test data away in a lock box that we will come back to only after we choose our final model. Let’s take care of that now. Note that for the remainder of the chapter we’ll be working with the entire Sacramento data set, as opposed to the smaller sample of 30 points above. sacramento_split &lt;- initial_split(sacramento, prop = 0.75, strata = price) sacramento_train &lt;- training(sacramento_split) sacramento_test &lt;- testing(sacramento_split) Next, we’ll use cross-validation to choose \\(K\\). In KNN classification, we used accuracy to see how well our predictions matched the true labels. We cannot use the same metric in the regression setting, since our predictions will almost never exactly match the true response variable values. Therefore in the context of KNN regression we will use root mean square prediction error (RMSPE) instead. The mathematical formula for calculating RMSPE is: \\[\\text{RMSPE} = \\sqrt{\\frac{1}{n}\\sum\\limits_{i=1}^{n}(y_i - \\hat{y}_i)^2}\\] where: \\(n\\) is the number of observations \\(y_i\\) is the observed value for the \\(i^\\text{th}\\) observation \\(\\hat{y}_i\\) is the forcasted/predicted value for the \\(i^\\text{th}\\) observation In other words, we compute the squared difference between the predicted and true response value for each observation in our test (or validation) set, compute the average, and then finally take the square root. The reason we use the squared difference (and not just the difference) is that the differences can be positive or negative, i.e., we can overshoot or undershoot the true response value. Figure 8.5 illustrates both positive and negative differences between predicted and true response values. So if we want to measure error—a notion of distance between our predicted and true response values—we want to make sure that we are only adding up positive values, with larger positive values representing larger mistakes. If the predictions are very close to the true values, then RMSPE will be small. If, on the other-hand, the predictions are very different to the true values, then RMSPE will be quite large. When we use cross validation, we will choose the \\(K\\) that gives us the smallest RMSPE. Figure 8.5: Scatter plot of price (USD) versus house size (square feet) with example predictions (blue line) and the error in those predictions compared with true response values for three selected observations (vertical red lines). RMSPE versus RMSE When using many code packages (tidymodels included), the evaluation output we will get to assess the prediction quality of our KNN regression models is labelled “RMSE,” or “root mean squared error.” Why is this so, and why not just RMSPE? In statistics, we try to be very precise with our language to indicate whether we are calculating the prediction error on the training data (in-sample prediction) versus on the testing data (out-of-sample prediction). When predicting and evaluating prediction quality on the training data, we say RMSE. By contrast, when predicting and evaluating prediction quality on the testing or validation data, we say RMSPE. The equation for calculating RMSE and RMSPE is exactly the same; all that changes is whether the \\(y\\)s are training or testing data. But many people just use RMSE for both, and rely on context to denote which data the root mean squared error is being calculated on. Now that we know how we can assess how well our model predicts a numerical value, let’s use R to perform cross-validation and to choose the optimal \\(K\\). First, we will create a recipe for preprocessing our data. Note that we include standardization in our preprocessing to build good habits, but since we only have one predictor it is technically not necessary; there is no risk of comparing two predictors of different scales. Next we create a model specification for K-nearest neighbours regression. Note that we use set_mode(\"regression\") now in the model specification to denote a regression problem, as opposed to the classification problems from the previous chapters. The use of set_mode(\"regression\") essentially tells tidymodels that we need to use different metrics (RMSPE, not accuracy) for tuning and evaluation. Then we create a 5-fold cross validation object, and put the recipe and model specification together in a workflow. sacr_recipe &lt;- recipe(price ~ sqft, data = sacramento_train) %&gt;% step_scale(all_predictors()) %&gt;% step_center(all_predictors()) sacr_spec &lt;- nearest_neighbor(weight_func = &quot;rectangular&quot;, neighbors = tune()) %&gt;% set_engine(&quot;kknn&quot;) %&gt;% set_mode(&quot;regression&quot;) sacr_vfold &lt;- vfold_cv(sacramento_train, v = 5, strata = price) sacr_wkflw &lt;- workflow() %&gt;% add_recipe(sacr_recipe) %&gt;% add_model(sacr_spec) sacr_wkflw ## ══ Workflow ════════════════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: nearest_neighbor() ## ## ── Preprocessor ──────────────────────────────────────────────────────────────── ## 2 Recipe Steps ## ## • step_scale() ## • step_center() ## ## ── Model ─────────────────────────────────────────────────────────────────────── ## K-Nearest Neighbor Model Specification (regression) ## ## Main Arguments: ## neighbors = tune() ## weight_func = rectangular ## ## Computational engine: kknn Next we run cross validation for a grid of numbers of neighbours ranging from 1 to 200. The following code tunes the model and returns the RMSPE for each number of neighbours. In the below results data frame, the neighbors variable contains the value of \\(K\\), the mean (mean) contains the value of the RMSPE estimated via cross-validation, and the standard error (std_err) is a measure of how uncertain we are in the mean value. A detailed treatment of this is beyond the scope of this chapter; but roughly, if your estimated mean is 100,000 and standard error is 1,000, you can expect the true RMSPE to be somewhere roughly between 99,000 and 101,000 (although it may fall outside this range). You may ignore the other columns in the metrics data frame, as they do not provide any additional insight. gridvals &lt;- tibble(neighbors = seq(from = 1, to = 200, by = 3)) sacr_results &lt;- sacr_wkflw %&gt;% tune_grid(resamples = sacr_vfold, grid = gridvals) %&gt;% collect_metrics() %&gt;% filter(.metric == &quot;rmse&quot;) # show the results sacr_results ## # A tibble: 67 x 7 ## neighbors .metric .estimator mean n std_err .config ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 rmse standard 116582. 5 1396. Preprocessor1_Model01 ## 2 4 rmse standard 94142. 5 2552. Preprocessor1_Model02 ## 3 7 rmse standard 89529. 5 2635. Preprocessor1_Model03 ## 4 10 rmse standard 88309. 5 3038. Preprocessor1_Model04 ## 5 13 rmse standard 87634. 5 3263. Preprocessor1_Model05 ## 6 16 rmse standard 87081. 5 3235. Preprocessor1_Model06 ## 7 19 rmse standard 87422. 5 3479. Preprocessor1_Model07 ## 8 22 rmse standard 86936. 5 3283. Preprocessor1_Model08 ## 9 25 rmse standard 87047. 5 3306. Preprocessor1_Model09 ## 10 28 rmse standard 87213. 5 3284. Preprocessor1_Model10 ## # … with 57 more rows Figure 8.6: Effect of the number of neighbours on the RMSPE Figure 8.6 visualizes how the RMSPE varies with the number of neighbours \\(K\\). We take the minimum RMSPE to find the best setting for the number of neighbours: # show only the row of minimum RMSPE sacr_min &lt;- sacr_results %&gt;% filter(mean == min(mean)) sacr_min ## # A tibble: 1 x 7 ## neighbors .metric .estimator mean n std_err .config ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 22 rmse standard 86936. 5 3283. Preprocessor1_Model08 The smallest RMSPE occurs when \\(K =\\) 22. 8.7 Underfitting and overfitting Similar to the setting of classification, by setting the number of neighbours to be too small or too large, we cause the RMSPE to increase, as shown in Figure 8.6. What is happening here? Figure 8.7 visualizes the effect of different settings of \\(K\\) on the regression model. Each plot shows the predicted values for house sale price from our KNN regression model for 6 different values for \\(K\\): 1, 3, 22, 41, 250, and 932 (i.e., the entire dataset). For each model, we predict prices for the range of possible home sizes we observed in the data set (here 500 to 5,000 square feet) and we plot the predicted prices as a blue line. Figure 8.7: Predicted values for house price (represented as a blue line) from KNN regression models for six different values for \\(K\\) Figure 8.7 shows that when \\(K\\) = 1, the blue line runs perfectly through (almost) all of our training observations. This happens because our predicted values for a given region (typically) depend on just a single observation. In general, when \\(K\\) is too small, the line follows the training data quite closely, even if it does not match it perfectly. If we used a different training data set of house prices and sizes from the Sacramento real estate market, we would end up with completely different predictions. In other words, the model is influenced too much by the data. Because the model follows the training data so closely, it will not make accurate predictions on new observations which, generally, will not have the same fluctuations as the original training data. Recall from the classification chapters that this behaviour—where the model is influenced too much by the noisy data—is called overfitting; we use this same term in the context of regression. What about the plots in Figure 8.7 where \\(K\\) is quite large, say, \\(K\\) = 250 or 932? In this case the blue line becomes extremely smooth, and actually becomes flat once \\(K\\) is equal to the number of datapoints in the entire data set. This happens because our predicted values for a given x value (here home size), depend on many neighbouring observations; in the case where \\(K\\) is equal to the size of the dataset, the prediction is just the mean of the house prices in the dataset (completely ignoring the house size). In contrast to the \\(K=1\\) example, the smooth, inflexible blue line does not follow the training observations very closely. In other words, the model is not influenced enough by the training data. Recall from the classification chapters that this behaviour is called underfitting; we again use this same term in the context of regression. Ideally, what we want is neither of the two situations discussed above. Instead, we would like a model that (1) follows the overall “trend” in the training data, so the model actually uses the training data to learn something useful, and (2) does not follow the noisy fluctuations, so that we can be confident that our model will transfer/generalize well to other new data. If we explore the other values for \\(K\\), in particular \\(K\\) = 22 (as suggested by cross-validation), we can see it achieves this goal: it follows the increasing trend of house price versus house size, but is not influenced too much by the idiosyncratic variations in price. All of this is similar to how the choice of \\(K\\) affects K-nearest neighbours classification, as discussed in the previous chapter. 8.8 Evaluating on the test set To assess how well our model might do at predicting on unseen data, we will assess its RMSPE on the test data. To do this, we will first re-train our KNN regression model on the entire training data set, using \\(K =\\) 22 neighbours. Then we will use predict to make predictions on the test data, and use the metrics function again to compute the summary of regression quality. Because we specify that we are performing regression in set_mode, the metrics function knows to output a quality summary related to regression, and not, say, classification. kmin &lt;- sacr_min %&gt;% pull(neighbors) sacr_spec &lt;- nearest_neighbor(weight_func = &quot;rectangular&quot;, neighbors = kmin) %&gt;% set_engine(&quot;kknn&quot;) %&gt;% set_mode(&quot;regression&quot;) sacr_fit &lt;- workflow() %&gt;% add_recipe(sacr_recipe) %&gt;% add_model(sacr_spec) %&gt;% fit(data = sacramento_train) sacr_summary &lt;- sacr_fit %&gt;% predict(sacramento_test) %&gt;% bind_cols(sacramento_test) %&gt;% metrics(truth = price, estimate = .pred) %&gt;% filter(.metric == &#39;rmse&#39;) sacr_summary ## # A tibble: 1 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 81793. Our final model’s test error as assessed by RMSPE is 81,793. Note that RMSPE is measured in the same units as the response variable. In other words, on new observations, we expect the error in our prediction to be roughly $81,793. From one perspective, this is good news: this is about the same as the cross-validation RMSPE estimate of our tuned model, so we can say that the model appears to generalize well to new data that it has never seen before. However, much like in the case of KNN classification, whether this value for RMSPE is good—i.e., whether an error of around $81,793 is acceptable—depends entirely on the application. Finally, Figure 8.8 shows the predictions that our final model makes across the range of house sizes we might encounter in the Sacramento area—from 500 to 5000 square feet. You have already seen a few plots like this in this chapter, but here we also provide the code that generated it as a learning challenge. sacr_preds &lt;- tibble(sqft = seq(from = 500, to = 5000, by = 10)) sacr_preds &lt;- sacr_fit %&gt;% predict(sacr_preds) %&gt;% bind_cols(sacr_preds) plot_final &lt;- ggplot() + geom_point(data = sacramento_train, mapping = aes(x = sqft, y = price), alpha = 0.4) + geom_line(data = sacr_preds, mapping = aes(x = sqft, y = .pred), color=&quot;blue&quot;) + xlab(&quot;House size (square feet)&quot;) + ylab(&quot;Price (USD)&quot;) + scale_y_continuous(labels = dollar_format()) + ggtitle(paste0(&quot;K = &quot;, kmin)) plot_final Figure 8.8: Predicted values of house price (blue line) for the final KNN regression model 8.9 Strengths and limitations of KNN regression As with KNN classification (or any prediction algorithm for that matter), KNN regression has both strengths and weaknesses. Some are listed here: Strengths: K-nearest neighbours regression is a simple, intuitive algorithm requires few assumptions about what the data must look like works well with non-linear relationships (i.e., if the relationship is not a straight line) Weaknesses: K-nearest neighbours regression becomes very slow as the training data gets larger may not perform well with a large number of predictors may not predict well beyond the range of values input in your training data 8.10 Multivariate KNN regression As in KNN classification, we can use multiple predictors in KNN regression. In this setting, we have the same concerns regarding the scale of the predictors. Once again, predictions are made by identifying the \\(K\\) observations that are nearest to the new point we want to predict; any variables that are on a large scale will have a much larger effect than variables on a small scale. But since the recipe we built above scales and centers all predictor variables, this is handled for us. Note that we also have the same concern regarding the selection of predictors in KNN regression as in KNN classification: more predictors is not always better, and the choice of which predictors to use has a potentially large influence on the quality of predictions. Fortunately, we can use the predictor selection algorithm from the classification chapter in KNN regression as well. As the algorithm is the same, we will not cover it again in this chapter. We will now demonstrate a multivariate KNN regression analysis of the Sacramento real estate data using tidymodels. This time we will use house size (measured in square feet) as well as number of bedrooms as our predictors, and continue to use house sale price as our outcome/target variable that we are trying to predict. It is always a good practice to do exploratory data analysis, such as visualizing the data, before we start modeling the data. Figure 8.9 shows that the number of bedrooms might provide useful information to help predict the sale price of a house. plot_beds &lt;- sacramento %&gt;% ggplot(aes(x = beds, y = price)) + geom_point(alpha = 0.4) + labs(x = &#39;Number of Bedrooms&#39;, y = &#39;Price (USD)&#39;) plot_beds Figure 8.9: Scatter plot of the sale price of houses versus the number of bedrooms Figure 8.9 shows that as the number of bedrooms increases, the house sale price tends to increase as well, but that the relationship is quite weak. Does adding the number of bedrooms to our model improve our ability to predict price? To answer that question, we will have to create a new KNN regression model using house size and number of bedrooms, and then we can compare it to the model we previously came up with that only used house size. Let’s do that now! First we’ll build a new model specification and recipe for the analysis. Note that we use the formula price ~ sqft + beds to denote that we have two predictors, and set neighbors = tune() to tell tidymodels to tune the number of neighbours for us. sacr_recipe &lt;- recipe(price ~ sqft + beds, data = sacramento_train) %&gt;% step_scale(all_predictors()) %&gt;% step_center(all_predictors()) sacr_spec &lt;- nearest_neighbor(weight_func = &quot;rectangular&quot;, neighbors = tune()) %&gt;% set_engine(&quot;kknn&quot;) %&gt;% set_mode(&quot;regression&quot;) Next, we’ll use 5-fold cross-validation to choose the number of neighbours via the minimum RMSPE: gridvals &lt;- tibble(neighbors = seq(1, 200)) sacr_multi &lt;- workflow() %&gt;% add_recipe(sacr_recipe) %&gt;% add_model(sacr_spec) %&gt;% tune_grid(sacr_vfold, grid = gridvals) %&gt;% collect_metrics() %&gt;% filter(.metric == &quot;rmse&quot;) %&gt;% filter(mean == min(mean)) sacr_k &lt;- sacr_multi %&gt;% pull(neighbors) sacr_multi ## # A tibble: 1 x 7 ## neighbors .metric .estimator mean n std_err .config ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 12 rmse standard 86429. 5 3687. Preprocessor1_Model012 Here we see that the smallest estimated RMSPE from cross-validation occurs when \\(K =\\) 12. If we want to compare this multivariate KNN regression model to the model with only a single predictor as part of the model tuning process (e.g., if we are running forward selection as described in the chapter on evaluating and tuning classification models), then we must compare the accuracy estimated using only the training data via cross-validation. Looking back, the estimated cross-validation accuracy for the single-predictor model was 86,936. The estimated cross validation accuracy for the multivariate model is 86,429. Thus in this case, we did not improve the model by a large amount by adding this additional predictor. Regardless, let’s continue the analysis to see how we can make predictions with a multivariate KNN regression model and evaluate its performance on test data. We first need to re-train the model on the entire training data set with \\(K =\\) 12, and then use that model to make predictions on the test data. sacr_spec &lt;- nearest_neighbor(weight_func = &quot;rectangular&quot;, neighbors = sacr_k) %&gt;% set_engine(&quot;kknn&quot;) %&gt;% set_mode(&quot;regression&quot;) knn_mult_fit &lt;- workflow() %&gt;% add_recipe(sacr_recipe) %&gt;% add_model(sacr_spec) %&gt;% fit(data = sacramento_train) knn_mult_preds &lt;- knn_mult_fit %&gt;% predict(sacramento_test) %&gt;% bind_cols(sacramento_test) knn_mult_mets &lt;- metrics(knn_mult_preds, truth = price, estimate = .pred) %&gt;% filter(.metric == &#39;rmse&#39;) knn_mult_mets ## # A tibble: 1 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 80728. This time when we performed KNN regression on the same data set, but also included number of bedrooms as a predictor, we obtained a RMSPE test error of 80,728. Figure 8.10 visualizes the model’s predictions overlaid on top of the data. This time the predictions are a surface in 3-D space, instead of a line in 2-D space, as we have 2 predictors instead of 1. Figure 8.10: KNN regression model’s predictions represented as a surface in 3-D space overlaid on top of the data using three predictors (price, house size, and the number of bedrooms) We can see that the predictions in this case, where we have 2 predictors, form a surface instead of a line. Because the newly added predictor (number of bedrooms) is related to price (as price changes, so does number of bedrooms) and is not totally determined by house size (our other predictor), we get additional and useful information for making our predictions. For example, in this model we would predict that the cost of a house with a size of 2,500 square feet generally increases slightly as the number of bedrooms increases. Without having the additional predictor of number of bedrooms, we would predict the same price for these two houses. "],["regression2.html", "Chapter 9 Regression II: linear regression 9.1 Overview 9.2 Chapter learning objectives 9.3 Simple linear regression 9.4 Linear regression in R 9.5 Comparing simple linear and KNN regression 9.6 Multivariate linear regression 9.7 Multicollinearity and outliers 9.8 Designing new predictors 9.9 The other sides of regression 9.10 Additional resources", " Chapter 9 Regression II: linear regression 9.1 Overview Up to this point, we have solved all of our predictive problems—both classification and regression—using K-nearest neighbours (KNN)-based approaches. In the context of regression, there is another method commonly used in scientific disciplines known as linear regression. This chapter provides an introduction to the basic concept of linear regression, shows how to use tidymodels to perform linear regression in R, and characterizes its strengths and weaknesses compared to KNN regression. The focus is, as usual, on the case where there is a single predictor and single response variable of interest; but the chapter concludes with an example using multivariate linear regression when there is more than one predictor. 9.2 Chapter learning objectives By the end of the chapter, students will be able to: Use R and tidymodels to fit a linear regression model on training data. Evaluate the linear regression model on test data. Compare and contrast predictions obtained from K-nearest neighbour regression to those obtained using linear regression from the same dataset. In R, overlay predictions from linear regression on a scatter plot of data using geom_smooth. 9.3 Simple linear regression At the end of the previous chapter, we noted some limitations of KNN regression. While the method is simple and easy to understand, KNN regression does not predict well beyond the range of the predictors in the training data, and the method gets significantly slower as the training dataset grows. Fortunately, there is an alternative to KNN regression—linear regression—that addresses both of these limitations. Linear regression is also much more commonly used in practice, especially in scientific applications, because it provides an interpretable mathematical equation that describes the relationship between the predictor and response variables. In this first part of the chapter, we will focus on simple linear regression, which involves only one predictor variable and one response variable; later on, we will consider multivariate linear regression, which involves multiple predictor variables. Like KNN regression, simple linear regression involves predicting a numerical response variable (like race time, house price, or height); but how it makes those predictions for a new observation is quite different from KNN regression. Instead of looking at the K nearest neighbours and averaging over their values for a prediction, in simple linear regression, we create a straight line of best fit through the training data and then “look up” the prediction using the line. Note: although we did not cover it in earlier chapters, there is another popular method for classification called logistic regression (it is used for classification even though the name, somewhat confusingly, has the word “regression” in it). In logistic regression—similar to linear regression—you “fit” the model to the training data and then “look up” the prediction for each new observation. Logistic regression and KNN classification have an advantage/disadvantage comparison similar to that of linear regression and KNN regression. It is useful to have a good understanding of linear regression before learning about logistic regression. After reading this chapter, see the “Additional Resources” section at the end of the classification chapters to learn more about logistic regression. Let’s return to the Sacramento housing data from the previous chapter to learn how to apply linear regression and compare it to KNN regression. For now, we will consider a smaller version of the housing data to help make our visualizations clear. Recall our predictive question: can we use the size of a house in the Sacramento, CA area to predict its sale price? In particular, recall that we have come across a new 2,000-square foot house we are interested in purchasing with an advertised list price of $350,000. Should we offer the list price, or is that over/undervalued? To answer this question using simple linear regression, we use the data we have to draw the straight line of best fit through our existing data points. The small subset of data as well as the line of best fit are shown in Figure 9.1. Figure 9.1: Scatter plot of sale price versus size with line of best fit for subset of the Sacramento housing data The equation for the straight line is: \\[\\text{house sale price} = \\beta_0 + \\beta_1 \\cdot (\\text{house size}),\\] where \\(\\beta_0\\) is the vertical intercept of the line (the price when house size is 0) \\(\\beta_1\\) is the slope of the line (how quickly the price increases as you increase house size) Therefore using the data to find the line of best fit is equivalent to finding coefficients \\(\\beta_0\\) and \\(\\beta_1\\) that parametrize (correspond to) the line of best fit. Now of course, in this particular problem, the idea of a 0-square-foot house is a bit silly; but you can think of \\(\\beta_0\\) here as the “base price,” and \\(\\beta_1\\) as the increase in price for each square foot of space. Let’s push this thought even further: what would happen in the equation for the line if you tried to evaluate the price of a house with size 6 million square feet? Or what about negative 2,000 square feet? As it turns out, nothing in the formula breaks; linear regression will happily make predictions for crazy predictor values if you ask it to. But even though you can make these wild predictions, you shouldn’t. You should only make predictions roughly within the range of your original data, and perhaps a bit beyond it only if it makes sense. For example, the data in Figure 9.1 only reaches around 800 square feet on the low end, but it would probably be reasonable to use the linear regression model to make a prediction at 600 square feet, say. Back to the example! Once we have the coefficients \\(\\beta_0\\) and \\(\\beta_1\\), we can use the equation above to evaluate the predicted sale price given the value we have for the predictor variable—here 2,000 square feet. Figure 9.2 demonstrates this process. Figure 9.2: Scatter plot of sale price versus size with line of best fit and a red dot at the predicted sale price for a 2000 square foot home By using simple linear regression on this small data set to predict the sale price for a 2,000 square foot house, we get a predicted value of $329,423. But wait a minute…how exactly does simple linear regression choose the line of best fit? Many different lines could be drawn through the data points. Some plausible examples are shown in Figure 9.3. Figure 9.3: Scatter plot of sale price versus size with many possible lines that could be drawn through the data points Simple linear regression chooses the straight line of best fit by choosing the line that minimizes the average squared vertical distance between itself and each of the observed data points in the training data. Figure 9.4 illustrates these vertical distances as red lines. Finally, to assess the predictive accuracy of a simple linear regression model, we use RMSPE—the same measure of predictive performance we used with KNN regression. Figure 9.4: Scatter plot of sale price versus size with red lines denoting the vertical distances between the predicted values and the observed data points 9.4 Linear regression in R We can perform simple linear regression in R using tidymodels in a very similar manner to how we performed KNN regression. To do this, instead of creating a nearest_neighbor model specification with the kknn engine, we use a linear_reg model specification with the lm engine. Another difference is that we do not need to choose \\(K\\) in the context of linear regression, and so we do not need to perform cross validation. Below we illustrate how we can use the usual tidymodels workflow to predict house sale price given house size using a simple linear regression approach using the full Sacramento real estate data set. An additional difference that you will notice below is that we do not standardize (i.e., scale and center) our predictors. In K-nearest neighbours models, recall that the model fit changes depending on whether we standardize first or not. In linear regression, standardization does not affect the fit (it does affect the coefficients in the equation, though!). So you can standardize if you want—it won’t hurt anything—but if you leave the predictors in their original form, the best fit coefficients are usually easier to interpret afterward. As usual, we start by putting some test data away in a lock box that we can come back to after we choose our final model. Let’s take care of that now. set.seed(1234) sacramento_split &lt;- initial_split(sacramento, prop = 0.6, strata = price) sacramento_train &lt;- training(sacramento_split) sacramento_test &lt;- testing(sacramento_split) Now that we have our training data, we will create the model specification and recipe, and fit our simple linear regression model: lm_spec &lt;- linear_reg() %&gt;% set_engine(&quot;lm&quot;) %&gt;% set_mode(&quot;regression&quot;) lm_recipe &lt;- recipe(price ~ sqft, data = sacramento_train) lm_fit &lt;- workflow() %&gt;% add_recipe(lm_recipe) %&gt;% add_model(lm_spec) %&gt;% fit(data = sacramento_train) lm_fit ## ══ Workflow [trained] ══════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: linear_reg() ## ## ── Preprocessor ──────────────────────────────────────────────────────────────── ## 0 Recipe Steps ## ## ── Model ─────────────────────────────────────────────────────────────────────── ## ## Call: ## stats::lm(formula = ..y ~ ., data = data) ## ## Coefficients: ## (Intercept) sqft ## 13665.0 138.1 Our coefficients are (intercept) \\(\\beta_0=\\) 13665 and (slope) \\(\\beta_1=\\) 138. This means that the equation of the line of best fit is \\[\\text{house sale price} = 13665 + 138\\cdot (\\text{house size}).\\] In other words, the model predicts that houses start at $13,665 for 0 square feet, and that every extra square foot increases the cost of the house by $138. Finally, we predict on the test data set to assess how well our model does: lm_test_results &lt;- lm_fit %&gt;% predict(sacramento_test) %&gt;% bind_cols(sacramento_test) %&gt;% metrics(truth = price, estimate = .pred) lm_test_results ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 79324. ## 2 rsq standard 0.625 ## 3 mae standard 60183. Our final model’s test error as assessed by RMSPE is 79,324. Remember that this is in units of the target/response variable, and here that is US Dollars (USD). Does this mean our model is “good” at predicting house sale price based off of the predictor of home size? Again, answering this is tricky and requires knowledge of how you intend to use the prediction. To visualize the simple linear regression model, we can plot the predicted house sale price across all possible house sizes we might encounter superimposed on a scatter plot of the original housing price data. There is a plotting function in the tidyverse, geom_smooth, that allows us to add a layer on our plot with the simple linear regression predicted line of best fit. By default geom_smooth adds some other information to the plot that we are not interested in at this point; we provide the argument se = FALSE to tell geom_smooth not to show that information. Figure 9.5 displays the result. lm_plot_final &lt;- ggplot(sacramento_train, aes(x = sqft, y = price)) + geom_point(alpha = 0.4) + xlab(&quot;House size (square feet)&quot;) + ylab(&quot;Price (USD)&quot;) + scale_y_continuous(labels = dollar_format()) + geom_smooth(method = &quot;lm&quot;, se = FALSE) lm_plot_final Figure 9.5: Scatter plot of sale price versus size with line of best fit for the full Sacramento housing data We can extract the coefficients from our model by accessing the fit object that is output by the fit function; we first have to extract it from the workflow using the pull_workflow_fit function, and then apply the tidy function to convert the result into a data frame: coeffs &lt;- lm_fit %&gt;% pull_workflow_fit() %&gt;% tidy() coeffs ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 13665. 9387. 1.46 1.46e- 1 ## 2 sqft 138. 5.13 26.9 9.61e-103 9.5 Comparing simple linear and KNN regression Now that we have a general understanding of both simple linear and KNN regression, we can start to compare and contrast these methods as well as the predictions made by them. To start, let’s look at the visualization of the simple linear regression model predictions for the Sacramento real estate data (predicting price from house size) and the “best” KNN regression model obtained from the same problem, shown in Figure 9.6. Figure 9.6: Comparison of simple linear regression and KNN regression What differences do we observe in Figure 9.6? One obvious difference is the shape of the blue lines. In simple linear regression we are restricted to a straight line, whereas in KNN regression our line is much more flexible and can be quite wiggly. But there is a major interpretability advantage in limiting the model to a straight line. A straight line can be defined by two numbers, the vertical intercept and the slope. The intercept tells us what the prediction is when all of the predictors are equal to 0; and the slope tells us what unit increase in the target/response variable we predict given a unit increase in the predictor variable. KNN regression, as simple as it is to implement and understand, has no such interpretability from its wiggly line. There can however also be a disadvantage to using a simple linear regression model in some cases, particularly when the relationship between the target and the predictor is not linear, but instead some other shape (e.g. curved or oscillating). In these cases the prediction model from a simple linear regression will underfit (have high bias), meaning that model/predicted values does not match the actual observed values very well. Such a model would probably have a quite high RMSE when assessing model goodness of fit on the training data and a quite high RMPSE when assessing model prediction quality on a test data set. On such a data set, KNN regression may fare better. Additionally, there are other types of regression you can learn about in future courses that may do even better at predicting with such data. How do these two models compare on the Sacramento house prices data set? In Figure 9.6, we also printed the RMPSE as calculated from predicting on the test data set that was not used to train/fit the models. The RMPSE for the simple linear regression model is slightly lower than the RMPSE for the KNN regression model. Considering that the simple linear regression model is also more interpretable, if we were comparing these in practice we would likely choose to use the simple linear regression model. Finally, note that the KNN regression model becomes “flat” at the left and right boundaries of the data, while the linear model predicts a constant slope. Predicting outside the range of the observed data is known as extrapolation; KNN and linear models behave quite differently when extrapolating. Depending on the application, the flat or constant slope trend may make more sense. For example, if our housing data were slightly different, the linear model may have actually predicted a negative price for a small houses (if the intercept \\(\\beta_0\\) was negative), which obviously does not match reality. On the other hand, the trend of increasing house size corresponding to increasing house price probably continues for large houses, so the “flat” extrapolation of KNN likely does not match reality. 9.6 Multivariate linear regression As in KNN classification and KNN regression, we can move beyond the simple case of only one predictor to the case with multiple predictors, known as multivariate linear regression. To do this, we follow a very similar approach to what we did for KNN regression: we just add more predictors to the model formula in the recipe. But recall that we do not need to use cross-validation to choose any parameters, nor do we need to standardize (i.e., center and scale) the data for linear regression. Note once again that we have the same concerns regarding multiple predictors as in the settings of multivariate KNN regression and classification: more predictors is not always better. But because the same predictor selection algorithm from the classification chapter extends to the setting of linear regression, it will not be covered again in this chapter. We will demonstrate multivariate linear regression using the Sacramento real estate data with both house size (measured in square feet) as well as number of bedrooms as our predictors, and continue to use house sale price as our response variable. We will start by changing the formula in the recipe to include both the sqft and beds variables as predictors: mlm_recipe &lt;- recipe(price ~ sqft + beds, data = sacramento_train) Now we can build our workflow and fit the model: mlm_fit &lt;- workflow() %&gt;% add_recipe(mlm_recipe) %&gt;% add_model(lm_spec) %&gt;% fit(data = sacramento_train) mlm_fit ## ══ Workflow [trained] ══════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: linear_reg() ## ## ── Preprocessor ──────────────────────────────────────────────────────────────── ## 0 Recipe Steps ## ## ── Model ─────────────────────────────────────────────────────────────────────── ## ## Call: ## stats::lm(formula = ..y ~ ., data = data) ## ## Coefficients: ## (Intercept) sqft beds ## 54465.7 157.9 -22375.8 And finally, we make predictions on the test data set to assess the quality of our model: lm_mult_test_results &lt;- mlm_fit %&gt;% predict(sacramento_test) %&gt;% bind_cols(sacramento_test) %&gt;% metrics(truth = price, estimate = .pred) lm_mult_test_results ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 77162. ## 2 rsq standard 0.645 ## 3 mae standard 58850. Our model’s test error as assessed by RMSPE is 77,162. In the case of two predictors, we can plot the predictions made by our linear regression creates a plane of best fit, as shown in Figure 9.7. Figure 9.7: Linear regression plane of best fit overlaid on top of the data (using price, house size, and number of bedrooms as predictors) We see that the predictions from linear regression with two predictors form a flat plane. This is the hallmark of linear regression, and differs from the wiggly, flexible surface we get from other methods such as KNN regression. As discussed this can be advantageous in one aspect, which is that for each predictor, we can get slopes/intercept from linear regression, and thus describe the plane mathematically. We can extract those slope values from our model object as shown below: mcoeffs &lt;- mlm_fit %&gt;% pull_workflow_fit() %&gt;% tidy() mcoeffs ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 54466. 14007. 3.89 1.13e- 4 ## 2 sqft 158. 7.19 22.0 1.76e-77 ## 3 beds -22376. 5759. -3.89 1.14e- 4 And then use those slopes to write a mathematical equation to describe the prediction plane: \\[\\text{house sale price} = \\beta_0 + \\beta_1\\cdot(\\text{house size}) + \\beta_2\\cdot(\\text{number of bedrooms}),\\] where: \\(\\beta_0\\) is the vertical intercept of the hyperplane (the price when both house size and number of bedrooms are 0) \\(\\beta_1\\) is the slope for the first predictor (how quickly the price increases as you increase house size) \\(\\beta_2\\) is the slope for the second predictor (how quickly the price increases as you increase the number of bedrooms) Finally, we can fill in the values for \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\beta_2\\) from the model output above to create the equation of the plane of best fit to the data: \\[\\text{house sale price} = 54466 + 158\\cdot (\\text{house size}) -22376 \\cdot (\\text{number of bedrooms})\\] This model is more interpretable than the multivariate KNN regression model; we can write a mathematical equation that explains how each predictor is affecting the predictions. But as always, we should question how well multivariate linear regression is doing compared to the other tools we have, such as simple linear regression and multivariate KNN regression. If this comparison is part of the model tuning process—for example, if we are trying out many different sets of predictors for multivariate linear and KNN regression—we must perform this comparison using cross-validation on only our training data. But if we have already decided on a small number (e.g., 2 or 3) of tuned candidate models and we want to make a final comparison, we can do so by comparing the prediction error of the methods on the test data. lm_mult_test_results ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 77162. ## 2 rsq standard 0.645 ## 3 mae standard 58850. We obtain an RMSPE for the multivariate linear regression model of 77,161.78. This prediction error is less than the prediction error for the multivariate KNN regression model, indicating that we should likely choose linear regression for predictions of house sale price on this data set. Revisiting the simple linear regression model with only a single predictor from earlier in this chapter, we see that the RMSPE for that model was 79,324.36, which is slightly higher than that of our more complex model. Our model with two predictors provided a slightly better fit on test data than our model with just one. As mentioned earlier, this is not always the case: sometimes including more predictors can negatively impact the prediction performance on unseen test data. 9.7 Multicollinearity and outliers What can go wrong when performing (multivariate) linear regression? Two common issues to be mindful of are outliers and, in the multivarate case, collinear predictors. 9.7.1 Outliers Outliers are data points that do not follow the usual pattern of the rest of the data. In the setting of linear regression, these are points that have a vertical distance to the line of best fit that is either much higher or much lower than you might expect based on the rest of the data. The problem with outliers is that they can have too much influence on the line of best fit. In general, it is very difficult to judge accurately which data are outliers without advanced techniques that are beyond the scope of this book. But to illustrate what can happen when you have outliers, Figure 9.8 shows a small subset of the Sacramento housing data again, except we have added a single data point (highlighted in red). This house is 5,000 square feet in size, and sold for only $50,000. Unbeknownst to the data analyst, this house was sold by a parent to their child for an absurdly low price. Of course this is not representative of the real housing market values that the other data points follow; the data point is an outlier. In blue we plot the original line of best fit, and in red we plot the new line of best fit including the outlier. You can see how different the red line is from the blue line, which is entirely caused by that one extra outlier data point. Figure 9.8: Scatter plot of a subset of the data, with outlier highlighted in red Fortunately, if you have enough data, the inclusion of one or two outliers—as long as their values are not too wild—will typically not have a large an effect on the line of best fit. Figure 9.9 shows how that same outlier data point from earlier influences the line of best fit when we are working with the entire original Sacramento training data. You can see that with this larger data set, the line changes much less when adding the outlier. Nevertheless, it is still important when working with linear regression to critically think about how much any individual data point is influencing the model. Figure 9.9: Scatter plot of the full data, with outlier highlighted in red 9.7.2 Multicollinearity The second, and much more subtle, issue can occur when performing multivariate linear regression. In particular, if you include multiple predictors that are strongly linearly related to one another, the coefficients that describe the plane of best fit can be very unstable. Consider an extreme example using the Sacramento housing data where the house was measured twice by two people. Since the two people are each slightly inaccurate, the two measurements might not agree exactly, but they are very strongly linearly related to each other, as shown in Figure 9.10. Figure 9.10: Scatter plot of the with possible outlier highlighted in red If we again fit the multivariate linear regression model on this data, then the plane of best fit has regression coefficients that are very sensitive to the exact values in the data. For example, if we change the data ever so slightly—e.g., by running cross validation, which splits up the data randomly into different chunks—the coefficients vary by large amounts: Best Fit 1: \\(\\text{house sale price} = 6204 + -10\\cdot (\\text{house size 1 (ft$^2$)}) + 148 \\cdot (\\text{house size 2 (ft$^2$)}).\\) Best Fit 2: \\(\\text{house sale price} = 19164 + 250\\cdot (\\text{house size 1 (ft$^2$)}) + -112 \\cdot (\\text{house size 2 (ft$^2$)}).\\) Best Fit 3: \\(\\text{house sale price} = 2631 + -111\\cdot (\\text{house size 1 (ft$^2$)}) + 249 \\cdot (\\text{house size 2 (ft$^2$)}).\\) Therefore, when performing multivariate linear regression, it is important to avoid including very linearly related predictors. 9.8 Designing new predictors We were quite fortunate in our initial exploration to find a predictor variable (house size) that seems to have a meaningful and nearly linear relationship with our response variable (sale price). But what should we do if we cannot immediately find such a nice variable? Well, sometimes it is just a fact that the variables in the data do not have enough of a relationship with the response variable to provide useful predictions. For example, if the only available predictor was “the current house owner’s favourite ice cream flavour,” we likely would have little hope of using that variable to predict the house’s sale price (barring any future remarkable scientific discoveries about the relationship between the housing market and homeowner ice cream preferences). In cases like these, the only option is to obtain measurements of more useful variables. There are, however, a wide variety of cases where the predictor variables do have a meaningful relationship with the response variable, but that relationship does not fit the assumptions of the regression method you have chosen. For example, a dataframe df with two variables—x and y—with a nonlinear relationship between the two variables will not be correctly captured by simple linear regression, as shown in Figure 9.11. Figure 9.11: Example of a dataset with a nonlinear relationship between the predictor and the response Instead of trying to predict the response y using a linear regression on x, we might have some scientific background about our problem to suggest that y should be a cubic function of x. So before performing regression, we might create a new predictor variable z using the mutate function: df &lt;- df %&gt;% mutate(z = x^3) Then we can perform linear regression for y using the predictor variable z, as shown in Figure 9.12. Here you can see that the transformed predictor z helps the linear regression model make more accurate predictions. Note that none of the y response values have changed between Figures 9.11 and 9.12; the only change is that the x values have been replaced by z values. Figure 9.12: Relationship between the transformed predictor and the response The process of transforming predictors (and potentially combining multiple predictors in the process) is known as feature engineering. In real data analysis problems, you will need to rely on a deep understanding of the problem—as well as the wrangling tools from previous chapters—to engineer useful new features that improve predictive performance. Note that feature engineering is part of tuning your model, and as such you must not use your test data to evaluate the quality of the features you produce. You are free to use cross-validation, though! 9.9 The other sides of regression So far in this textbook we have used regression only in the context of prediction. However, regression can also be seen as a method to understand and quantify the effects of individual variables on a response / outcome of interest. In the housing example from this chapter, beyond just using past data to predict future sale prices, we might also be interested in describing the individual relationships of house size and the number of bedrooms with house price, quantifying how strong each of these relationships are, and assessing how accurately we can estimate their magnitudes. And even beyond that, we may be interested in understanding whether the predictors cause changes in the price. These sides of regression are well beyond the scope of this book; but the material you have learned here should give you a foundation of knowledge that will serve you well when moving to more advanced books on the topic. 9.10 Additional resources The tidymodels website is an excellent reference for more details on, and advanced usage of, the functions and packages in the past two chapters. Aside from that, it also has a nice beginner’s tutorial and an extensive list of more advanced examples that you can use to continue learning beyond the scope of this book. Modern Dive is another textbook that uses the tidyverse / tidymodels framework. Chapter 6 complements the material in the current chapter well; it covers some slightly more advanced concepts than we do without getting mathematical. Give this chapter a read before moving on to the next reference. It is also worth noting that this book takes a more “explanatory” / “inferential” approach to regression in general (in Chapters 5, 6, and 10), which provides a nice complement to the predictive tack we take in the present book. An Introduction to Statistical Learning (2013) provides a great next stop in the process of learning about regression. Chapter 3 covers linear regression at a slightly more mathematical level than we do here, but it is not too large a leap and so should provide a good stepping stone. Chapter 6 discusses how to pick a subset of “informative” predictors when you have a data set with many predictors, and you expect only a few of them to be relevant. Chapter 7 covers regression models that are more flexible than linear regression models but still enjoy the computational efficiency of linear regression. In contrast, the KNN methods we covered earlier are indeed more flexible but become very slow when given lots of data. References "],["clustering.html", "Chapter 10 Clustering 10.1 Overview 10.2 Chapter learning objectives 10.3 Clustering 10.4 K-means 10.5 Data pre-processing for K-means 10.6 K-means in R 10.7 Additional resources", " Chapter 10 Clustering 10.1 Overview As part of exploratory data analysis, it is often helpful to see if there are meaningful subgroups (or clusters) in the data; this grouping can be used for many purposes, such as generating new questions or improving predictive analyses. This chapter provides an introduction to clustering using the K-means algorithm, including techniques to choose the number of clusters. 10.2 Chapter learning objectives By the end of the chapter, students will be able to: Describe a case where clustering is appropriate, and what insight it might extract from the data. Explain the K-means clustering algorithm. Interpret the output of a K-means analysis. Identify when it is necessary to scale variables before clustering, and do this using R. Perform K-means clustering in R using kmeans. Use the elbow method to choose the number of clusters for K-means. Visualize the output of K-means clustering in R using coloured scatter plots. Describe the advantages, limitations and assumptions of the K-means clustering algorithm. 10.3 Clustering Clustering is a data analysis task involving separating a data set into subgroups of related data. For example, we might use clustering to separate a data set of documents into groups that correspond to topics, a data set of human genetic information into groups that correspond to ancestral subpopulations, or a data set of online customers into groups that correspond to purchasing behaviours. Once the data are separated, we can, for example, use the subgroups to generate new questions about the data and follow up with a predictive modelling exercise. In this course, clustering will be used only for exploratory analysis, i.e., uncovering patterns in the data. Note that clustering is a fundamentally different kind of task than classification or regression. In particular, both classification and regression are supervised tasks where there is a predictive target (a class label or value), and we have examples of past data with labels/values that help us predict those of future data. By contrast, clustering is an unsupervised task, as we are trying to understand and examine the structure of data without any labels to help us. This approach has both advantages and disadvantages. Clustering requires no additional annotation or input on the data. For example, it would be nearly impossible to annotate all the articles on Wikipedia with human-made topic labels. However, we can still cluster the articles without this information to find groupings corresponding to topics automatically. However, because there is no predictive target, it is not as easy to evaluate the “quality” of a clustering. With classification, we can use a test data set to assess prediction performance. In clustering, there is not a single good choice for evaluation. In this book, we will use visualization to ascertain the quality of a clustering, and leave rigorous evaluation for more advanced courses. There are also so-called semisupervised tasks, where only some of the data come with labels/annotations, but the vast majority don’t. The goal is to try to uncover underlying structure in the data that allows one to guess the missing labels. This sort of task is beneficial, for example, when one has an unlabelled data set that is too large to manually label, but one is willing to provide a few informative example labels as a “seed” to guess the labels for all the data. An illustrative example Here we will present an illustrative example using a data set from the {palmerpenguins} R data package. This data set was collected by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER and includes measurements for adult penguins near Palmer Station (Horst, Hill, and Gorman 2020). We have modified the data set for use in this chapter. Here we will focus on using two variables—penguin bill and flipper length, both in mm—to determine whether there are distinct types of penguins in our data. Understanding this might help us with species discovery and classification in a data-driven way. Figure 10.1: Gentoo penguin. (Andrew Shiva 2016) Below we will work with penguin_data, a subset of 18 observations of the original data, which has already been scaled. We will discuss scaling for K-means in more detail later in this chapter. library(tidyverse) penguin_data ## # A tibble: 18 x 2 ## flipper_length_scaled[,1] bill_length_scaled[,1] ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.190 -0.641 ## 2 -1.33 -1.14 ## 3 -0.922 -1.52 ## 4 -0.922 -1.11 ## 5 -1.41 -0.847 ## 6 -0.678 -0.641 ## 7 -0.271 -1.24 ## 8 -0.434 -0.902 ## 9 1.19 0.720 ## 10 1.36 0.646 ## 11 1.36 0.963 ## 12 1.76 0.440 ## 13 1.11 1.21 ## 14 0.786 0.123 ## 15 -0.271 0.627 ## 16 -0.271 0.757 ## 17 -0.108 1.78 ## 18 -0.759 0.776 Figure 10.2: Subset data of penguin scaled bill length versus flipper length. Based on the visualization in Figure 10.2, we might suspect there are a few subtypes of penguins, selected from combinations of high/low flipper length and high/low bill length. How do we find this grouping automatically, and how do we pick the number of subtypes? The way to rigorously separate the data into groups is to use a clustering algorithm. In this chapter, we will focus on the K-means algorithm, a widely-used and often very effective clustering method, combined with the elbow method for selecting the number of clusters. This procedure will separate the data into the following groups denoted by colour: Figure 10.3: Subset data of penguin scaled bill length versus flipper length with coloured groups What are the labels for these groups? Unfortunately, we don’t have any. K-means, like almost all clustering algorithms, just outputs meaningless “cluster labels” that are typically whole numbers: 1, 2, 3, etc. But in a simple case like this, where we can easily visualize the clusters on a scatter plot, we can give human-made labels to the groups using their positions on the plot: small flipper length and small bill length (orange cluster), small flipper length and large bill length (blue cluster). and large flipper length and large bill length (yellow cluster). Once we have made these determinations, we can use them to inform our species classifications or ask further questions about our data. For example, we might be interested in understanding the relationship between flipper length and bill length, and that relationship may differ depending on the type of penguin we have. 10.4 K-means 10.4.1 Measuring cluster quality The K-means algorithm is a procedure that groups data into K clusters. It starts with an initial clustering of the data, and then iteratively improves it by making adjustments to the assignment of data to clusters until it cannot improve any further. But how do we measure the “quality” of a clustering, and what does it mean to improve it? In K-means clustering, we measure the quality of a cluster by its within-cluster sum-of-squared-distances (WSSD). Computing this involves two steps. First, we find the cluster centers by computing the mean of each variable over data points in the cluster. For example, suppose we have a cluster containing four observations, and we are using two variables, \\(x\\) and \\(y\\), to cluster the data. Then we would compute the \\(x\\) and \\(y\\) variables, \\(\\mu_x\\) and \\(\\mu_y\\), of the cluster center via \\[\\mu_x = \\frac{1}{4}(x_1+x_2+x_3+x_4) \\quad \\mu_y = \\frac{1}{4}(y_1+y_2+y_3+y_4).\\] In the first cluster from the example, there are 4 data points. These are shown with their cluster center (flipper_length_scaled = -0.35 and bill_length_scaled = 0.99) highlighted in Figure 10.4. Figure 10.4: Cluster 1 from the toy example, with center highlighted. The second step in computing the WSSD is to add up the squared distance between each point in the cluster and the cluster center. We use the straight-line / Euclidean distance formula that we learned about in the classification chapter. In the 4-observation cluster example above, we would compute the WSSD \\(S^2\\) via \\[\\begin{align*} S^2 = \\left((x_1 - \\mu_x)^2 + (y_1 - \\mu_y)^2\\right) + \\left((x_2 - \\mu_x)^2 + (y_2 - \\mu_y)^2\\right) + \\\\ \\left((x_3 - \\mu_x)^2 + (y_3 - \\mu_y)^2\\right) + \\left((x_4 - \\mu_x)^2 + (y_4 - \\mu_y)^2\\right). \\end{align*}\\] These distances are denoted by lines in Figure 10.5 for the first cluster of the penguin data example. Figure 10.5: Cluster 1 from the toy example, with distances to the center highlighted. The larger the value of \\(S^2\\), the more spread-out the cluster is, since large \\(S^2\\) means that points are far from the cluster center. Note, however, that “large” is relative to both the scale of the variables for clustering and the number of points in the cluster. A cluster where points are very close to the center might still have a large \\(S^2\\) if there are many data points in the cluster. 10.4.2 The clustering algorithm We begin the K-means algorithm by picking K, and uniformly randomly assigning data to the K clusters. Then K-means consists of two major steps that attempt to minimize the sum of WSSDs over all the clusters, i.e. the total WSSD: Center update: Compute the center of each cluster. Label update: Reassign each data point to the cluster with the nearest center. These two steps are repeated until the cluster assignments no longer change. For example, in the penguin data example, our initialization might look like this: Figure 10.6: Random initialization of labels. And the first four iterations of K-means would look like (each row corresponds to an iteration, where the left column depicts the center update, and the right column depicts the reassignment of data to clusters): Center Update Label Update Note that at this point, we can terminate the algorithm since none of the assignments changed in the fourth iteration; both the centers and labels will remain the same from this point onward. Is K-means guaranteed to stop at some point, or could it iterate forever? As it turns out, thankfully, the answer is that K-means is guaranteed to stop after some number of iterations. For the interested reader, the logic for this has three steps: (1) both the label update and the center update decrease total WSSD in each iteration, (2) the total WSSD is always greater than or equal to 0, and (3) there are only a finite number of possible ways to assign the data to clusters. So at some point, the total WSSD must stop decreasing, which means none of the assignments are changing, and the algorithm terminates. 10.4.3 Random restarts Unlike the classification and regression models we studied in previous chapters, K-means can get “stuck” in a bad solution. For example, if we were unlucky and initialized K-means with the following labels: Figure 10.7: Random initialization of labels. Then the iterations of K-means would look like: Center Update Label Update This looks like a relatively bad clustering of the data, but K-means cannot improve it. To solve this problem when clustering data using K-means, we should randomly re-initialize the labels a few times, run K-means for each initialization, and pick the clustering that has the lowest final total WSSD. 10.4.4 Choosing K In order to cluster data using K-means, we also have to pick the number of clusters, K. But unlike in classification, we have no data labels and cannot perform cross-validation with some measure of model prediction error. Further, if K is chosen too small, then multiple clusters get grouped together; if K is too large, then clusters get subdivided. In both cases, we will potentially miss interesting structure in the data. For example, take a look below at the K-means clustering of our penguin flipper and bill length data for a number of clusters ranging from 1 to 9. Figure 10.8: Clustering of the penguin data for # clusters ranging from 1 to 9. If we set K less than 3, then the clustering merges separate groups of data; this causes a large total WSSD, since the cluster center (denoted by an “x”) is not close to any of the data in the cluster. On the other hand, if we set K greater than 3, the clustering subdivides subgroups of data; this does indeed still decrease the total WSSD, but by only a diminishing amount. If we plot the total WSSD versus the number of clusters, we see that the decrease in total WSSD levels off (or forms an “elbow shape”) when we reach roughly the right number of clusters. Figure 10.9: Total WSSD for # clusters ranging from 1 to 9. 10.5 Data pre-processing for K-means Similar to K-nearest neighbours classification and regression, K-means clustering uses straight-line distance to decide which points are similar to each other. Therefore, the scale of each of the variables in the data will influence which cluster data points end up being assigned. Variables with a large scale will have a much larger effect on deciding cluster assignment than variables with a small scale. To address this problem, we typically standardize our data before clustering, which ensures that each variable has a mean of 0 and standard deviation of 1. The scale function in R can be used to do this. We show an example of how to use this function below using an unscaled version of data set in this chapter: unscaled_data ## # A tibble: 18 x 2 ## bill_length_mm flipper_length_mm ## &lt;dbl&gt; &lt;dbl&gt; ## 1 39.2 196 ## 2 36.5 182 ## 3 34.5 187 ## 4 36.7 187 ## 5 38.1 181 ## 6 39.2 190 ## 7 36 195 ## 8 37.8 193 ## 9 46.5 213 ## 10 46.1 215 ## 11 47.8 215 ## 12 45 220 ## 13 49.1 212 ## 14 43.3 208 ## 15 46 195 ## 16 46.7 195 ## 17 52.2 197 ## 18 46.8 189 scaled_data &lt;- map_df(unscaled_data, scale) scaled_data ## # A tibble: 18 x 2 ## bill_length_mm[,1] flipper_length_mm[,1] ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.641 -0.190 ## 2 -1.14 -1.33 ## 3 -1.52 -0.922 ## 4 -1.11 -0.922 ## 5 -0.847 -1.41 ## 6 -0.641 -0.678 ## 7 -1.24 -0.271 ## 8 -0.902 -0.434 ## 9 0.720 1.19 ## 10 0.646 1.36 ## 11 0.963 1.36 ## 12 0.440 1.76 ## 13 1.21 1.11 ## 14 0.123 0.786 ## 15 0.627 -0.271 ## 16 0.757 -0.271 ## 17 1.78 -0.108 ## 18 0.776 -0.759 10.6 K-means in R To perform K-means clustering in R, we use the kmeans function. It takes at least two arguments: the data frame containing the data you wish to cluster, and K, the number of clusters (here we choose K = 3). Note that since the K-means algorithm uses a random initialization of assignments, we need to set the random seed to make the clustering reproducible. set.seed(1234) penguin_clust &lt;- kmeans(scaled_data, centers = 3) penguin_clust ## K-means clustering with 3 clusters of sizes 4, 8, 6 ## ## Cluster means: ## bill_length_mm flipper_length_mm ## 1 0.9858721 -0.3524358 ## 2 -1.0050404 -0.7692589 ## 3 0.6828058 1.2606357 ## ## Clustering vector: ## [1] 2 2 2 2 2 2 2 2 3 3 3 3 3 3 1 1 1 1 ## ## Within cluster sum of squares by cluster: ## [1] 1.098928 2.121932 1.247042 ## (between_SS / total_SS = 86.9 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; &quot;tot.withinss&quot; ## [6] &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; &quot;ifault&quot; As you can see above, the clustering object returned by kmeans has a lot of information that can be used to visualize the clusters, pick K, and evaluate the total WSSD. To obtain this information in a tidy format, we will call in help from the broom package. Let’s start by visualizing the clustering as a coloured scatter plot. To do that we use the augment function, which takes in the model and the original data frame, and returns a data frame with the data and the cluster assignments for each point: clustered_data &lt;- augment(penguin_clust, scaled_data) clustered_data ## # A tibble: 18 x 3 ## bill_length_mm[,1] flipper_length_mm[,1] .cluster ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 -0.641 -0.190 2 ## 2 -1.14 -1.33 2 ## 3 -1.52 -0.922 2 ## 4 -1.11 -0.922 2 ## 5 -0.847 -1.41 2 ## 6 -0.641 -0.678 2 ## 7 -1.24 -0.271 2 ## 8 -0.902 -0.434 2 ## 9 0.720 1.19 3 ## 10 0.646 1.36 3 ## 11 0.963 1.36 3 ## 12 0.440 1.76 3 ## 13 1.21 1.11 3 ## 14 0.123 0.786 3 ## 15 0.627 -0.271 1 ## 16 0.757 -0.271 1 ## 17 1.78 -0.108 1 ## 18 0.776 -0.759 1 Now that we have this information in a tidy data frame, we can make a visualization of the cluster assignments for each point: cluster_plot &lt;- ggplot(clustered_data, aes(x = flipper_length_mm, y = bill_length_mm, colour = .cluster), size = 2) + geom_point() + labs(x = &quot;Flipper Length (scaled)&quot;, y = &quot;Bill Length (scaled)&quot;, colour = &quot;Cluster&quot;) + scale_color_manual(values = c(&quot;dodgerblue3&quot;,&quot;darkorange3&quot;, &quot;goldenrod1&quot;)) cluster_plot As mentioned above, we also need to select K by finding where the “elbow” occurs in the plot of total WSSD versus the number of clusters. We can obtain the total WSSD (tot.withinss) from our clustering using broom’s glance function. For example: glance(penguin_clust) ## # A tibble: 1 x 4 ## totss tot.withinss betweenss iter ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 34 4.47 29.5 2 To calculate the total WSSD for a variety of Ks, we will create a data frame with a column named k with rows containing each value of K we want to run K-means with (here, 1 to 9). penguin_clust_ks &lt;- tibble(k = 1:9) penguin_clust_ks ## # A tibble: 9 x 1 ## k ## &lt;int&gt; ## 1 1 ## 2 2 ## 3 3 ## 4 4 ## 5 5 ## 6 6 ## 7 7 ## 8 8 ## 9 9 Then we use map to apply the kmeans function to each K. However, we need to use map a little bit differently than we have before. This is because we need to iterate over k, which is the second argument to the kmeans function. In the past, we have used map only to iterate over values of the first argument of a function. Since that is the default, we could simply write map(data_frame, function_name). This won’t work here; we need to provide our data frame as the first argument to the kmeans function. The solution is to create something called an anonymous function. An anonymous function is a function that has no name, unlike other functions you’ve seen so far (kmeans, select, etc). To do this we will write our map statement like this: map(penguin_clust_ks, function(k) kmeans(scaled_data, k)) The anonymous function in the above call is function(k) kmeans(scaled_data, k). This function takes a single argument (k) and evaluates kmeans(scaled_data, k). Since k is the first (and only) argument to the function, we can use map just like we did before! The rest of the call above does just that – it passes each row of penguin_clust_ks to our anonymous function. Below, we execute this map call inside of a mutate call on the penguin_clust_ks data frame and get a list column that contains a K-means clustering object for each value of K we had: penguin_clust_ks &lt;- tibble(k = 1:9) %&gt;% mutate(penguin_clusts = map(k, function(ks) kmeans(scaled_data, ks))) penguin_clust_ks ## # A tibble: 9 x 2 ## k penguin_clusts ## &lt;int&gt; &lt;list&gt; ## 1 1 &lt;kmeans&gt; ## 2 2 &lt;kmeans&gt; ## 3 3 &lt;kmeans&gt; ## 4 4 &lt;kmeans&gt; ## 5 5 &lt;kmeans&gt; ## 6 6 &lt;kmeans&gt; ## 7 7 &lt;kmeans&gt; ## 8 8 &lt;kmeans&gt; ## 9 9 &lt;kmeans&gt; Next, we use map again to apply glance to each of the K-means clustering objects to get the clustering statistics (including WSSD). The output of glance is a data frame, and so we get another list column. This results in a complex data frame with 3 columns, one for K, one for the K-means clustering objects, and one for the clustering statistics: penguin_clust_ks &lt;- tibble(k = 1:9) %&gt;% mutate( penguin_clusts = map(k, ~ kmeans(scaled_data, .x)), glanced = map(penguin_clusts, glance) ) penguin_clust_ks ## # A tibble: 9 x 3 ## k penguin_clusts glanced ## &lt;int&gt; &lt;list&gt; &lt;list&gt; ## 1 1 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt; ## 2 2 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt; ## 3 3 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt; ## 4 4 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt; ## 5 5 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt; ## 6 6 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt; ## 7 7 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt; ## 8 8 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt; ## 9 9 &lt;kmeans&gt; &lt;tibble [1 × 4]&gt; Finally we extract the total WSSD from the glanced column. Given that each item in this column is a data frame, we will need to use the unnest function to unpack the data frames into simpler column data types. clustering_statistics &lt;- penguin_clust_ks %&gt;% unnest(glanced) clustering_statistics ## # A tibble: 9 x 6 ## k penguin_clusts totss tot.withinss betweenss iter ## &lt;int&gt; &lt;list&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 &lt;kmeans&gt; 34 34 7.11e-15 1 ## 2 2 &lt;kmeans&gt; 34 10.9 2.31e+ 1 1 ## 3 3 &lt;kmeans&gt; 34 4.47 2.95e+ 1 2 ## 4 4 &lt;kmeans&gt; 34 3.16 3.08e+ 1 2 ## 5 5 &lt;kmeans&gt; 34 2.86 3.11e+ 1 2 ## 6 6 &lt;kmeans&gt; 34 2.21 3.18e+ 1 1 ## 7 7 &lt;kmeans&gt; 34 1.37 3.26e+ 1 2 ## 8 8 &lt;kmeans&gt; 34 1.23 3.28e+ 1 2 ## 9 9 &lt;kmeans&gt; 34 0.706 3.33e+ 1 2 Now that we have tot.withinss and k as columns in a data frame, we can make a line plot and search for the “elbow” to find which value of K to use. elbow_plot &lt;- ggplot(clustering_statistics, aes(x = k, y = tot.withinss)) + geom_point() + geom_line() + xlab(&quot;K&quot;) + ylab(&quot;Total within-cluster sum of squares&quot;) + scale_x_continuous(breaks = 1:9) elbow_plot It looks like 3 clusters is the right choice for this data. But why is there a “bump” in the total WSSD plot here? Shouldn’t total WSSD always decrease as we add more clusters? Technically yes, but remember: K-means can get “stuck” in a bad solution. Unfortunately, for K = 8 we had an unlucky initialization and found a bad clustering! We can help prevent finding a bad clustering by trying a few different random initializations via the nstart argument (here we use 10 restarts). penguin_clust_ks &lt;- tibble(k = 1:9) %&gt;% mutate( penguin_clusts = map(k, ~ kmeans(scaled_data, nstart = 10, .x)), glanced = map(penguin_clusts, glance) ) clustering_statistics &lt;- penguin_clust_ks %&gt;% unnest(glanced) elbow_plot &lt;- ggplot(clustering_statistics, aes(x = k, y = tot.withinss)) + geom_point() + geom_line() + xlab(&quot;K&quot;) + ylab(&quot;Total within-cluster sum of squares&quot;) + scale_x_continuous(breaks = 1:9) elbow_plot 10.7 Additional resources Chapter 10 of An Introduction to Statistical Learning (2013) provides a great next stop in the process of learning about clustering and unsupervised learning in general. In the realm of clustering specifically, it provides a great companion introduction to K-means, but also covers hierarchical clustering for when you expect there to be subgroups, and then subgroups within subgroups, etc. in your data. In the realm of more general unsupervised learning, it covers principal components analysis (PCA), which is a very popular technique in scientific applications for reducing the number of predictors in a dataset. References "],["inference.html", "Chapter 11 Introduction to Statistical Inference 11.1 Overview 11.2 Chapter learning objectives 11.3 Why do we need sampling? 11.4 Sampling distributions 11.5 Bootstrapping 11.6 Additional resources", " Chapter 11 Introduction to Statistical Inference 11.1 Overview A typical data analysis task in practice is to draw conclusions about some unknown aspect of a population of interest based on observed data sampled from that population; we typically do not get data on the entire population. Data analysis questions regarding how summaries, patterns, trends, or relationships in a data set extend to the wider population are called inferential questions. This chapter will start with the fundamental ideas of sampling from populations and then introduce two common techniques in statistical inference: point estimation and interval estimation. 11.2 Chapter learning objectives By the end of the chapter, students will be able to: Describe real-world examples of questions that can be answered with the statistical inference. Define common population parameters (e.g. mean, proportion, standard deviation) that are often estimated using sampled data, and estimate these from a sample. Define the following statistical sampling terms (population, sample, population parameter, point estimate, sampling distribution). Explain the difference between a population parameter and a sample point estimate. Use R to draw random samples from a finite population. Use R to create a sampling distribution from a finite population. Describe how sample size influences the sampling distribution. Define bootstrapping. Use R to create a bootstrap distribution to approximate a sampling distribution. Contrast the bootstrap and sampling distributions. 11.3 Why do we need sampling? Statistical inference can help us decide how quantities we observe in a subset of data relate to the same quantities in the broader population. Suppose a retailer is considering selling iPhone accessories, and they want to estimate how big the market might be. Additionally, they want to strategize how they can market their products on North American college and university campuses. This retailer might use statistical inference to answer the question: What proportion of all undergraduate students in North America own an iPhone? In the above question, we are interested in making a conclusion about all undergraduate students in North America; this is our population. In general, the population is the complete collection of individuals or cases we are interested in studying. Further, in the above question, we are interested in computing a quantity—the proportion of iPhone owners—based on the entire population. This proportion is our population parameter. In general, a population parameter is a numerical characteristic of the entire population. To compute this number in the example above, we would need to ask every single undergraduate in North America whether they own an iPhone. In practice, directly computing population parameters is often time-consuming and costly, and sometimes impossible. A more practical approach would be to collect measurements for a sample: a subset of individuals collected from the population. We can then compute a sample estimate—a numerical characteristic of the sample—that estimates the population parameter. For example, suppose we randomly selected ten undergraduate students across North America (the sample) and computed the proportion of those students who own an iPhone (the sample estimate). In that case, we might suspect that that proportion is a reasonable estimate of the proportion of students who own an iPhone in the entire population. Figure 11.1: Population versus sample Note that proportions are not the only kind of population parameter we might be interested in. Suppose an undergraduate student studying at the University of British Columbia in Vancouver, British Columbia, is looking for an apartment to rent. They need to create a budget, so they want to know something about studio apartment rental prices in Vancouver, BC. This student might use statistical inference to tackle the question: What is the average price-per-month of studio apartment rentals in Vancouver, Canada? The population consists of all studio apartment rentals in Vancouver, and the population parameter is the average price-per-month. Here we used the average as a measure of center to describe the “typical value” of studio apartment rental prices. But even within this one example, we could also be interested in many other population parameters. For instance, we know that not every studio apartment rental in Vancouver will have the same price-per-month. The student might be interested in how much monthly prices vary and want to find a measure of the rentals’ spread (or variability), such as the standard deviation. The student might be interested in the fraction of studio apartment rentals that cost more than $1000 per month. The question we want to answer will help us determine the parameter we want to estimate. If we were somehow able to observe the whole population of studio apartment rental offerings in Vancouver, we could compute each of these numbers exactly; therefore, these are all population parameters. There are many kinds of observations and population parameters that you will run into in practice, but in this chapter, we will focus on two settings: Using categorical observations to estimate the proportion of a category Using quantitative observations to estimate the average (or mean) 11.4 Sampling distributions 11.4.1 Sampling distributions for proportions We will look at an example using data from Inside Airbnb (Cox n.d.). Airbnb is an online marketplace for arranging vacation rentals and places to stay. The data set, airbnb, contains listings for Vancouver, Canada, in September 2020. Our data includes an ID number, neighbourhood, type of room, the number of people the rental accommodates, number of bathrooms, bedrooms, beds, and the price per night. airbnb ## # A tibble: 4,594 x 8 ## id neighbourhood room_type accommodates bathrooms bedrooms beds price ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Downtown Entire hom… 5 2 baths 2 2 150 ## 2 2 Downtown Easts… Entire hom… 4 2 baths 2 2 132 ## 3 3 West End Entire hom… 2 1 bath 1 1 85 ## 4 4 Kensington-Ced… Entire hom… 2 1 bath 1 0 146 ## 5 5 Kensington-Ced… Entire hom… 4 1 bath 1 2 110 ## 6 6 Hastings-Sunri… Entire hom… 4 1 bath 2 3 195 ## 7 7 Renfrew-Collin… Entire hom… 8 3 baths 4 5 130 ## 8 8 Mount Pleasant Entire hom… 2 1 bath 1 1 94 ## 9 9 Grandview-Wood… Private ro… 2 1 privat… 1 1 79 ## 10 10 West End Private ro… 2 1 privat… 1 1 75 ## # … with 4,584 more rows Suppose the city of Vancouver wants information about Airbnb rentals to help plan city bylaws, and they want to know how many Airbnb places are listed as entire homes and apartments (rather than as private or shared rooms). Therefore they may want to estimate the true proportion of all Airbnb listings where the “type of place” is listed as “entire home or apartment.” Of course, we usually do not have access to the true population, but here let’s imagine (for learning purposes) that our data set represents the population of all Airbnb rental listings in Vancouver, Canada. We can find the proportion of listings where room_type == \"Entire home/apt\". library(tidyverse) library(infer) airbnb %&gt;% summarize( n = sum(room_type == &quot;Entire home/apt&quot;), proportion = sum(room_type == &quot;Entire home/apt&quot;) / nrow(airbnb) ) ## # A tibble: 1 x 2 ## n proportion ## &lt;int&gt; &lt;dbl&gt; ## 1 3434 0.747 From our data, we can see that the proportion of Entire home/apt listings in the data set is 0.747. This value, 0.747, is the population parameter. Remember, this parameter value is usually unknown in real data analysis problems. Therefore normally, we can’t compute it. Instead, perhaps we can approximate it with a small set of data. Let’s see what would happen if we were to randomly select 40 listings and count the number of entire home/apartment listings (i.e., take a random sample of size 40 from our population). Let’s use R to simulate this using our airbnb population. We can do this using the rep_sample_n function from the infer package. The arguments of rep_sample_n are (1) the data frame (or tibble) to sample from, and (2) the size of the sample to take. set.seed(123) sample_1 &lt;- rep_sample_n(tbl = airbnb, size = 40) airbnb_sample_1 &lt;- summarize(sample_1, n = sum(room_type == &quot;Entire home/apt&quot;), prop = sum(room_type == &quot;Entire home/apt&quot;) / 40 ) airbnb_sample_1 ## # A tibble: 1 x 3 ## replicate n prop ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 28 0.7 Here we see that the proportion of entire home/apartment listings in this random sample is 0.7. Wow, that’s close to our true population value! But remember, we just used a random sample of 40 so this value is an estimate — our best guess of our population parameter using this sample. Given that it is a single value that we are estimating, we often refer to it as a point estimate. The keyword there is random – if we were to take another random sample of 40 listings from the population, we wouldn’t get the same answer: set.seed(1234) sample_2 &lt;- rep_sample_n(airbnb, size = 40) airbnb_sample_2 &lt;- summarize(sample_2, n = sum(room_type == &quot;Entire home/apt&quot;), prop = sum(room_type == &quot;Entire home/apt&quot;) / 40 ) airbnb_sample_2 ## # A tibble: 1 x 3 ## replicate n prop ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 30 0.75 Confirmed! We get a different value for our estimate this time so that means that our point estimate might be unreliable! Estimates vary from sample to sample due to sampling variability. So if we were to do this again, another random sample could also give a different result. But just how much should we expect the estimates of our random samples to vary? To understand this, we will simulate taking many samples (rather than just two) of size 40 from our population of listings and calculate the proportion of entire home/apartment listings in each sample. From this simulation, will get many sample proportions, which we can visualize on a histogram. The distribution of the estimate for all possible samples of a given size (which we commonly refer to as \\(n\\)) from a population is called a sampling distribution. The sampling distribution will help us see how much we would expect our sample proportions from this population to vary for samples of size 40. Below we again use the rep_sample_n to take samples of size 40 from our population of Airbnb listings, but we set the reps argument to specify the number of samples to take, here 20,000. We will use the function tail() to see the last few rows of our samples data frame. samples &lt;- rep_sample_n(airbnb, size = 40, reps = 20000) samples ## # A tibble: 800,000 x 9 ## # Groups: replicate [20,000] ## replicate id neighbourhood room_type accommodates bathrooms bedrooms beds ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 3965 West Point G… Entire h… 2 1 bath 2 2 ## 2 1 2675 Kitsilano Private … 1 1 shared… 2 1 ## 3 1 248 Downtown Entire h… 2 1 bath 1 1 ## 4 1 4437 Strathcona Entire h… 2 1 bath 1 1 ## 5 1 3821 Riley Park Private … 4 1 shared… 2 2 ## 6 1 4382 Downtown Entire h… 4 2 baths 2 2 ## 7 1 1536 Strathcona Entire h… 4 1 bath 2 2 ## 8 1 1252 Kensington-C… Entire h… 10 3 baths 5 7 ## 9 1 2481 Downtown Private … 1 1 shared… 1 1 ## 10 1 4004 Mount Pleasa… Entire h… 4 1 bath 1 3 ## # … with 799,990 more rows, and 1 more variable: price &lt;dbl&gt; tail(samples) ## # A tibble: 6 x 9 ## # Groups: replicate [1] ## replicate id neighbourhood room_type accommodates bathrooms bedrooms beds ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 20000 514 West Point Gr… Private … 4 1 privat… 1 1 ## 2 20000 2670 Dunbar Southl… Private … 2 1.5 baths 1 1 ## 3 20000 4333 West End Entire h… 2 1 bath 1 1 ## 4 20000 1762 Downtown Entire h… 4 1 bath 2 2 ## 5 20000 1026 Kensington-Ce… Entire h… 2 1 bath 1 1 ## 6 20000 837 Sunset Entire h… 4 1 bath 2 2 ## # … with 1 more variable: price &lt;dbl&gt; Notice the column replicate indicates the replicate, or sample, with which each listing belongs. Since we took 20,000 samples of size 40, there are 20,000 replicates. Now that we have taken 20,000 samples, to create a sampling distribution of sample proportions for samples of size 40, we need to calculate the proportion of entire home/apartment listings for each sample: sample_estimates &lt;- samples %&gt;% group_by(replicate) %&gt;% summarise(sample_proportion = sum(room_type == &quot;Entire home/apt&quot;) / 40) sample_estimates ## # A tibble: 20,000 x 2 ## replicate sample_proportion ## &lt;int&gt; &lt;dbl&gt; ## 1 1 0.8 ## 2 2 0.85 ## 3 3 0.725 ## 4 4 0.75 ## 5 5 0.7 ## 6 6 0.85 ## 7 7 0.875 ## 8 8 0.75 ## 9 9 0.75 ## 10 10 0.625 ## # … with 19,990 more rows tail(sample_estimates) ## # A tibble: 6 x 2 ## replicate sample_proportion ## &lt;int&gt; &lt;dbl&gt; ## 1 19995 0.8 ## 2 19996 0.775 ## 3 19997 0.725 ## 4 19998 0.7 ## 5 19999 0.75 ## 6 20000 0.675 We calculated the proportion of entire home/apartment listings for each sample to visualize the sampling distribution of sample proportions for samples of size 40. Remember, in the real world, since we don’t have the population, we can’t actually construct the sampling distribution, but in this example, we do have it so we can actually visualize it directly. sampling_distribution &lt;- ggplot(sample_estimates, aes(x = sample_proportion)) + geom_histogram(fill = &quot;dodgerblue3&quot;, color = &quot;lightgrey&quot;, bins = 12) + xlab(&quot;Sample proportions&quot;) sampling_distribution Figure 11.2: Sampling distribution of the sample proportion for sample size 40 The sampling distribution in Figure 11.2 appears to be bell-shaped with one peak. It is centered around 0.7 and the sample proportions range from about 0.4 to about 1. In fact, we can calculate the mean of the sample proportions. sample_estimates %&gt;% summarise(mean = mean(sample_proportion)) ## # A tibble: 1 x 1 ## mean ## &lt;dbl&gt; ## 1 0.748 We notice that the sample proportions are centred around the population proportion value, 0.747! In general, the mean of the distribution of \\(\\hat{p}\\) should be equal to \\(p\\), which is good because that means the sample proportion is neither an overestimate nor an underestimate of the population proportion. Any individual sample may give you a sample proportion that is larger or smaller than the population proportion, but when we repeat the sampling process many times (as we did in this example), there is no tendency towards over or underestimating. 11.4.2 Sampling distributions for means In the previous section, our variable of interest— room_type —was categorical, and the population parameter was a proportion. As mentioned in the chapter introduction, there are many choices of the population parameter for each type of variable. What if we wanted to infer something about a population of quantitative variables instead? For instance, a traveller visiting Vancouver, BC, may wish to know about the prices of staying somewhere using Airbnb. Particularly, they might be interested in estimating the population mean price per night of Airbnb listings in Vancouver, BC. Knowing the average could help them tell whether a particular listing is overpriced. Let’s look at the case where we are interested in the population mean of a quantitative variable. We can visualize the population distribution of the price per night with a histogram. population_distribution &lt;- ggplot(airbnb, aes(x = price)) + geom_histogram(fill = &quot;dodgerblue3&quot;, color = &quot;lightgrey&quot;) + xlab(&quot;Price per night ($)&quot;) population_distribution Figure 11.3: Population distribution of price per night ($) for all Airbnb listings in Vancouver, Canada From Figure 11.3, we see that the population distribution has one peak and is skewed (i.e. not symmetric) since most of the listings are less than $250 per night, but a small proportion of listings cost more than that, creating a long tail on the histogram’s right side. Along with visualizing the population, we can calculate the population mean, the average price per night for all the Airbnb listings. population_parameters &lt;- airbnb %&gt;% summarize(pop_mean = mean(price)) population_parameters ## # A tibble: 1 x 1 ## pop_mean ## &lt;dbl&gt; ## 1 154.51 The price per night of all Airbnb rentals in Vancouver, BC is $154.51, on average. This value is our population parameter since we are calculating it using the population data. Suppose we did not have access to the population data (which is usually the case!), yet we wanted to estimate the mean price per night. We could answer this question by taking a random sample of as many Airbnb listings as our time and resources allow. Let’s say we could do this for 40 listings. What would such a sample look like? Let’s take advantage of the fact that we do have access to the population data and simulate taking one random sample of 40 listings in R, again using rep_sample_n. one_sample &lt;- airbnb %&gt;% rep_sample_n(40) We can create a histogram to visualize the distribution of observations in the sample (Figure 11.4), and calculate the mean of our sample. sample_distribution &lt;- ggplot(one_sample, aes(price)) + geom_histogram(fill = &quot;dodgerblue3&quot;, color = &quot;lightgrey&quot;) + xlab(&quot;Price per night ($)&quot;) sample_distribution Figure 11.4: Distribution of price per night ($) for sample of 40 Airbnb listings estimates &lt;- one_sample %&gt;% summarize(sample_mean = mean(price)) estimates ## # A tibble: 1 x 2 ## replicate sample_mean ## &lt;int&gt; &lt;dbl&gt; ## 1 1 149.68 From the sample of 40, the average is $149.68. This number is a point estimate for the mean of the full population. Recall that the population mean was $154.51. So our estimate was fairly close to the population parameter: the mean was about 3.1% off. Note that we usually cannot compute the estimate’s accuracy in practice since we do not have access to the population parameter; if we did, we wouldn’t need to estimate it! Also, recall from the previous section that the point estimate can vary; if we took another random sample from the population, our estimate’s value might change. So then, did we just get lucky with our point estimate above? How much does our estimate vary across different samples of size 40 in this example? Again, since we have access to the population, we can take many samples and plot the sampling distribution of sample means for samples of size 40 to get a sense for this variation. In this case, we’ll use 20,000 samples of size 40. samples &lt;- rep_sample_n(airbnb, size = 40, reps = 20000) samples ## # A tibble: 800,000 x 9 ## # Groups: replicate [20,000] ## replicate id neighbourhood room_type accommodates bathrooms bedrooms beds ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 3369 Hastings-Sun… Entire h… 4 2 baths 2 2 ## 2 1 2406 Dunbar South… Private … 1 1.5 shar… 1 1 ## 3 1 1976 Downtown Entire h… 2 1 bath 1 1 ## 4 1 2470 Hastings-Sun… Entire h… 4 1 bath 1 3 ## 5 1 2405 Hastings-Sun… Entire h… 8 2 baths 3 4 ## 6 1 1786 Dunbar South… Private … 2 1 privat… 1 1 ## 7 1 172 Kerrisdale Entire h… 4 1 bath 1 2 ## 8 1 2369 Downtown Entire h… 4 1 bath 1 3 ## 9 1 2624 Kensington-C… Entire h… 7 2 baths 4 3 ## 10 1 1572 Fairview Entire h… 2 1 bath 1 1 ## # … with 799,990 more rows, and 1 more variable: price &lt;dbl&gt; Now we can calculate the sample mean for each replicate and plot the sampling distribution of sample means for samples of size 40. sample_estimates &lt;- samples %&gt;% group_by(replicate) %&gt;% summarise(sample_mean = mean(price)) sample_estimates ## # A tibble: 20,000 x 2 ## replicate sample_mean ## &lt;int&gt; &lt;dbl&gt; ## 1 1 144.48 ## 2 2 179.96 ## 3 3 169.58 ## 4 4 185.33 ## 5 5 151.59 ## 6 6 186.18 ## 7 7 140.02 ## 8 8 128.43 ## 9 9 134.83 ## 10 10 134.93 ## # … with 19,990 more rows sampling_distribution_40 &lt;- ggplot(sample_estimates, aes(x = sample_mean)) + geom_histogram(fill = &quot;dodgerblue3&quot;, color = &quot;lightgrey&quot;) + xlab(&quot;Sample mean price per night ($)&quot;) sampling_distribution_40 Figure 11.5: Sampling distribution of the sample means for sample size of 40 In Figure 11.5, the sampling distribution of the mean has one peak and is bell-shaped. Most of the estimates are between about $140 and $170; but there are a good fraction of cases outside this range (i.e., where the point estimate was not close to the population parameter). So it does indeed look like we were quite lucky when we estimated the population mean with only 3.1% error. Let’s visualize the population distribution, distribution of the sample, and the sampling distribution on one plot to compare them in Figure 11.6. Comparing these three distributions, the centers of the distributions are all around the same price (around $150). The original population distribution has a long right tail, and the sample distribution has a similar shape to that of the population distribution. However, the sampling distribution is not shaped like the population or sample distribution. Instead, it has a bell shape, and it has a lower spread than the population or sample distributions. The sample means vary less than the individual observations because there will be some high values and some small values in any random sample, which will keep the average from being too extreme. Figure 11.6: Comparision of population distribution, sample distribution and sampling distribution Given that there is quite a bit of variation in the sampling distribution of the sample mean—i.e., the point estimate that we obtain is not very reliable—is there any way to improve the estimate? One way to improve a point estimate is to take a larger sample. To illustrate what effect this has, we will take many samples of size 20, 50, 100, and 500, and plot the sampling distribution of the sample mean. Figure 11.7: Comparision of sampling distributions Based on the visualization in Figure 11.7, two points about the sample mean become clear. First, the mean of the sample mean (across samples) is equal to the population mean. In other words, the sampling distribution is centred at the population mean. Second, increasing the size of the sample decreases the spread (i.e., the variability) of the sampling distribution. Therefore, a larger sample size results in a more reliable point estimate of the population parameter. 11.4.3 Summary A point estimate is a single value computed using a sample from a population (e.g. a mean or proportion) The sampling distribution of an estimate is the distribution of the estimate for all possible samples of a fixed size from the same population. The sample means and proportions calculated from samples are centred around the population mean and proportion, respectively. The spread of the sampling distribution is related to the sample size. As the sample size increases, the spread of the sampling distribution decreases. The shape of the sampling distribution is usually bell-shaped with one peak and centred at the population mean or proportion. 11.5 Bootstrapping 11.5.1 Overview Why all this emphasis on sampling distributions? We saw in the previous section that we could compute a point estimate of a population parameter using a sample of observations from the population. And since we had access to the population, we could evaluate how accurate the estimate was, and even get a sense of how much the estimate would vary for different samples from the population. But in real data analysis settings, we usually have just one sample from our population and do not have access to the population itself. Therefore we cannot construct the sampling distribution as we did in the previous section. As we saw, our sample estimate’s value will likely not equal the population parameter value exactly. We saw from the sampling distribution just how much our estimates can vary. So reporting a single point estimate for the population parameter alone may not be enough. Using simulations, we can see patterns of the sample estimate’s sampling distribution would look like for a sample of a given size. We can use these patterns to approximate the sampling distribution when we only have one sample, which is the realistic case. If we can “predict” what the sampling distribution would look like for a sample, we could construct a range of values we think the population parameter’s value might lie. We can use our single sample and its properties that influence sampling distributions, such as the spread and sample size, to approximate the sampling distribution as best as we can. There are several methods to do this; however, in this book, we will use the bootstrap method to do this, as we will see in this section. We will discuss interval estimation and construct confidence intervals using just a single sample from a population. A confidence interval is a range of plausible values for our population parameter. Here is the key idea. First, if you take a big enough sample, it looks like the population. Notice the histograms’ shapes for samples of different sizes taken from the population in Figure 11.8. We see that the sample’s distribution looks like that of the population for a large enough sample. Figure 11.8: Comparision of samples of different sizes from the population In the previous section, we took many samples of the same size from our population to get a sense of the variability of a sample estimate. But if our sample is big enough that it looks like our population, we can pretend that our sample is the population, and take more samples (with replacement) of the same size from it instead! This very clever technique is called the bootstrap. Note that by taking many samples from our single, observed sample, we do not obtain the true sampling distribution, but rather an approximation that we call the bootstrap distribution. Note that we need to sample with replacement when using the bootstrap. Otherwise, if we had a sample of size \\(n\\), and obtained a sample from it of size \\(n\\) without replacement, it would just return our original sample. This section will explore how to create a bootstrap distribution from a single sample using R. For a sample of size \\(n\\), the process we will go through is as follows: Randomly select an observation from the original sample, which was drawn from the population Record the observation’s value Replace that observation Repeat steps 1 - 3 (sampling with replacement) until you have \\(n\\) observations, which form a bootstrap sample Calculate the bootstrap point estimate (e.g., mean, median, proportion, slope, etc.) of the \\(n\\) observations in your bootstrap sample Repeat steps (1) - (5) many times to create a distribution of point estimates (the bootstrap distribution) Calculate the plausible range of values around our observed point estimate Figure 11.9: Overview of the bootstrap process 11.5.2 Bootstrapping in R Let’s continue working with our Airbnb data. Once again, let’s say we are interested in estimating the population mean price per night of all Airbnb listings in Vancouver, Canada, using the single sample we collected of size 40. Recall our point estimate was $149.68, and the distribution of the sample is displayed in Figure 11.10. one_sample ## # A tibble: 40 x 8 ## id neighbourhood room_type accommodates bathrooms bedrooms beds price ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 698 Downtown Entire ho… 2 1 bath 1 1 133 ## 2 2337 Grandview-Wood… Entire ho… 2 1 bath 1 2 75 ## 3 3646 Kitsilano Private r… 2 1.5 share… 4 2 93 ## 4 1534 West End Entire ho… 2 1 bath 1 1 60 ## 5 4550 Downtown Easts… Entire ho… 2 1 bath 1 1 104 ## 6 2706 Riley Park Entire ho… 4 1 bath 2 2 199 ## 7 784 Downtown Entire ho… 2 1 bath 1 1 140 ## 8 3864 Marpole Private r… 2 1 shared … 1 1 88 ## 9 3734 Sunset Entire ho… 3 1 bath 2 2 62 ## 10 3266 Oakridge Private r… 2 1 shared … 1 1 275 ## # … with 30 more rows one_sample_dist &lt;- ggplot(one_sample, aes(price)) + geom_histogram(fill = &quot;dodgerblue3&quot;, color = &quot;lightgrey&quot;) + xlab(&quot;Price per night ($)&quot;) one_sample_dist Figure 11.10: Histogram of price per night ($) for one sample of size 40 The sample distribution is skewed with a few observations out to the right. The mean of the sample is $149.68. Remember, in practice, we usually only have one sample from the population. So this sample and estimate are the only data we can work with. We now perform steps (1) - (5) listed above to generate a single bootstrap sample in R using the sample we just took, and calculate the bootstrap estimate for that sample. We will use the rep_sample_n function as we did when we were creating our sampling distribution. Since we want to sample with replacement, we change the argument for replace from its default value of FALSE to TRUE. boot1 &lt;- one_sample %&gt;% rep_sample_n(size = 40, replace = TRUE, reps = 1) boot1_dist &lt;- ggplot(boot1, aes(price)) + geom_histogram(fill = &quot;dodgerblue3&quot;, color = &quot;lightgrey&quot;) + xlab(&quot;Price per night ($)&quot;) boot1_dist Figure 11.11: Bootstrap distribution summarise(boot1, mean = mean(price)) ## # A tibble: 1 x 2 ## replicate mean ## &lt;int&gt; &lt;dbl&gt; ## 1 1 155.2 Notice in Figure 11.11 that our bootstrap distribution has a similar shape to the original sample distribution. Though the shapes of the distributions are similar, they are not identical. You’ll also notice that the original sample mean and the bootstrap sample mean differ. How might that happen? Remember that we are sampling with replacement from the original sample, so we don’t end up with the same sample values again. We are trying to mimic drawing another sample from the population without actually having to do that. Let’s now take 20,000 bootstrap samples from the original sample we drew from the population (one_sample) using rep_sample_n and calculate the means for each of those replicates. Recall that this assumes that one_sample looks like our original population; but since we do not have access to the population itself, this is often the best we can do. boot20000 &lt;- one_sample %&gt;% rep_sample_n(size = 40, replace = TRUE, reps = 20000) boot20000 ## # A tibble: 800,000 x 9 ## # Groups: replicate [20,000] ## replicate id neighbourhood room_type accommodates bathrooms bedrooms beds ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 54 Downtown Eas… Entire h… 4 1 bath 2 2 ## 2 1 4186 Downtown Entire h… 2 1 bath 1 1 ## 3 1 2894 Kensington-C… Entire h… 2 1 bath 1 1 ## 4 1 1718 West End Entire h… 3 1 bath 1 1 ## 5 1 817 Downtown Entire h… 4 2 baths 2 3 ## 6 1 214 Fairview Entire h… 2 1 bath 1 1 ## 7 1 30 Grandview-Wo… Entire h… 3 1 bath 2 2 ## 8 1 54 Downtown Eas… Entire h… 4 1 bath 2 2 ## 9 1 666 Kitsilano Entire h… 4 2 baths 2 2 ## 10 1 603 Kitsilano Entire h… 2 1.5 baths 1 1 ## # … with 799,990 more rows, and 1 more variable: price &lt;dbl&gt; tail(boot20000) ## # A tibble: 6 x 9 ## # Groups: replicate [1] ## replicate id neighbourhood room_type accommodates bathrooms bedrooms beds ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 20000 2432 Riley Park Entire h… 4 2 baths 2 2 ## 2 20000 2231 Arbutus Ridge Entire h… 3 1 bath 1 2 ## 3 20000 365 Kensington-Ce… Entire h… 2 1 bath 1 1 ## 4 20000 2261 Renfrew-Colli… Entire h… 1 1 bath 1 1 ## 5 20000 211 West End Private … 2 1 privat… 1 1 ## 6 20000 1059 Sunset Entire h… 4 1.5 baths 2 2 ## # … with 1 more variable: price &lt;dbl&gt; Let’s take a look at histograms of the first six replicates of our bootstrap samples. six_bootstrap_samples &lt;- boot20000 %&gt;% filter(replicate &lt;= 6) ggplot(six_bootstrap_samples, aes(price)) + geom_histogram(fill = &quot;dodgerblue3&quot;, color = &quot;lightgrey&quot;) + xlab(&quot;Price per night ($)&quot;) + facet_wrap(~replicate) Figure 11.12: Histograms of first six replicates of bootstrap samples We see in Figure 11.12 how the bootstrap samples differ. We can also calculate the sample mean for each of these six replicates. six_bootstrap_samples %&gt;% group_by(replicate) %&gt;% summarize(mean = mean(price)) ## # A tibble: 6 x 2 ## replicate mean ## &lt;int&gt; &lt;dbl&gt; ## 1 1 152.15 ## 2 2 141.7 ## 3 3 148.45 ## 4 4 136.88 ## 5 5 139.78 ## 6 6 134.52 We can see that the bootstrap sample distributions and the sample means are different. They are different because we are sampling with replacement. We will now calculate point estimates for our 20,000 bootstrap samples and generate a bootstrap distribution of our point estimates. The bootstrap distribution suggests how we might expect our point estimate to behave if we took another sample. boot20000_means &lt;- boot20000 %&gt;% group_by(replicate) %&gt;% summarize(mean = mean(price)) boot20000_means ## # A tibble: 20,000 x 2 ## replicate mean ## &lt;int&gt; &lt;dbl&gt; ## 1 1 152.15 ## 2 2 141.7 ## 3 3 148.45 ## 4 4 136.88 ## 5 5 139.78 ## 6 6 134.52 ## 7 7 141.88 ## 8 8 141.78 ## 9 9 157.93 ## 10 10 147.88 ## # … with 19,990 more rows tail(boot20000_means) ## # A tibble: 6 x 2 ## replicate mean ## &lt;int&gt; &lt;dbl&gt; ## 1 19995 164.58 ## 2 19996 172.85 ## 3 19997 142.82 ## 4 19998 145.28 ## 5 19999 152.85 ## 6 20000 133.4 boot_est_dist &lt;- ggplot(boot20000_means, aes(x = mean)) + geom_histogram(fill = &quot;dodgerblue3&quot;, color = &quot;lightgrey&quot;) + xlab(&quot;Sample mean price per night ($)&quot;) boot_est_dist Figure 11.13: Distribution of the bootstrap sample means Let’s compare our bootstrap distribution with the true sampling distribution (taking many samples from the population). Figure 11.14: Comparison of distribution of the bootstrap sample means and sampling distribution There are two essential points that we can take away from Figure 11.14. First, the shape and spread of the true sampling distribution and the bootstrap distribution are similar; the bootstrap distribution lets us get a sense of the point estimate’s variability. The second important point is that the means of these two distributions are different. The sampling distribution is centred at $154.51, the population mean value. However, the bootstrap distribution is centred at the original sample’s mean price per night, $149.56. Because we are resampling from the original sample repeatedly, we see that the bootstrap distribution is centred at the original sample’s mean value (unlike the sampling distribution of the sample mean, which is centred at the population parameter value). Figure 11.15 summarizes the bootstrapping process. The idea here is that we can use this distribution of bootstrap sample means to approximate the sampling distribution of the sample means when we only have one sample. Since the bootstrap distribution pretty well approximates the sampling distribution spread, we can use the bootstrap spread to help us develop a plausible range for our population parameter along with our estimate! Figure 11.15: Summary of bootstrapping process 11.5.3 Using the bootstrap to calculate a plausible range Now that we have constructed our bootstrap distribution, let’s use it to create an approximate 95% percentile bootstrap confidence interval. A confidence interval is a range of plausible values for the population parameter. We will find the range of values covering the middle 95% of the bootstrap distribution, giving us a 95% confidence interval. You may be wondering, what does “95% confidence” mean? If we took 100 random samples and calculated 100 95% confidence intervals, then about 95% of the ranges would capture the population parameter’s value. Note there’s nothing special about 95%. We could have used other levels, such as 90% or 99%. There is a balance between our level of confidence and precision. A higher confidence level corresponds to a wider range of the interval, and a lower confidence level corresponds to a narrower range. Therefore the level we choose is based on what chance we are willing to take of being wrong based on the implications of being wrong for our application. In general, we choose confidence levels to be comfortable with our level of uncertainty but not so strict that the interval is unhelpful. For instance, if our decision impacts human life and the implications of being wrong are deadly, we may want to be very confident and choose a higher confidence level. To calculate our 95% percentile bootstrap confidence interval, we will do the following: Arrange the observations in the bootstrap distribution in ascending order Find the value such that 2.5% of observations fall below it (the 2.5% percentile). Use that value as the lower bound of the interval Find the value such that 97.5% of observations fall below it (the 97.5% percentile). Use that value as the upper bound of the interval To do this in R, we can use the quantile() function: bounds &lt;- boot20000_means %&gt;% select(mean) %&gt;% pull() %&gt;% quantile(c(0.025, 0.975)) bounds ## 2.5% 97.5% ## 128 175 Our interval, $127.5 to $174.93, captures the middle 95% of the sample mean prices in the bootstrap distribution. We can visualize the interval on our distribution in Figure 11.16. Figure 11.16: Distribution of the bootstrap sample means with percentile lower and upper bounds To finish our estimation of the population parameter, we would report the point estimate and our confidence interval’s lower and upper bounds. Here the sample mean price-per-night of 40 Airbnb listings was $149.68, and we are 95% “confident” that the true population mean price-per-night for all Airbnb listings in Vancouver is between $(127.5, 174.93). Notice that our interval does indeed contain the true population mean value, $154.51! However, in practice, we would not know whether our interval captured the population parameter or not because we usually only have a single sample, not the entire population. However, this is the best we can do when we only have one sample! This chapter is only the beginning of the journey into statistical inference. We can extend the concepts learned here to do much more than report point estimates and confidence intervals, such as testing for real differences between populations, tests for associations between variables, and so much more! We have just scratched the surface of statistical inference; however, the material presented here will serve as the foundation for more advanced statistical techniques you may learn about in the future! 11.6 Additional resources Chapters 7 to 10 of Modern Dive provide a great next step in learning about inference. In particular, Chapters 7 and 8 cover sampling and bootstrapping using tidyverse and infer in a slightly more in-depth manner than the present chapter. Chapters 9 and 10 take the next step beyond the scope of this chapter and begin to provide some of the initial mathematical underpinnings of inference and more advanced applications of the concept of inference in testing hypotheses and performing regression. This material offers a great starting point for getting more into the technical side of statistics. Chapters 4 to 7 of OpenIntro Statistics - Fourth Edition provide a good next step after Modern Dive. Although it is still certainly an introductory text, things get a bit more mathematical here. Depending on your background, you may actually want to start going through Chapters 1 to 3 first, where you will learn some fundamental concepts in probability theory. Although it may seem like a diversion, probability theory is the language of statistics; if you have a solid grasp of probability, more advanced statistics will come naturally to you! References "],["Getting-started-with-Jupyter.html", "Chapter 12 Getting started with Jupyter 12.1 Overview 12.2 Chapter learning objectives 12.3 Jupyter 12.4 Code cells 12.5 Markdown cells 12.6 Saving your work 12.7 Best practices for running a notebook 12.8 Exploring data files 12.9 Exporting to a different file format 12.10 Creating a new Jupyter notebook 12.11 Additional resources", " Chapter 12 Getting started with Jupyter 12.1 Overview To be able to effectively and efficiently write R code for your analysis you will need access to the R programming language on a computer. This chapter will show you how to use the R programming language with a common coding platform in data science, Jupyter. In this chapter, we will show you how to use this platform via a web-interface for ease of getting going quickly. In a later chapter, we will also show you how to install and configure this on your local computer (i.e., laptop). These skills are essential to getting your analysis running; think of it like getting dressed in the morning! 12.2 Chapter learning objectives By the end of the chapter, students will be able to: use Jupyter to run, edit and write R code open and view plain text data files in Jupyter create new Jupyter notebooks export Jupyter notebooks to other standard file types (e.g., .html, .pdf) 12.3 Jupyter Jupyter is a web-based interactive development environment for creating, editing and executing documents called Jupyter notebooks. Jupyter notebooks are documents that contain a mix of computer code (and its output) and formattable text. Given that they combine these two analysis artifacts in a single document—code is not separate from the output or written report—notebooks are one of the leading tools to create reproducible data analyses. Reproducible data analysis is one where you can reliably and easily recreate the same results when analyzing the same data. Although this sounds like something that should always be true of any data analysis, in reality, this is not often the case; one needs to make a conscious effort to perform data analysis in a reproducible manner. The name Jupyter came from combining the names of the three programming languages that it was initially targeted for (Julia, Python, and R), and now many other languages can be used with Jupyter notebooks. An example of what a Jupyter notebook looks like is shown in Figure ??. Figure 12.1: A screenshot of a Jupyter Notebook. 12.3.1 Accessing Jupyter One of the easiest ways to start computing in R with Jupyter is to use a web-based platform called JupyterHub that already has Jupyter, R, a number of R packages, and collaboration tools installed, configured and ready to use. JupyterHub’s are usually created and provisioned by organizations (for example, a university, a company, etcetera) and require authentication to gain access. Jupyter can also be installed on your own computer (desktop or laptop), and we provide examples of how to do this in chapter 13. 12.4 Code cells The sections of a Jupyter notebook that contain code in the R programming language are referred to as code cells. A code cell that has not yet been executed has no number inside the square brackets to the left of the cell (Figure 12.2). Running a code cell will execute all of the code it contains, and the output (if any exists) will be displayed directly underneath the code that generated it. Outputs may include printed text or numbers, data frames and data visualizations. Cells that have been executed also have a number inside the square brackets to the left of the cell. This number indicates the order in which the cells were run (Figure 12.3). Figure 12.2: A code cell in Jupyter that has not yet been executed. Figure 12.3: A code cell in Jupyter that has been executed. 12.4.1 Executing code cells Code cells can be run independently or as part of executing the entire notebook using one of the “Run all” commands found in the Run or Kernel menus in Jupyter. Running a single code cell independently is a workflow typically used when editing or writing your own R code. Executing an entire notebook is a workflow typically used to ensure that your analysis runs in its entirety before sharing it with others, and when using a notebook as part of an automated process. To run the a code cell independently, the cell needs to first be activated. This is done by clicking on it with the cursor. Jupyter will indicate a cell has been activated by highlighting it with a blue rectangle to its left. After the cell has been activated, the cell can be run by either pressing the Run (▶) button in the Jupyter notebook tab menu, or by using a keyboard shortcut of Shift + Enter. Figure 12.4: An activated cell that is ready to be be run. The red arrow points to blue rectangle to the cell’s left. The blue rectangle indicates that it is ready to be run. This can be done by clicking the run button (circled in red). To execute all of the code cells in an entire notebook, you can either select: Run &gt;&gt; Run All Cells Kernel &gt;&gt; Restart Kernel and Run All Cells… from the Jupyter menu Click the Restart the kernel, then re-run the whole notebook button (▶▶) in the Jupyter notebook tab menu. All of these commands result in all of the code cells in a notebook being run, however only the Restart Kernel and Run All Cells… command and the ▶▶ button will restart the R session before running all of the cells. Restarting the R session before running all of the cells means that all previous objects that were created from running cells before this command was run will be deleted. This command emulates how your notebook code would run if you completely restarted Jupyter before executing your entire notebook. Figure 12.5: Restarting the R session can be accomplished by clicking Restart Kernel and Run All Cells… The Kernel The kernel is a program that executes the code inside your notebook and outputs the results. Kernels for many different programming languages have been created for Jupyter, which means that Jupyter can interpret and execute the code of many different programming languages. To run R code, your notebook will need an R kernel. In the top right of your window, you can see a circle that indicates the status of your kernel. If the circle is empty (⚪), the kernel is idle and ready to execute code. If the circle is filled in (⚫), the kernel is busy running some code. You may run into problems where your kernel is stuck for an excessive amount of time, your notebook is very slow and unresponsive, or your kernel loses its connection. If this happens, try the following steps: At the top of your screen, click Kernel, then Interrupt Kernel. If that doesn’t help, click Kernel, then Restart Kernel…. If you do this, you will have to run your code cells from the start of your notebook up until where you paused your work. If that doesn’t help, restart your server. First, save your work by clicking File at the top left of your screen, then Save Notebook. Next, from the File menu click Hub Control Panel. Choose Stop My Server to shut it down, then the My Server button to start it back up. Then, navigate back to the notebook you were working on. 12.4.2 Creating new code cells To create a new code cell in Jupyter, click the + button in the Jupyter notebook tab menu. By default, all new cells in Jupyter start out as code cells, so after this, all you have to do is write R code within the new cell you just created! Figure 12.6: New cells can be created by clicking the + button, and are by default code cells. 12.5 Markdown cells Text cells inside a Jupyter notebook are called Markdown cells. Markdown cells are rich formatted text cells, which means you can bold and italicize text, create subject headers, format bullet and numbered lists. These cells are given the name “Markdown” because they use the Markdown markup language to specify the rich text formatting. A markup language is a human readable computer language for specifying how plain text should be richly formatted by the program that renders it. You do not need to learn Markdown to write text in the Markdown cells in Jupyter, plain text will work just fine. However, you might want to learn a bit about the Markdown markup language eventually. Teaching the markdown formatting language is beyond the scope of this book. However, If you are keen to learn more, a good place to start is this cheatsheet by Commonmark: https://commonmark.org/help/, as well as their tutorial: https://commonmark.org/help/tutorial/. 12.5.1 Editing Markdown cells To edit a Markdown cell in Jupyter, you need to double click on any Markdown cell and the unformatted (we call this unrendered) version of the text will be shown. You can then use your keyboard to edit the text. To view the formatted (we call this rendered) text, click the Run (▶) button in the Jupyter notebook tab menu, or use the Shift + Enter keyboard shortcut. Figure 12.7: A Markdown cell in Jupyter that has not yet been rendered and can be edited. Figure 12.8: A Markdown cell in Jupyter that has been rendered and exhibits rich text formatting. 12.5.2 Creating new Markdown cells To create a new Markdown cell in Jupyter, click the + button in the Jupyter notebook tab menu. By default, all new cells in Jupyter start as code cells, so the cell format needs to be changed to be recognized and rendered as a Markdown cell. To do this, click on the cell with your cursor to ensure it is activated, and then click on the drop-down box next to the Restart the kernel, then re-run the whole notebook button (▶▶) in the Jupyter notebook tab menu and changing it from “Code” to “Markdown.” Figure 12.9: New cells are by default code cells. To create Markdown cells, the cell format must be changed. 12.6 Saving your work As with any file you work on, it is critical to save your work often so you don’t lose your progress! Jupyter has an autosave feature, where open files are saved periodically. The default for this is every two minutes. You can also manually save a Jupyter notebook by selecting Save Notebook from the File menu, by clicking the dick icon on the tab menu of the Jupyter notebook, or by using a keyboard shortcut (Control + S for Windows, or Command + S for Mac OS). 12.7 Best practices for running a notebook 12.7.1 Best practices for executing code cells As you might know (or at least imagine) by now, Jupyter notebooks are great for interactively editing, writing and running R code - this is what they were designed for. Consequently, Jupyter notebooks are flexible in regards to code cell execution order. This flexibility means that code cells can be run in any arbitrary order using the Run (▶) button. This flexibility has a downside since it can lead to Jupyter notebooks whose code cannot be executed in a linear order (from top to bottom of the notebook). A non-linear notebook is problematic because a linear order is the conventional way code documents are run, and others will have this expectation when running your notebook. Finally, if the code is used in some automated process, it will need to run in a linear order, from top to bottom of the notebook. Even with the best intentions, the flexibility of a Jupyter notebook can sometimes cause us to misstep and write non-linear code. This misstep is most often due to relying solely on using the ▶ button to execute cells. For example, suppose you write some R code that creates an R object, say a variable named y. When you execute that cell and create y, it will continue to exist until it is deliberately deleted with R code, or when the Jupyter notebook R session (i.e., kernel) is stopped or restarted. It can also be referenced in another distinct code cell (Figure 12.10). Together, this means that you could then write a code cell further above in the notebook that references y and execute it without error in the current session (Figure 12.11). This could also be done successfully in future sessions if, and only if, you run the cells in the same non-conventional order. However, it is difficult to remember this non-conventional order, and it is not the order that others would expect your code to be executed in. Thus in future this would lead to errors when the notebook is run in the conventional linear order (Figure 12.12). Figure 12.10: Code that was written out of order, but not yet executed. Figure 12.11: Code that was written out of order, and was executed using the run button in a non-linear error without error. The order of execution can be traced by following the numbers to the left of the code cells; their order indicates the order in which the cells were executed. Figure 12.12: Code that was written out of order, and was executed in a linear order using “Restart Kernel and Run All Cells…” This resulted in an error at the execution of the second code cell and it failed to run all code cells in the notebook. Another thing that the flexibility of a Jupyter notebooks allows you to do is create objects through running a cell, which later gets deleted. In such a scenario, that object only exists for that one particular R session and would not exist again if the notebook session was restarted and the notebook run again. If that object was referenced in another cell in that notebook, an error would likely occur when the Jupyter notebook was run again in a new session. These events may not negatively affect the current R session when the code was being written, but as you might now see, they will likely lead to errors when that notebook is run in a future session. Regularly, and intentionally executing the entire notebook in a fresh R session will help guard against this by letting a user know that some aspect of the code does not allow the notebook to be run linearly. Knowing this sooner than later will allow the user to fix the issue quickly and easily, ideally within that session. We recommend as a best practice to run the entire notebook in a fresh R session, at least 2-3 times within any work session on a Jupyter notebook. Note that a key thing about our best practice recommendation here is not just running all the cells, but also doing this in a fresh R session. To do this in Jupyter, we recommend using either the Kernel &gt;&gt; Restart Kernel and Run All Cells… command from the Jupyter menu or the ▶▶ button in the Jupyter notebook tab menu. The Run &gt;&gt; Run All Cells command from the Jupyter menu will run all the cells, but it will not do this a fresh R session, and so it is not sufficient to guard against these errors. 12.7.2 Best practices for including R packages in notebooks Most data analyses these days depend on functions from external R packages that are not built-in to R. One example is the tidyverse metapackage that we heavily rely on in this book. This package provides us access to functions like read_csv for reading data, select for subsetting columns, and ggplot for creating high-quality graphics. As mentioned earlier in the book, external R packages need to be loaded before the functions they contain can be used. Our recommended way to do this is via library(package_name). But where should this line of code be written in a Jupyter notebook? One idea could be to load the library right before the function is used in the notebook. However, although this technically works, this causes hidden, or at least non-obvious, R package dependencies when others view or try to run the notebook. These hidden dependencies can lead to errors when the notebook is executed on another computer if the needed R packages are not installed. Additionally, if the data analysis code takes a long time to run, uncovering the hidden dependencies that need to be installed so that the analysis can run without error can take a great deal of time to uncover. Therefore, we recommend you load all R packages in a code cell near the top of the Jupyter notebook. Loading all your packages at the start ensures that all packages are loaded before their functions are called (assuming the notebook is run in a linear order from top to bottom, as recommended above) and makes it easy for others viewing or running the notebook to see what external R packages were used in the analysis, and hence, what packages they should install on their computer to run the analysis successfully. 12.7.3 Summary of best practices for running a notebook Write code so that it can be executed in a linear order. As you write code in a Jupyter notebook, run the notebook in a linear order and in it’s entirety often (2-3 times every work session) via the Kernel &gt;&gt; Restart Kernel and Run All Cells… command from the Jupyter menu or the ▶▶ button in the Jupyter notebook tab menu. Write the code that loads external R packages near the top of the Jupyter notebook. 12.8 Exploring data files It is essential to preview data files before you try to read them into R to see whether or not there are column names, what the delimiters are, and if there are lines you need to skip. In Jupyter, you preview data files stored as plain text files (e.g., comma- and tab-separated files) in their plain text format by right-clicking on the file’s name in the Jupyter file explorer and selecting Open with and then selecting Editor. Suppose you do not specify to open the data file with an editor. In that case, Jupyter will render a nice table for you, and you will not be able to see the column delimiters, and therefore you will not know which function to use, nor which arguments to use and values to specify for them. Figure 12.13: Opening data files with an editor in Jupyter. Figure 12.14: A data file as viewed in an editor in Jupyter. 12.9 Exporting to a different file format In Jupyter, viewing, editing and running R code is done in the Jupyter notebook (file extension .ipynb) file format. This file format is not easy to open and view outside of Jupyter. Thus, to share your analysis with people who do not commonly use Jupyter, it is recommended that you export your executed analysis as a more common file type, such as an .html file, or a .pdf. We recommend exporting the Jupyter notebook after executing the analysis so that you can also share the outputs of your code. 12.9.1 Exporting to HTML The .html file format will result in a shareable file that anyone can open using a web browser (e.g., Firefox, Safari, Chrome, Edge, etc.). It will result in a document that is visually similar to what the Jupyter notebook looked like inside Jupyter. One point of caution here is that if there are images in your Jupyter notebook, you will need to share the image files and the .html file to see them. 12.9.2 Exporting to PDF The .pdf file format will result in a shareable file that anyone can open using many programs, including Adobe Acrobat, Preview, web browsers and many more. The benefit of exporting to PDF is that it is a standalone document, even if the Jupyter notebook included references to image files. The conventional route for exporting Jupyter notebooks to .pdf files depends on a powerful tool called LaTeX. The default settings will result in a document that visually looks quite different from what the Jupyter notebook looked like inside Jupyter regarding font and page margins. The formatting can be customized, however, this requires some learning of the LaTeX markup language. 12.10 Creating a new Jupyter notebook At some point, you will want to create a new, fresh Jupyter notebook for your own project instead of viewing, running or editing a notebook that was started by someone else. To do this, navigate to the Launcher tab, and click on the R icon under the Notebook heading. If no Launcher tab is visible, you can get a new one via clicking the + button at the top of the Jupyter file explorer. Figure 12.15: Clicking on the R icon under the Notebook heading will create a new Jupyter notebook with an R kernel. Once you have created a new Jupyter notebook, be sure to give it a descriptive name, as the default file name is Untitle.ipynb. You can rename files by right-clicking on the file name of the Jupyter notebook you just created and click Rename. This will make the file name editable. Use your keyboard to change the name. Pressing Enter or clicking anywhere else in the Jupyter the interface will save the changed file name. We recommend not using white space or non-standard characters in file names. Doing so will not prevent you from using that file in Jupyter. However, these sorts of things become troublesome as you start to do more advanced data science projects that involve repetition and automation. We recommend naming files using lower case characters and separating words by a dash (-) or an underscore (_). 12.11 Additional resources The JupyterLab Documentation "],["move-to-your-own-machine.html", "Chapter 13 Moving to your own machine 13.1 Overview 13.2 Chapter learning objectives 13.3 Installing software on your own computer 13.4 Moving files to your computer", " Chapter 13 Moving to your own machine 13.1 Overview Throughout this book, we have assumed that you are working on a web-based platform (e.g., JupyterHub) that already has Jupyter, R, a number of R packages, and Git set up and ready to use. In this chapter, you’ll learn how to install all of that software on your own computer in case you don’t have a preconfigured JupyterHub available to you. 13.2 Chapter learning objectives By the end of the chapter, students will be able to: install Git and the miniconda Python distribution install and launch a local instance of JupyterLab with the R kernel download files from a JupyterHub for later local use 13.3 Installing software on your own computer In this section we will provide instructions for installing the software required by this book on our own computer. Given that installation instructions can vary widely based on the computer setup we have created instructions for multiple operating systems. In particular, the installation instructions below have been verified to work on a computer that: runs one of the following operating systems: MacOS 10.15.X (Catalina); Ubuntu 20.04; Windows 10, version 2004. can connect to networks via a wireless connection uses a 64-bit CPU uses English as the default language For macOS users only: Apple recently changed the default shell in the terminal to Zsh. However, the programs we need work better with the Bash shell. Thus, we recomend you change the default shell to Bash by opening the terminal (how to video) and typing: chsh -s /bin/bash You will have to quit all instances of open terminals and then restart the terminal for this to take effect. 13.3.1 Git As shown in the version control chapter, Git is a very useful tool for version controlling your projects, as well as sharing your work with others. Windows: To install Git on Windows go to https://git-scm.com/download/win and download the windows version of git. Once the download has finished, run the installer and accept the default configuration for all pages. MacOS: To install Git on Mac OS open the terminal and type the following command: xcode-select --install Ubuntu: To install Git on Ubuntu open the terminal and type the following commands: sudo apt update sudo apt install git 13.3.2 Miniconda To run Jupyter notebooks on our computers we will need to install a program similar to the one we used as our web-based platform. One such program is JupyterLab. But JupyterLab relies on Python; we can install this via the miniconda Python package distribution. Windows: To install miniconda on Windows, download the Python 3.8 64-bit version from here. Once the download has finished, run the installer and accept the default configuration for all pages. After installation, you can open the Anaconda Prompt by opening the Start Menu and searching for the program called “Anaconda Prompt (miniconda3).” When this opens you will see a prompt similar to (base) C:\\Users\\your_name. MacOS: To install miniconda on MacOS, download the Python 3.8 64-bit version from here. After the download has finished, run the installer and accept the default configuration for all pages. Ubuntu: To install miniconda on Ubuntu, we first download the Python 3.8 64-bit version from here. After the download has finished, open the terminal and execute the following commands: bash path/to/Miniconda3-latest-Linux-x86_64.sh Note: most often this file is downloaded to the Downloads directory, and thus the command will look like this: bash Downloads/Miniconda3-latest-Linux-x86_64.sh The instructions for the installation will then appear: Press Enter. Once the licence agreement shows, you can press space scroll down, or press q to skip reading it. Type yes and press enter to accept the licence agreement. Press enter to accept the default installation location. Type yes and press enter to instruct the installer to run conda init, which makes conda available from the terminal/shell. 13.3.3 JupyterLab With miniconda set up, we can now install JupyterLab and the Jupyter Git extension. Type the following into the Anaconda Prompt (Windows) or the terminal (MacOS and Ubuntu) and press enter: conda install -c conda-forge -y jupyterlab conda install -y nodejs=10.* pip install --upgrade jupyterlab-git jupyter lab build To test that your JupyterLab installation is functional, you can type jupyter lab into the Anaconda Prompt (Windows) or terminal (MacOS and Ubuntu) and press enter. This should open a new tab in your default browser with the JupyterLab interface. To exit out of JupyterLab you can click File -&gt; Shutdown, or go to the terminal from which you launched JupyterLab, hold Ctrl, and press c twice. 13.3.4 R and the IRkernel To have R available to you in JupyterLab, you will need to install the R programming language and the IRkernel. To install these, type the following into the Anaconda Prompt (Windows) or terminal (MacOS and Ubuntu): conda install -c conda-forge -y r-base conda install -c conda-forge -y r-irkernel To improve the experience of using R in JupyterLab, we will add an extension that allows us to setup keyboard shortcuts for inserting text. By default, this extension creates shortcuts for inserting two of the most common R operators: &lt;- and %&gt;%. Type the following in the Anaconda Prompt (Windows) or terminal (MacOS and Ubuntu) and press enter: jupyter labextension install @techrah/text-shortcuts jupyter lab build 13.3.5 R packages To install the packages used in this book, type the following in the Anaconda Prompt (Windows) or terminal (MacOS and Ubuntu) and press enter: conda install -c conda-forge -y \\ r-cowplot \\ r-ggally \\ r-gridextra \\ r-infer \\ r-kknn \\ r-rodbc \\ r-rpostgres \\ r-rsqlite \\ r-testthat \\ r-tidymodels \\ r-tinytex \\ unixodbc 13.3.6 LaTeX To be able to render .ipynb files to .pdf you need to install a LaTeX distribution. These can be quite large, so we will opt to use tinytex, a light-weight cross-platform, portable, and easy-to-maintain LaTeX distribution based on TeX Live. To install it open JupyterLab by typing jupyter lab in the Anaconda Prompt (Windows) or terminal (MacOS and Ubuntu) and press enter. Then from JupyterLab open an R console and type the commands listed below and press Shift + enter to install tinytex: tinytex::install_tinytex() tinytex::tlmgr_install(c(&quot;eurosym&quot;, &quot;adjustbox&quot;, &quot;caption&quot;, &quot;collectbox&quot;, &quot;enumitem&quot;, &quot;environ&quot;, &quot;fp&quot;, &quot;jknapltx&quot;, &quot;ms&quot;, &quot;oberdiek&quot;, &quot;parskip&quot;, &quot;pgf&quot;, &quot;rsfs&quot;, &quot;tcolorbox&quot;, &quot;titling&quot;, &quot;trimspaces&quot;, &quot;ucs&quot;, &quot;ulem&quot;, &quot;upquote&quot;)) 13.4 Moving files to your computer In the course that uses this textbook, students work on a web-based platform (a JupyterHub) to do their course work. This section is to help students save their work from this platform at the end of the course. First in JupyterHub, open a terminal by clicking “terminal” in the Launcher tab. Next, type the following in the terminal to create a compressed .zip archive for the course work you are interested in downloading: zip -r course_folder.zip your_course_folder After the compressing process is complete, right-click on course_folder.zip in the JupyterHub file browser and click “Download.” You should be able to use your computer’s software to unzip the compressed folder by double-clicking on it. "],["references.html", "References", " References "]]
