<!DOCTYPE html>

<html lang="" xml:lang="">
<head>
<meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<title>Chapter 9 Clustering |  Data Science</title>
<meta content="This is a textbook for teaching a first introduction to data science." name="description"/>
<meta content="bookdown 0.34 and GitBook 2.6.7" name="generator"/>
<meta content="Chapter 9 Clustering |  Data Science" property="og:title"/>
<meta content="book" property="og:type"/>
<meta content="This is a textbook for teaching a first introduction to data science." property="og:description"/>
<meta content="UBC-DSCI/introduction-to-datascience" name="github-repo"/>
<meta content="summary" name="twitter:card"/>
<meta content="Chapter 9 Clustering |  Data Science" name="twitter:title"/>
<meta content="This is a textbook for teaching a first introduction to data science." name="twitter:description"/>
<meta content="Tiffany Timbers, Trevor Campbell, and Melissa Lee" name="author"/>
<meta content="2024-12-30" name="date"/><meta content="2024-08-21" name="date"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="yes" name="apple-mobile-web-app-capable"/>
<meta content="black" name="apple-mobile-web-app-status-bar-style"/>
<link href="regression2.html" rel="prev"/>
<link href="inference.html" rel="next"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet"/>
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet"/>
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet"/>
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet"/>
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet"/>
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet"/>
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet"/>
<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet"/>
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet"/>
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet"/>
<script src="libs/htmlwidgets-1.6.2/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.10.2/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet"/>
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet"/>
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script>
<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-JJ9MD0LBF5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-JJ9MD0LBF5');
</script>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>
<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>
<link href="source/style.css" rel="stylesheet" type="text/css"/>
<script src="website_diff.js"></script><link href="website_diff.css" rel="stylesheet" type="text/css"/></head>
<body>
<div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">
<div class="book-summary">
<nav role="navigation">
<ul class="summary">
<li><a href="https://datasciencebook.ca">Data Science: A First Introduction</a></li>
<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a class="link-to-diff" href="index.html"><i class="fa fa-check"></i>Welcome!</a></li>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html"><i class="fa fa-check"></i>Foreword</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a class="link-to-diff" href="preface.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the authors</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a class="link-to-diff" href="intro.html"><i class="fa fa-check"></i><b>1</b> R and the Tidyverse</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a class="link-to-diff" href="intro.html#overview"><i class="fa fa-check"></i><b>1.1</b> Overview</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a class="link-to-diff" href="intro.html#chapter-learning-objectives"><i class="fa fa-check"></i><b>1.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a class="link-to-diff" href="intro.html#canadian-languages-data-set"><i class="fa fa-check"></i><b>1.3</b> Canadian languages data set</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a class="link-to-diff" href="intro.html#asking-a-question"><i class="fa fa-check"></i><b>1.4</b> Asking a question</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a class="link-to-diff" href="intro.html#loading-a-tabular-data-set"><i class="fa fa-check"></i><b>1.5</b> Loading a tabular data set</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a class="link-to-diff" href="intro.html#naming-things-in-r"><i class="fa fa-check"></i><b>1.6</b> Naming things in R</a></li>
<li class="chapter" data-level="1.7" data-path="intro.html"><a class="link-to-diff" href="intro.html#creating-subsets-of-data-frames-with-filter-select"><i class="fa fa-check"></i><b>1.7</b> Creating subsets of data frames with <code>filter</code> &amp; <code>select</code></a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="intro.html"><a class="link-to-diff" href="intro.html#using-filter-to-extract-rows"><i class="fa fa-check"></i><b>1.7.1</b> Using <code>filter</code> to extract rows</a></li>
<li class="chapter" data-level="1.7.2" data-path="intro.html"><a class="link-to-diff" href="intro.html#using-select-to-extract-columns"><i class="fa fa-check"></i><b>1.7.2</b> Using <code>select</code> to extract columns</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="intro.html"><a class="link-to-diff" href="intro.html#arrangesliceintro"><i class="fa fa-check"></i><b>1.8</b> Using <code>arrange</code> to order and <code>slice</code> to select rows by index number</a></li>
<li class="chapter" data-level="1.9" data-path="intro.html"><a class="link-to-diff" href="intro.html#adding-and-modifying-columns-using-mutate"><i class="fa fa-check"></i><b>1.9</b> Adding and modifying columns using <code>mutate</code></a></li>
<li class="chapter" data-level="1.10" data-path="intro.html"><a class="link-to-diff" href="intro.html#exploring-data-with-visualizations"><i class="fa fa-check"></i><b>1.10</b> Exploring data with visualizations</a>
<ul>
<li class="chapter" data-level="1.10.1" data-path="intro.html"><a class="link-to-diff" href="intro.html#using-ggplot-to-create-a-bar-plot"><i class="fa fa-check"></i><b>1.10.1</b> Using <code>ggplot</code> to create a bar plot</a></li>
<li class="chapter" data-level="1.10.2" data-path="intro.html"><a class="link-to-diff" href="intro.html#formatting-ggplot-objects"><i class="fa fa-check"></i><b>1.10.2</b> Formatting ggplot objects</a></li>
<li class="chapter" data-level="1.10.3" data-path="intro.html"><a class="link-to-diff" href="intro.html#putting-it-all-together"><i class="fa fa-check"></i><b>1.10.3</b> Putting it all together</a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="intro.html"><a class="link-to-diff" href="intro.html#accessing-documentation"><i class="fa fa-check"></i><b>1.11</b> Accessing documentation</a></li>
<li class="chapter" data-level="1.12" data-path="intro.html"><a class="link-to-diff" href="intro.html#exercises"><i class="fa fa-check"></i><b>1.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="reading.html"><a class="link-to-diff" href="reading.html"><i class="fa fa-check"></i><b>2</b> Reading in data locally and from the web</a>
<ul>
<li class="chapter" data-level="2.1" data-path="reading.html"><a class="link-to-diff" href="reading.html#overview-1"><i class="fa fa-check"></i><b>2.1</b> Overview</a></li>
<li class="chapter" data-level="2.2" data-path="reading.html"><a class="link-to-diff" href="reading.html#chapter-learning-objectives-1"><i class="fa fa-check"></i><b>2.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="2.3" data-path="reading.html"><a class="link-to-diff" href="reading.html#absolute-and-relative-file-paths"><i class="fa fa-check"></i><b>2.3</b> Absolute and relative file paths</a></li>
<li class="chapter" data-level="2.4" data-path="reading.html"><a class="link-to-diff" href="reading.html#reading-tabular-data-from-a-plain-text-file-into-r"><i class="fa fa-check"></i><b>2.4</b> Reading tabular data from a plain text file into R</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="reading.html"><a class="link-to-diff" href="reading.html#readcsv"><i class="fa fa-check"></i><b>2.4.1</b> <code>read_csv</code> to read in comma-separated values files</a></li>
<li class="chapter" data-level="2.4.2" data-path="reading.html"><a class="link-to-diff" href="reading.html#skipping-rows-when-reading-in-data"><i class="fa fa-check"></i><b>2.4.2</b> Skipping rows when reading in data</a></li>
<li class="chapter" data-level="2.4.3" data-path="reading.html"><a class="link-to-diff" href="reading.html#read_tsv-to-read-in-tab-separated-values-files"><i class="fa fa-check"></i><b>2.4.3</b> <code>read_tsv</code> to read in tab-separated values files</a></li>
<li class="chapter" data-level="2.4.4" data-path="reading.html"><a class="link-to-diff" href="reading.html#read_delim-as-a-more-flexible-method-to-get-tabular-data-into-r"><i class="fa fa-check"></i><b>2.4.4</b> <code>read_delim</code> as a more flexible method to get tabular data into R</a></li>
<li class="chapter" data-level="2.4.5" data-path="reading.html"><a class="link-to-diff" href="reading.html#reading-tabular-data-directly-from-a-url"><i class="fa fa-check"></i><b>2.4.5</b> Reading tabular data directly from a URL</a></li>
<li class="chapter" data-level="2.4.6" data-path="reading.html"><a class="link-to-diff" href="reading.html#downloading-data-from-a-url"><i class="fa fa-check"></i><b>2.4.6</b> Downloading data from a URL</a></li>
<li class="chapter" data-level="2.4.7" data-path="reading.html"><a class="link-to-diff" href="reading.html#previewing-a-data-file-before-reading-it-into-r"><i class="fa fa-check"></i><b>2.4.7</b> Previewing a data file before reading it into R</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="reading.html"><a class="link-to-diff" href="reading.html#reading-tabular-data-from-a-microsoft-excel-file"><i class="fa fa-check"></i><b>2.5</b> Reading tabular data from a Microsoft Excel file</a></li>
<li class="chapter" data-level="2.6" data-path="reading.html"><a class="link-to-diff" href="reading.html#reading-data-from-a-database"><i class="fa fa-check"></i><b>2.6</b> Reading data from a database</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="reading.html"><a class="link-to-diff" href="reading.html#reading-data-from-a-sqlite-database"><i class="fa fa-check"></i><b>2.6.1</b> Reading data from a SQLite database</a></li>
<li class="chapter" data-level="2.6.2" data-path="reading.html"><a class="link-to-diff" href="reading.html#reading-data-from-a-postgresql-database"><i class="fa fa-check"></i><b>2.6.2</b> Reading data from a PostgreSQL database</a></li>
<li class="chapter" data-level="2.6.3" data-path="reading.html"><a class="link-to-diff" href="reading.html#why-should-we-bother-with-databases-at-all"><i class="fa fa-check"></i><b>2.6.3</b> Why should we bother with databases at all?</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="reading.html"><a class="link-to-diff" href="reading.html#writing-data-from-r-to-a-.csv-file"><i class="fa fa-check"></i><b>2.7</b> Writing data from R to a <code>.csv</code> file</a></li>
<li class="chapter" data-level="2.8" data-path="reading.html"><a class="link-to-diff" href="reading.html#obtaining-data-from-the-web"><i class="fa fa-check"></i><b>2.8</b> Obtaining data from the web</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="reading.html"><a class="link-to-diff" href="reading.html#web-scraping"><i class="fa fa-check"></i><b>2.8.1</b> Web scraping</a></li>
<li class="chapter" data-level="2.8.2" data-path="reading.html"><a class="link-to-diff" href="reading.html#using-an-api"><i class="fa fa-check"></i><b>2.8.2</b> Using an API</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="reading.html"><a class="link-to-diff" href="reading.html#exercises-1"><i class="fa fa-check"></i><b>2.9</b> Exercises</a></li>
<li class="chapter" data-level="2.10" data-path="reading.html"><a class="link-to-diff" href="reading.html#additional-resources"><i class="fa fa-check"></i><b>2.10</b> Additional resources</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="wrangling.html"><a class="link-to-diff" href="wrangling.html"><i class="fa fa-check"></i><b>3</b> Cleaning and wrangling data</a>
<ul>
<li class="chapter" data-level="3.1" data-path="wrangling.html"><a class="link-to-diff" href="wrangling.html#overview-2"><i class="fa fa-check"></i><b>3.1</b> Overview</a></li>
<li class="chapter" data-level="3.2" data-path="wrangling.html"><a class="link-to-diff" href="wrangling.html#chapter-learning-objectives-2"><i class="fa fa-check"></i><b>3.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="3.3" data-path="wrangling.html"><a class="link-to-diff" href="wrangling.html#data-frames-vectors-and-lists"><i class="fa fa-check"></i><b>3.3</b> Data frames, vectors, and lists</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="wrangling.html"><a class="link-to-diff" href="wrangling.html#what-is-a-data-frame"><i class="fa fa-check"></i><b>3.3.1</b> What is a data frame?</a></li>
<li class="chapter" data-level="3.3.2" data-path="wrangling.html"><a class="link-to-diff" href="wrangling.html#what-is-a-vector"><i class="fa fa-check"></i><b>3.3.2</b> What is a vector?</a></li>
<li class="chapter" data-level="3.3.3" data-path="wrangling.html"><a class="link-to-diff" href="wrangling.html#what-is-a-list"><i class="fa fa-check"></i><b>3.3.3</b> What is a list?</a></li>
<li class="chapter" data-level="3.3.4" data-path="wrangling.html"><a class="link-to-diff" href="wrangling.html#what-does-this-have-to-do-with-data-frames"><i class="fa fa-check"></i><b>3.3.4</b> What does this have to do with data frames?</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="wrangling.html"><a class="link-to-diff" href="wrangling.html#tidy-data"><i class="fa fa-check"></i><b>3.4</b> Tidy data</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="wrangling.html"><a class="link-to-diff" href="wrangling.html#tidying-up-going-from-wide-to-long-using-pivot_longer"><i class="fa fa-check"></i><b>3.4.1</b> Tidying up: going from wide to long using <code>pivot_longer</code></a></li>
<li class="chapter" data-level="3.4.2" data-path="wrangling.html"><a class="link-to-diff" href="wrangling.html#pivot-wider"><i class="fa fa-check"></i><b>3.4.2</b> Tidying up: going from long to wide using <code>pivot_wider</code></a></li>
<li class="chapter" data-level="3.4.3" data-path="wrangling.html"><a class="link-to-diff" href="wrangling.html#separate"><i class="fa fa-check"></i><b>3.4.3</b> Tidying up: using <code>separate</code> to deal with multiple delimiters</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="wrangling.html"><a class="link-to-diff" href="wrangling.html#using-select-to-extract-a-range-of-columns"><i class="fa fa-check"></i><b>3.5</b> Using <code>select</code> to extract a range of columns</a></li>
<li class="chapter" data-level="3.6" data-path="wrangling.html"><a class="link-to-diff" href="wrangling.html#using-filter-to-extract-rows-1"><i class="fa fa-check"></i><b>3.6</b> Using <code>filter</code> to extract rows</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="wrangling.html"><a class="link-to-diff" href="wrangling.html#extracting-rows-that-have-a-certain-value-with"><i class="fa fa-check"></i><b>3.6.1</b> Extracting rows that have a certain value with <code>==</code></a></li>
<li class="chapter" data-level="3.6.2" data-path="wrangling.html"><a class="link-to-diff" href="wrangling.html#extracting-rows-that-do-not-have-a-certain-value-with"><i class="fa fa-check"></i><b>3.6.2</b> Extracting rows that do not have a certain value with <code>!=</code></a></li>
<li class="chapter" data-level="3.6.3" data-path="wrangling.html"><a class="link-to-diff" href="wrangling.html#filter-and"><i class="fa fa-check"></i><b>3.6.3</b> Extracting rows satisfying multiple conditions using <code>,</code> or <code>&amp;</code></a></li>
<li class="chapter" data-level="3.6.4" data-path="wrangling.html"><a class="link-to-diff" href="wrangling.html#extracting-rows-satisfying-at-least-one-condition-using"><i class="fa fa-check"></i><b>3.6.4</b> Extracting rows satisfying at least one condition using <code>|</code></a></li>
<li class="chapter" data-level="3.6.5" data-path="wrangling.html"><a class="link-to-diff" href="wrangling.html#extracting-rows-with-values-in-a-vector-using-in"><i class="fa fa-check"></i><b>3.6.5</b> Extracting rows with values in a vector using <code>%in%</code></a></li>
<li class="chapter" data-level="3.6.6" data-path="wrangling.html"><a class="link-to-diff" href="wrangling.html#extracting-rows-above-or-below-a-threshold-using-and"><i class="fa fa-check"></i><b>3.6.6</b> Extracting rows above or below a threshold using <code>&gt;</code> and <code>&lt;</code></a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="wrangling.html"><a class="link-to-diff" href="wrangling.html#using-mutate-to-modify-or-add-columns"><i class="fa fa-check"></i><b>3.7</b> Using <code>mutate</code> to modify or add columns</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="wrangling.html"><a class="link-to-diff" href="wrangling.html#using-mutate-to-modify-columns"><i class="fa fa-check"></i><b>3.7.1</b> Using <code>mutate</code> to modify columns</a></li>
<li class="chapter" data-level="3.7.2" data-path="wrangling.html"><a class="link-to-diff" href="wrangling.html#using-mutate-to-create-new-columns"><i class="fa fa-check"></i><b>3.7.2</b> Using <code>mutate</code> to create new columns</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="wrangling.html"><a class="link-to-diff" href="wrangling.html#combining-functions-using-the-pipe-operator"><i class="fa fa-check"></i><b>3.8</b> Combining functions using the pipe operator, <code>|&gt;</code></a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="wrangling.html"><a class="link-to-diff" href="wrangling.html#using-to-combine-filter-and-select"><i class="fa fa-check"></i><b>3.8.1</b> Using <code>|&gt;</code> to combine <code>filter</code> and <code>select</code></a></li>
<li class="chapter" data-level="3.8.2" data-path="wrangling.html"><a class="link-to-diff" href="wrangling.html#using-with-more-than-two-functions"><i class="fa fa-check"></i><b>3.8.2</b> Using <code>|&gt;</code> with more than two functions</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="wrangling.html"><a class="link-to-diff" href="wrangling.html#aggregating-data-with-summarize-and-map"><i class="fa fa-check"></i><b>3.9</b> Aggregating data with <code>summarize</code> and <code>map</code></a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="wrangling.html"><a class="link-to-diff" href="wrangling.html#calculating-summary-statistics-on-whole-columns"><i class="fa fa-check"></i><b>3.9.1</b> Calculating summary statistics on whole columns</a></li>
<li class="chapter" data-level="3.9.2" data-path="wrangling.html"><a class="link-to-diff" href="wrangling.html#calculating-summary-statistics-when-there-are-nas"><i class="fa fa-check"></i><b>3.9.2</b> Calculating summary statistics when there are <code>NA</code>s</a></li>
<li class="chapter" data-level="3.9.3" data-path="wrangling.html"><a class="link-to-diff" href="wrangling.html#calculating-summary-statistics-for-groups-of-rows"><i class="fa fa-check"></i><b>3.9.3</b> Calculating summary statistics for groups of rows</a></li>
<li class="chapter" data-level="3.9.4" data-path="wrangling.html"><a class="link-to-diff" href="wrangling.html#calculating-summary-statistics-on-many-columns"><i class="fa fa-check"></i><b>3.9.4</b> Calculating summary statistics on many columns</a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="wrangling.html"><a class="link-to-diff" href="wrangling.html#apply-functions-across-many-columns-with-mutate-and-across"><i class="fa fa-check"></i><b>3.10</b> Apply functions across many columns with <code>mutate</code> and <code>across</code></a></li>
<li class="chapter" data-level="3.11" data-path="wrangling.html"><a class="link-to-diff" href="wrangling.html#apply-functions-across-columns-within-one-row-with-rowwise-and-mutate"><i class="fa fa-check"></i><b>3.11</b> Apply functions across columns within one row with <code>rowwise</code> and <code>mutate</code></a></li>
<li class="chapter" data-level="3.12" data-path="wrangling.html"><a class="link-to-diff" href="wrangling.html#summary"><i class="fa fa-check"></i><b>3.12</b> Summary</a></li>
<li class="chapter" data-level="3.13" data-path="wrangling.html"><a class="link-to-diff" href="wrangling.html#exercises-2"><i class="fa fa-check"></i><b>3.13</b> Exercises</a></li>
<li class="chapter" data-level="3.14" data-path="wrangling.html"><a class="link-to-diff" href="wrangling.html#additional-resources-1"><i class="fa fa-check"></i><b>3.14</b> Additional resources</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="viz.html"><a class="link-to-diff" href="viz.html"><i class="fa fa-check"></i><b>4</b> Effective data visualization</a>
<ul>
<li class="chapter" data-level="4.1" data-path="viz.html"><a class="link-to-diff" href="viz.html#overview-3"><i class="fa fa-check"></i><b>4.1</b> Overview</a></li>
<li class="chapter" data-level="4.2" data-path="viz.html"><a class="link-to-diff" href="viz.html#chapter-learning-objectives-3"><i class="fa fa-check"></i><b>4.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="4.3" data-path="viz.html"><a class="link-to-diff" href="viz.html#choosing-the-visualization"><i class="fa fa-check"></i><b>4.3</b> Choosing the visualization</a></li>
<li class="chapter" data-level="4.4" data-path="viz.html"><a class="link-to-diff" href="viz.html#refining-the-visualization"><i class="fa fa-check"></i><b>4.4</b> Refining the visualization</a></li>
<li class="chapter" data-level="4.5" data-path="viz.html"><a class="link-to-diff" href="viz.html#creating-visualizations-with-ggplot2"><i class="fa fa-check"></i><b>4.5</b> Creating visualizations with <code>ggplot2</code></a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="viz.html"><a class="link-to-diff" href="viz.html#scatter-plots-and-line-plots-the-mauna-loa-co_text2-data-set"><i class="fa fa-check"></i><b>4.5.1</b> Scatter plots and line plots: the Mauna Loa CO<span class="math inline">\(_{\text{2}}\)</span> data set</a></li>
<li class="chapter" data-level="4.5.2" data-path="viz.html"><a class="link-to-diff" href="viz.html#scatter-plots-the-old-faithful-eruption-time-data-set"><i class="fa fa-check"></i><b>4.5.2</b> Scatter plots: the Old Faithful eruption time data set</a></li>
<li class="chapter" data-level="4.5.3" data-path="viz.html"><a class="link-to-diff" href="viz.html#axis-transformation-and-colored-scatter-plots-the-canadian-languages-data-set"><i class="fa fa-check"></i><b>4.5.3</b> Axis transformation and colored scatter plots: the Canadian languages data set</a></li>
<li class="chapter" data-level="4.5.4" data-path="viz.html"><a class="link-to-diff" href="viz.html#bar-plots-the-island-landmass-data-set"><i class="fa fa-check"></i><b>4.5.4</b> Bar plots: the island landmass data set</a></li>
<li class="chapter" data-level="4.5.5" data-path="viz.html"><a class="link-to-diff" href="viz.html#histogramsviz"><i class="fa fa-check"></i><b>4.5.5</b> Histograms: the Michelson speed of light data set</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="viz.html"><a class="link-to-diff" href="viz.html#explaining-the-visualization"><i class="fa fa-check"></i><b>4.6</b> Explaining the visualization</a></li>
<li class="chapter" data-level="4.7" data-path="viz.html"><a class="link-to-diff" href="viz.html#saving-the-visualization"><i class="fa fa-check"></i><b>4.7</b> Saving the visualization</a></li>
<li class="chapter" data-level="4.8" data-path="viz.html"><a class="link-to-diff" href="viz.html#exercises-3"><i class="fa fa-check"></i><b>4.8</b> Exercises</a></li>
<li class="chapter" data-level="4.9" data-path="viz.html"><a class="link-to-diff" href="viz.html#additional-resources-2"><i class="fa fa-check"></i><b>4.9</b> Additional resources</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="classification1.html"><a class="link-to-diff" href="classification1.html"><i class="fa fa-check"></i><b>5</b> Classification I: training &amp; predicting</a>
<ul>
<li class="chapter" data-level="5.1" data-path="classification1.html"><a class="link-to-diff" href="classification1.html#overview-4"><i class="fa fa-check"></i><b>5.1</b> Overview</a></li>
<li class="chapter" data-level="5.2" data-path="classification1.html"><a class="link-to-diff" href="classification1.html#chapter-learning-objectives-4"><i class="fa fa-check"></i><b>5.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="5.3" data-path="classification1.html"><a class="link-to-diff" href="classification1.html#the-classification-problem"><i class="fa fa-check"></i><b>5.3</b> The classification problem</a></li>
<li class="chapter" data-level="5.4" data-path="classification1.html"><a class="link-to-diff" href="classification1.html#exploring-a-data-set"><i class="fa fa-check"></i><b>5.4</b> Exploring a data set</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="classification1.html"><a class="link-to-diff" href="classification1.html#loading-the-cancer-data"><i class="fa fa-check"></i><b>5.4.1</b> Loading the cancer data</a></li>
<li class="chapter" data-level="5.4.2" data-path="classification1.html"><a class="link-to-diff" href="classification1.html#describing-the-variables-in-the-cancer-data-set"><i class="fa fa-check"></i><b>5.4.2</b> Describing the variables in the cancer data set</a></li>
<li class="chapter" data-level="5.4.3" data-path="classification1.html"><a class="link-to-diff" href="classification1.html#exploring-the-cancer-data"><i class="fa fa-check"></i><b>5.4.3</b> Exploring the cancer data</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="classification1.html"><a class="link-to-diff" href="classification1.html#classification-with-k-nearest-neighbors"><i class="fa fa-check"></i><b>5.5</b> Classification with K-nearest neighbors</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="classification1.html"><a class="link-to-diff" href="classification1.html#distance-between-points"><i class="fa fa-check"></i><b>5.5.1</b> Distance between points</a></li>
<li class="chapter" data-level="5.5.2" data-path="classification1.html"><a class="link-to-diff" href="classification1.html#more-than-two-explanatory-variables"><i class="fa fa-check"></i><b>5.5.2</b> More than two explanatory variables</a></li>
<li class="chapter" data-level="5.5.3" data-path="classification1.html"><a class="link-to-diff" href="classification1.html#summary-of-k-nearest-neighbors-algorithm"><i class="fa fa-check"></i><b>5.5.3</b> Summary of K-nearest neighbors algorithm</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="classification1.html"><a class="link-to-diff" href="classification1.html#k-nearest-neighbors-with-tidymodels"><i class="fa fa-check"></i><b>5.6</b> K-nearest neighbors with <code>tidymodels</code></a></li>
<li class="chapter" data-level="5.7" data-path="classification1.html"><a class="link-to-diff" href="classification1.html#data-preprocessing-with-tidymodels"><i class="fa fa-check"></i><b>5.7</b> Data preprocessing with <code>tidymodels</code></a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="classification1.html"><a class="link-to-diff" href="classification1.html#centering-and-scaling"><i class="fa fa-check"></i><b>5.7.1</b> Centering and scaling</a></li>
<li class="chapter" data-level="5.7.2" data-path="classification1.html"><a class="link-to-diff" href="classification1.html#balancing"><i class="fa fa-check"></i><b>5.7.2</b> Balancing</a></li>
<li class="chapter" data-level="5.7.3" data-path="classification1.html"><a class="link-to-diff" href="classification1.html#missing-data"><i class="fa fa-check"></i><b>5.7.3</b> Missing data</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="classification1.html"><a class="link-to-diff" href="classification1.html#puttingittogetherworkflow"><i class="fa fa-check"></i><b>5.8</b> Putting it together in a <code>workflow</code></a></li>
<li class="chapter" data-level="5.9" data-path="classification1.html"><a class="link-to-diff" href="classification1.html#exercises-4"><i class="fa fa-check"></i><b>5.9</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="classification2.html"><a class="link-to-diff" href="classification2.html"><i class="fa fa-check"></i><b>6</b> Classification II: evaluation &amp; tuning</a>
<ul>
<li class="chapter" data-level="6.1" data-path="classification2.html"><a class="link-to-diff" href="classification2.html#overview-5"><i class="fa fa-check"></i><b>6.1</b> Overview</a></li>
<li class="chapter" data-level="6.2" data-path="classification2.html"><a class="link-to-diff" href="classification2.html#chapter-learning-objectives-5"><i class="fa fa-check"></i><b>6.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="6.3" data-path="classification2.html"><a class="link-to-diff" href="classification2.html#evaluating-performance"><i class="fa fa-check"></i><b>6.3</b> Evaluating performance</a></li>
<li class="chapter" data-level="6.4" data-path="classification2.html"><a class="link-to-diff" href="classification2.html#randomseeds"><i class="fa fa-check"></i><b>6.4</b> Randomness and seeds</a></li>
<li class="chapter" data-level="6.5" data-path="classification2.html"><a class="link-to-diff" href="classification2.html#evaluating-performance-with-tidymodels"><i class="fa fa-check"></i><b>6.5</b> Evaluating performance with <code>tidymodels</code></a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="classification2.html"><a class="link-to-diff" href="classification2.html#create-the-train-test-split"><i class="fa fa-check"></i><b>6.5.1</b> Create the train / test split</a></li>
<li class="chapter" data-level="6.5.2" data-path="classification2.html"><a class="link-to-diff" href="classification2.html#preprocess-the-data"><i class="fa fa-check"></i><b>6.5.2</b> Preprocess the data</a></li>
<li class="chapter" data-level="6.5.3" data-path="classification2.html"><a class="link-to-diff" href="classification2.html#train-the-classifier"><i class="fa fa-check"></i><b>6.5.3</b> Train the classifier</a></li>
<li class="chapter" data-level="6.5.4" data-path="classification2.html"><a class="link-to-diff" href="classification2.html#predict-the-labels-in-the-test-set"><i class="fa fa-check"></i><b>6.5.4</b> Predict the labels in the test set</a></li>
<li class="chapter" data-level="6.5.5" data-path="classification2.html"><a class="link-to-diff" href="classification2.html#eval-performance-cls2"><i class="fa fa-check"></i><b>6.5.5</b> Evaluate performance</a></li>
<li class="chapter" data-level="6.5.6" data-path="classification2.html"><a class="link-to-diff" href="classification2.html#critically-analyze-performance"><i class="fa fa-check"></i><b>6.5.6</b> Critically analyze performance</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="classification2.html"><a class="link-to-diff" href="classification2.html#tuning-the-classifier"><i class="fa fa-check"></i><b>6.6</b> Tuning the classifier</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="classification2.html"><a class="link-to-diff" href="classification2.html#cross-validation"><i class="fa fa-check"></i><b>6.6.1</b> Cross-validation</a></li>
<li class="chapter" data-level="6.6.2" data-path="classification2.html"><a class="link-to-diff" href="classification2.html#parameter-value-selection"><i class="fa fa-check"></i><b>6.6.2</b> Parameter value selection</a></li>
<li class="chapter" data-level="6.6.3" data-path="classification2.html"><a class="link-to-diff" href="classification2.html#underoverfitting"><i class="fa fa-check"></i><b>6.6.3</b> Under/Overfitting</a></li>
<li class="chapter" data-level="6.6.4" data-path="classification2.html"><a class="link-to-diff" href="classification2.html#evaluating-on-the-test-set"><i class="fa fa-check"></i><b>6.6.4</b> Evaluating on the test set</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="classification2.html"><a class="link-to-diff" href="classification2.html#summary-1"><i class="fa fa-check"></i><b>6.7</b> Summary</a></li>
<li class="chapter" data-level="6.8" data-path="classification2.html"><a class="link-to-diff" href="classification2.html#predictor-variable-selection"><i class="fa fa-check"></i><b>6.8</b> Predictor variable selection</a>
<ul>
<li class="chapter" data-level="6.8.1" data-path="classification2.html"><a class="link-to-diff" href="classification2.html#the-effect-of-irrelevant-predictors"><i class="fa fa-check"></i><b>6.8.1</b> The effect of irrelevant predictors</a></li>
<li class="chapter" data-level="6.8.2" data-path="classification2.html"><a class="link-to-diff" href="classification2.html#finding-a-good-subset-of-predictors"><i class="fa fa-check"></i><b>6.8.2</b> Finding a good subset of predictors</a></li>
<li class="chapter" data-level="6.8.3" data-path="classification2.html"><a class="link-to-diff" href="classification2.html#forward-selection-in-r"><i class="fa fa-check"></i><b>6.8.3</b> Forward selection in R</a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="classification2.html"><a class="link-to-diff" href="classification2.html#exercises-5"><i class="fa fa-check"></i><b>6.9</b> Exercises</a></li>
<li class="chapter" data-level="6.10" data-path="classification2.html"><a class="link-to-diff" href="classification2.html#additional-resources-3"><i class="fa fa-check"></i><b>6.10</b> Additional resources</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="regression1.html"><a class="link-to-diff" href="regression1.html"><i class="fa fa-check"></i><b>7</b> Regression I: K-nearest neighbors</a>
<ul>
<li class="chapter" data-level="7.1" data-path="regression1.html"><a class="link-to-diff" href="regression1.html#overview-6"><i class="fa fa-check"></i><b>7.1</b> Overview</a></li>
<li class="chapter" data-level="7.2" data-path="regression1.html"><a class="link-to-diff" href="regression1.html#chapter-learning-objectives-6"><i class="fa fa-check"></i><b>7.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="7.3" data-path="regression1.html"><a class="link-to-diff" href="regression1.html#the-regression-problem"><i class="fa fa-check"></i><b>7.3</b> The regression problem</a></li>
<li class="chapter" data-level="7.4" data-path="regression1.html"><a class="link-to-diff" href="regression1.html#exploring-a-data-set-1"><i class="fa fa-check"></i><b>7.4</b> Exploring a data set</a></li>
<li class="chapter" data-level="7.5" data-path="regression1.html"><a class="link-to-diff" href="regression1.html#k-nearest-neighbors-regression"><i class="fa fa-check"></i><b>7.5</b> K-nearest neighbors regression</a></li>
<li class="chapter" data-level="7.6" data-path="regression1.html"><a class="link-to-diff" href="regression1.html#training-evaluating-and-tuning-the-model"><i class="fa fa-check"></i><b>7.6</b> Training, evaluating, and tuning the model</a></li>
<li class="chapter" data-level="7.7" data-path="regression1.html"><a class="link-to-diff" href="regression1.html#underfitting-and-overfitting"><i class="fa fa-check"></i><b>7.7</b> Underfitting and overfitting</a></li>
<li class="chapter" data-level="7.8" data-path="regression1.html"><a class="link-to-diff" href="regression1.html#evaluating-on-the-test-set-1"><i class="fa fa-check"></i><b>7.8</b> Evaluating on the test set</a></li>
<li class="chapter" data-level="7.9" data-path="regression1.html"><a class="link-to-diff" href="regression1.html#multivariable-k-nn-regression"><i class="fa fa-check"></i><b>7.9</b> Multivariable K-NN regression</a></li>
<li class="chapter" data-level="7.10" data-path="regression1.html"><a class="link-to-diff" href="regression1.html#strengths-and-limitations-of-k-nn-regression"><i class="fa fa-check"></i><b>7.10</b> Strengths and limitations of K-NN regression</a></li>
<li class="chapter" data-level="7.11" data-path="regression1.html"><a class="link-to-diff" href="regression1.html#exercises-6"><i class="fa fa-check"></i><b>7.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="regression2.html"><a class="link-to-diff" href="regression2.html"><i class="fa fa-check"></i><b>8</b> Regression II: linear regression</a>
<ul>
<li class="chapter" data-level="8.1" data-path="regression2.html"><a class="link-to-diff" href="regression2.html#overview-7"><i class="fa fa-check"></i><b>8.1</b> Overview</a></li>
<li class="chapter" data-level="8.2" data-path="regression2.html"><a class="link-to-diff" href="regression2.html#chapter-learning-objectives-7"><i class="fa fa-check"></i><b>8.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="8.3" data-path="regression2.html"><a class="link-to-diff" href="regression2.html#simple-linear-regression"><i class="fa fa-check"></i><b>8.3</b> Simple linear regression</a></li>
<li class="chapter" data-level="8.4" data-path="regression2.html"><a class="link-to-diff" href="regression2.html#linear-regression-in-r"><i class="fa fa-check"></i><b>8.4</b> Linear regression in R</a></li>
<li class="chapter" data-level="8.5" data-path="regression2.html"><a class="link-to-diff" href="regression2.html#comparing-simple-linear-and-k-nn-regression"><i class="fa fa-check"></i><b>8.5</b> Comparing simple linear and K-NN regression</a></li>
<li class="chapter" data-level="8.6" data-path="regression2.html"><a class="link-to-diff" href="regression2.html#multivariable-linear-regression"><i class="fa fa-check"></i><b>8.6</b> Multivariable linear regression</a></li>
<li class="chapter" data-level="8.7" data-path="regression2.html"><a class="link-to-diff" href="regression2.html#multicollinearity-and-outliers"><i class="fa fa-check"></i><b>8.7</b> Multicollinearity and outliers</a>
<ul>
<li class="chapter" data-level="8.7.1" data-path="regression2.html"><a class="link-to-diff" href="regression2.html#outliers"><i class="fa fa-check"></i><b>8.7.1</b> Outliers</a></li>
<li class="chapter" data-level="8.7.2" data-path="regression2.html"><a class="link-to-diff" href="regression2.html#multicollinearity"><i class="fa fa-check"></i><b>8.7.2</b> Multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="regression2.html"><a class="link-to-diff" href="regression2.html#designing-new-predictors"><i class="fa fa-check"></i><b>8.8</b> Designing new predictors</a></li>
<li class="chapter" data-level="8.9" data-path="regression2.html"><a class="link-to-diff" href="regression2.html#the-other-sides-of-regression"><i class="fa fa-check"></i><b>8.9</b> The other sides of regression</a></li>
<li class="chapter" data-level="8.10" data-path="regression2.html"><a class="link-to-diff" href="regression2.html#exercises-7"><i class="fa fa-check"></i><b>8.10</b> Exercises</a></li>
<li class="chapter" data-level="8.11" data-path="regression2.html"><a class="link-to-diff" href="regression2.html#additional-resources-4"><i class="fa fa-check"></i><b>8.11</b> Additional resources</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="clustering.html"><a class="link-to-diff" href="clustering.html"><i class="fa fa-check"></i><b>9</b> Clustering</a>
<ul>
<li class="chapter" data-level="9.1" data-path="clustering.html"><a class="link-to-diff" href="clustering.html#overview-8"><i class="fa fa-check"></i><b>9.1</b> Overview</a></li>
<li class="chapter" data-level="9.2" data-path="clustering.html"><a class="link-to-diff" href="clustering.html#chapter-learning-objectives-8"><i class="fa fa-check"></i><b>9.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="9.3" data-path="clustering.html"><a class="link-to-diff" href="clustering.html#clustering-1"><i class="fa fa-check"></i><b>9.3</b> Clustering</a></li>
<li class="chapter" data-level="9.4" data-path="clustering.html"><a class="link-to-diff" href="clustering.html#an-illustrative-example"><i class="fa fa-check"></i><b>9.4</b> An illustrative example</a></li>
<li class="chapter" data-level="9.5" data-path="clustering.html"><a class="link-to-diff" href="clustering.html#k-means"><i class="fa fa-check"></i><b>9.5</b> K-means</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="clustering.html"><a class="link-to-diff" href="clustering.html#measuring-cluster-quality"><i class="fa fa-check"></i><b>9.5.1</b> Measuring cluster quality</a></li>
<li class="chapter" data-level="9.5.2" data-path="clustering.html"><a class="link-to-diff" href="clustering.html#the-clustering-algorithm"><i class="fa fa-check"></i><b>9.5.2</b> The clustering algorithm</a></li>
<li class="chapter" data-level="9.5.3" data-path="clustering.html"><a class="link-to-diff" href="clustering.html#random-restarts"><i class="fa fa-check"></i><b>9.5.3</b> Random restarts</a></li>
<li class="chapter" data-level="9.5.4" data-path="clustering.html"><a class="link-to-diff" href="clustering.html#choosing-k"><i class="fa fa-check"></i><b>9.5.4</b> Choosing K</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="clustering.html"><a class="link-to-diff" href="clustering.html#k-means-in-r"><i class="fa fa-check"></i><b>9.6</b> K-means in R</a></li>
<li class="chapter" data-level="9.7" data-path="clustering.html"><a class="link-to-diff" href="clustering.html#exercises-8"><i class="fa fa-check"></i><b>9.7</b> Exercises</a></li>
<li class="chapter" data-level="9.8" data-path="clustering.html"><a class="link-to-diff" href="clustering.html#additional-resources-5"><i class="fa fa-check"></i><b>9.8</b> Additional resources</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="inference.html"><a class="link-to-diff" href="inference.html"><i class="fa fa-check"></i><b>10</b> Statistical inference</a>
<ul>
<li class="chapter" data-level="10.1" data-path="inference.html"><a class="link-to-diff" href="inference.html#overview-9"><i class="fa fa-check"></i><b>10.1</b> Overview</a></li>
<li class="chapter" data-level="10.2" data-path="inference.html"><a class="link-to-diff" href="inference.html#chapter-learning-objectives-9"><i class="fa fa-check"></i><b>10.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="10.3" data-path="inference.html"><a class="link-to-diff" href="inference.html#why-do-we-need-sampling"><i class="fa fa-check"></i><b>10.3</b> Why do we need sampling?</a></li>
<li class="chapter" data-level="10.4" data-path="inference.html"><a class="link-to-diff" href="inference.html#sampling-distributions"><i class="fa fa-check"></i><b>10.4</b> Sampling distributions</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="inference.html"><a class="link-to-diff" href="inference.html#sampling-distributions-for-proportions"><i class="fa fa-check"></i><b>10.4.1</b> Sampling distributions for proportions</a></li>
<li class="chapter" data-level="10.4.2" data-path="inference.html"><a class="link-to-diff" href="inference.html#sampling-distributions-for-means"><i class="fa fa-check"></i><b>10.4.2</b> Sampling distributions for means</a></li>
<li class="chapter" data-level="10.4.3" data-path="inference.html"><a class="link-to-diff" href="inference.html#summary-2"><i class="fa fa-check"></i><b>10.4.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="inference.html"><a class="link-to-diff" href="inference.html#bootstrapping"><i class="fa fa-check"></i><b>10.5</b> Bootstrapping</a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="inference.html"><a class="link-to-diff" href="inference.html#overview-10"><i class="fa fa-check"></i><b>10.5.1</b> Overview</a></li>
<li class="chapter" data-level="10.5.2" data-path="inference.html"><a class="link-to-diff" href="inference.html#bootstrapping-in-r"><i class="fa fa-check"></i><b>10.5.2</b> Bootstrapping in R</a></li>
<li class="chapter" data-level="10.5.3" data-path="inference.html"><a class="link-to-diff" href="inference.html#using-the-bootstrap-to-calculate-a-plausible-range"><i class="fa fa-check"></i><b>10.5.3</b> Using the bootstrap to calculate a plausible range</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="inference.html"><a class="link-to-diff" href="inference.html#exercises-9"><i class="fa fa-check"></i><b>10.6</b> Exercises</a></li>
<li class="chapter" data-level="10.7" data-path="inference.html"><a class="link-to-diff" href="inference.html#additional-resources-6"><i class="fa fa-check"></i><b>10.7</b> Additional resources</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="jupyter.html"><a href="jupyter.html"><i class="fa fa-check"></i><b>11</b> Combining code and text with Jupyter</a>
<ul>
<li class="chapter" data-level="11.1" data-path="jupyter.html"><a href="jupyter.html#overview-11"><i class="fa fa-check"></i><b>11.1</b> Overview</a></li>
<li class="chapter" data-level="11.2" data-path="jupyter.html"><a href="jupyter.html#chapter-learning-objectives-10"><i class="fa fa-check"></i><b>11.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="11.3" data-path="jupyter.html"><a href="jupyter.html#jupyter-1"><i class="fa fa-check"></i><b>11.3</b> Jupyter</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="jupyter.html"><a href="jupyter.html#accessing-jupyter"><i class="fa fa-check"></i><b>11.3.1</b> Accessing Jupyter</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="jupyter.html"><a href="jupyter.html#code-cells"><i class="fa fa-check"></i><b>11.4</b> Code cells</a>
<ul>
<li class="chapter" data-level="11.4.1" data-path="jupyter.html"><a href="jupyter.html#executing-code-cells"><i class="fa fa-check"></i><b>11.4.1</b> Executing code cells</a></li>
<li class="chapter" data-level="11.4.2" data-path="jupyter.html"><a href="jupyter.html#the-kernel"><i class="fa fa-check"></i><b>11.4.2</b> The Kernel</a></li>
<li class="chapter" data-level="11.4.3" data-path="jupyter.html"><a href="jupyter.html#creating-new-code-cells"><i class="fa fa-check"></i><b>11.4.3</b> Creating new code cells</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="jupyter.html"><a href="jupyter.html#markdown-cells"><i class="fa fa-check"></i><b>11.5</b> Markdown cells</a>
<ul>
<li class="chapter" data-level="11.5.1" data-path="jupyter.html"><a href="jupyter.html#editing-markdown-cells"><i class="fa fa-check"></i><b>11.5.1</b> Editing Markdown cells</a></li>
<li class="chapter" data-level="11.5.2" data-path="jupyter.html"><a href="jupyter.html#creating-new-markdown-cells"><i class="fa fa-check"></i><b>11.5.2</b> Creating new Markdown cells</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="jupyter.html"><a href="jupyter.html#saving-your-work"><i class="fa fa-check"></i><b>11.6</b> Saving your work</a></li>
<li class="chapter" data-level="11.7" data-path="jupyter.html"><a href="jupyter.html#best-practices-for-running-a-notebook"><i class="fa fa-check"></i><b>11.7</b> Best practices for running a notebook</a>
<ul>
<li class="chapter" data-level="11.7.1" data-path="jupyter.html"><a href="jupyter.html#best-practices-for-executing-code-cells"><i class="fa fa-check"></i><b>11.7.1</b> Best practices for executing code cells</a></li>
<li class="chapter" data-level="11.7.2" data-path="jupyter.html"><a href="jupyter.html#best-practices-for-including-r-packages-in-notebooks"><i class="fa fa-check"></i><b>11.7.2</b> Best practices for including R packages in notebooks</a></li>
<li class="chapter" data-level="11.7.3" data-path="jupyter.html"><a href="jupyter.html#summary-of-best-practices-for-running-a-notebook"><i class="fa fa-check"></i><b>11.7.3</b> Summary of best practices for running a notebook</a></li>
</ul></li>
<li class="chapter" data-level="11.8" data-path="jupyter.html"><a href="jupyter.html#exploring-data-files"><i class="fa fa-check"></i><b>11.8</b> Exploring data files</a></li>
<li class="chapter" data-level="11.9" data-path="jupyter.html"><a href="jupyter.html#exporting-to-a-different-file-format"><i class="fa fa-check"></i><b>11.9</b> Exporting to a different file format</a>
<ul>
<li class="chapter" data-level="11.9.1" data-path="jupyter.html"><a href="jupyter.html#exporting-to-html"><i class="fa fa-check"></i><b>11.9.1</b> Exporting to HTML</a></li>
<li class="chapter" data-level="11.9.2" data-path="jupyter.html"><a href="jupyter.html#exporting-to-pdf"><i class="fa fa-check"></i><b>11.9.2</b> Exporting to PDF</a></li>
</ul></li>
<li class="chapter" data-level="11.10" data-path="jupyter.html"><a href="jupyter.html#creating-a-new-jupyter-notebook"><i class="fa fa-check"></i><b>11.10</b> Creating a new Jupyter notebook</a></li>
<li class="chapter" data-level="11.11" data-path="jupyter.html"><a href="jupyter.html#additional-resources-7"><i class="fa fa-check"></i><b>11.11</b> Additional resources</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="version-control.html"><a class="link-to-diff" href="version-control.html"><i class="fa fa-check"></i><b>12</b> Collaboration with version control</a>
<ul>
<li class="chapter" data-level="12.1" data-path="version-control.html"><a class="link-to-diff" href="version-control.html#overview-12"><i class="fa fa-check"></i><b>12.1</b> Overview</a></li>
<li class="chapter" data-level="12.2" data-path="version-control.html"><a class="link-to-diff" href="version-control.html#chapter-learning-objectives-11"><i class="fa fa-check"></i><b>12.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="12.3" data-path="version-control.html"><a class="link-to-diff" href="version-control.html#what-is-version-control-and-why-should-i-use-it"><i class="fa fa-check"></i><b>12.3</b> What is version control, and why should I use it?</a></li>
<li class="chapter" data-level="12.4" data-path="version-control.html"><a class="link-to-diff" href="version-control.html#version-control-repositories"><i class="fa fa-check"></i><b>12.4</b> Version control repositories</a></li>
<li class="chapter" data-level="12.5" data-path="version-control.html"><a class="link-to-diff" href="version-control.html#version-control-workflows"><i class="fa fa-check"></i><b>12.5</b> Version control workflows</a>
<ul>
<li class="chapter" data-level="12.5.1" data-path="version-control.html"><a class="link-to-diff" href="version-control.html#commit-changes"><i class="fa fa-check"></i><b>12.5.1</b> Committing changes to a local repository</a></li>
<li class="chapter" data-level="12.5.2" data-path="version-control.html"><a class="link-to-diff" href="version-control.html#pushing-changes-to-a-remote-repository"><i class="fa fa-check"></i><b>12.5.2</b> Pushing changes to a remote repository</a></li>
<li class="chapter" data-level="12.5.3" data-path="version-control.html"><a class="link-to-diff" href="version-control.html#pulling-changes-from-a-remote-repository"><i class="fa fa-check"></i><b>12.5.3</b> Pulling changes from a remote repository</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="version-control.html"><a class="link-to-diff" href="version-control.html#working-with-remote-repositories-using-github"><i class="fa fa-check"></i><b>12.6</b> Working with remote repositories using GitHub</a>
<ul>
<li class="chapter" data-level="12.6.1" data-path="version-control.html"><a class="link-to-diff" href="version-control.html#creating-a-remote-repository-on-github"><i class="fa fa-check"></i><b>12.6.1</b> Creating a remote repository on GitHub</a></li>
<li class="chapter" data-level="12.6.2" data-path="version-control.html"><a class="link-to-diff" href="version-control.html#editing-files-on-github-with-the-pen-tool"><i class="fa fa-check"></i><b>12.6.2</b> Editing files on GitHub with the pen tool</a></li>
<li class="chapter" data-level="12.6.3" data-path="version-control.html"><a class="link-to-diff" href="version-control.html#creating-files-on-github-with-the-add-file-menu"><i class="fa fa-check"></i><b>12.6.3</b> Creating files on GitHub with the “Add file” menu</a></li>
</ul></li>
<li class="chapter" data-level="12.7" data-path="version-control.html"><a class="link-to-diff" href="version-control.html#local-repo-jupyter"><i class="fa fa-check"></i><b>12.7</b> Working with local repositories using Jupyter</a>
<ul>
<li class="chapter" data-level="12.7.1" data-path="version-control.html"><a class="link-to-diff" href="version-control.html#generating-a-github-personal-access-token"><i class="fa fa-check"></i><b>12.7.1</b> Generating a GitHub personal access token</a></li>
<li class="chapter" data-level="12.7.2" data-path="version-control.html"><a class="link-to-diff" href="version-control.html#cloning-a-repository-using-jupyter"><i class="fa fa-check"></i><b>12.7.2</b> Cloning a repository using Jupyter</a></li>
<li class="chapter" data-level="12.7.3" data-path="version-control.html"><a class="link-to-diff" href="version-control.html#specifying-files-to-commit"><i class="fa fa-check"></i><b>12.7.3</b> Specifying files to commit</a></li>
<li class="chapter" data-level="12.7.4" data-path="version-control.html"><a class="link-to-diff" href="version-control.html#making-the-commit"><i class="fa fa-check"></i><b>12.7.4</b> Making the commit</a></li>
<li class="chapter" data-level="12.7.5" data-path="version-control.html"><a class="link-to-diff" href="version-control.html#pushing-the-commits-to-github"><i class="fa fa-check"></i><b>12.7.5</b> Pushing the commits to GitHub</a></li>
</ul></li>
<li class="chapter" data-level="12.8" data-path="version-control.html"><a class="link-to-diff" href="version-control.html#collaboration"><i class="fa fa-check"></i><b>12.8</b> Collaboration</a>
<ul>
<li class="chapter" data-level="12.8.1" data-path="version-control.html"><a class="link-to-diff" href="version-control.html#giving-collaborators-access-to-your-project"><i class="fa fa-check"></i><b>12.8.1</b> Giving collaborators access to your project</a></li>
<li class="chapter" data-level="12.8.2" data-path="version-control.html"><a class="link-to-diff" href="version-control.html#pulling-changes-from-github-using-jupyter"><i class="fa fa-check"></i><b>12.8.2</b> Pulling changes from GitHub using Jupyter</a></li>
<li class="chapter" data-level="12.8.3" data-path="version-control.html"><a class="link-to-diff" href="version-control.html#handling-merge-conflicts"><i class="fa fa-check"></i><b>12.8.3</b> Handling merge conflicts</a></li>
<li class="chapter" data-level="12.8.4" data-path="version-control.html"><a class="link-to-diff" href="version-control.html#communicating-using-github-issues"><i class="fa fa-check"></i><b>12.8.4</b> Communicating using GitHub issues</a></li>
</ul></li>
<li class="chapter" data-level="12.9" data-path="version-control.html"><a class="link-to-diff" href="version-control.html#exercises-10"><i class="fa fa-check"></i><b>12.9</b> Exercises</a></li>
<li class="chapter" data-level="12.10" data-path="version-control.html"><a class="link-to-diff" href="version-control.html#vc-add-res"><i class="fa fa-check"></i><b>12.10</b> Additional resources</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="setup.html"><a class="link-to-diff" href="setup.html"><i class="fa fa-check"></i><b>13</b> Setting up your computer</a>
<ul>
<li class="chapter" data-level="13.1" data-path="setup.html"><a class="link-to-diff" href="setup.html#overview-13"><i class="fa fa-check"></i><b>13.1</b> Overview</a></li>
<li class="chapter" data-level="13.2" data-path="setup.html"><a class="link-to-diff" href="setup.html#chapter-learning-objectives-12"><i class="fa fa-check"></i><b>13.2</b> Chapter learning objectives</a></li>
<li class="chapter" data-level="13.3" data-path="setup.html"><a class="link-to-diff" href="setup.html#obtaining-the-worksheets-for-this-book"><i class="fa fa-check"></i><b>13.3</b> Obtaining the worksheets for this book</a></li>
<li class="chapter" data-level="13.4" data-path="setup.html"><a class="link-to-diff" href="setup.html#working-with-docker"><i class="fa fa-check"></i><b>13.4</b> Working with Docker</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="setup.html"><a class="link-to-diff" href="setup.html#windows"><i class="fa fa-check"></i><b>13.4.1</b> Windows</a></li>
<li class="chapter" data-level="13.4.2" data-path="setup.html"><a class="link-to-diff" href="setup.html#macos"><i class="fa fa-check"></i><b>13.4.2</b> MacOS</a></li>
<li class="chapter" data-level="13.4.3" data-path="setup.html"><a class="link-to-diff" href="setup.html#ubuntu"><i class="fa fa-check"></i><b>13.4.3</b> Ubuntu</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="setup.html"><a class="link-to-diff" href="setup.html#working-with-jupyterlab-desktop"><i class="fa fa-check"></i><b>13.5</b> Working with JupyterLab Desktop</a>
<ul>
<li class="chapter" data-level="13.5.1" data-path="setup.html"><a class="link-to-diff" href="setup.html#windows-1"><i class="fa fa-check"></i><b>13.5.1</b> Windows</a></li>
<li class="chapter" data-level="13.5.2" data-path="setup.html"><a class="link-to-diff" href="setup.html#macos-1"><i class="fa fa-check"></i><b>13.5.2</b> MacOS</a></li>
<li class="chapter" data-level="13.5.3" data-path="setup.html"><a class="link-to-diff" href="setup.html#ubuntu-1"><i class="fa fa-check"></i><b>13.5.3</b> Ubuntu</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>
</nav>
</div>
<div class="book-body">
<div class="body-inner">
<div class="book-header" role="navigation">
<h1>
<i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><p><img src="img/frontmatter/ds-a-first-intro-graphic.jpg"/>
Data Science</p></a>
</h1>
</div>
<div class="page-wrapper" role="main" tabindex="-1">
<div class="page-inner">
<section class="normal" id="section-">
<div class="section level1 hasAnchor" id="clustering" number="9">
<h1><span class="header-section-number">Chapter 9</span> Clustering<a aria-label="Anchor link to header" class="anchor-section link-to-diff" href="clustering.html#clustering"></a></h1>
<div class="section level2 hasAnchor" id="overview-8" number="9.1">
<h2><span class="header-section-number">9.1</span> Overview<a aria-label="Anchor link to header" class="anchor-section link-to-diff" href="clustering.html#overview-8"></a></h2>
<p>As part of exploratory data analysis, it is often helpful to see if there are
meaningful subgroups (or <em>clusters</em>) in the data.
This grouping can be used for many purposes,
such as generating new questions or improving predictive analyses.
This chapter provides an introduction to clustering
using the K-means algorithm,
including techniques to choose the number of clusters.</p>
</div>
<div class="section level2 hasAnchor" id="chapter-learning-objectives-8" number="9.2">
<h2><span class="header-section-number">9.2</span> Chapter learning objectives<a aria-label="Anchor link to header" class="anchor-section link-to-diff" href="clustering.html#chapter-learning-objectives-8"></a></h2>
<p>By the end of the chapter, readers will be able to do the following:</p>
<ul>
<li>Describe a situation in which clustering is an appropriate technique to use,
and what insight it might extract from the data.</li>
<li>Explain the K-means clustering algorithm.</li>
<li>Interpret the output of a K-means analysis.</li>
<li>Differentiate between clustering, classification, and regression.</li>
<li>Identify when it is necessary to scale variables before clustering, and do this using R.</li>
<li>Perform K-means clustering in R using <code>tidymodels</code> workflows.</li>
<li>Use the elbow method to choose the number of clusters for K-means.</li>
<li>Visualize the output of K-means clustering in R using colored scatter plots.</li>
<li>Describe the advantages, limitations and assumptions of the K-means clustering algorithm.</li>
</ul>
</div>
<div class="section level2 hasAnchor" id="clustering-1" number="9.3">
<h2><span class="header-section-number">9.3</span> Clustering<a aria-label="Anchor link to header" class="anchor-section link-to-diff" href="clustering.html#clustering-1"></a></h2>
<p>Clustering is a data analysis technique
involving separating a data set into subgroups of related data.
For example, we might use clustering to separate a
data set of documents into groups that correspond to topics, a data set of
human genetic information into groups that correspond to ancestral
subpopulations, or a data set of online customers into groups that correspond
to purchasing behaviors. Once the data are separated, we can, for example,
use the subgroups to generate new questions about the data and follow up with a
predictive modeling exercise. In this course, clustering will be used only for
exploratory analysis, i.e., uncovering patterns in the data.</p>
<p>Note that clustering is a fundamentally different kind of task
than classification or regression.
In particular, both classification and regression are <em>supervised tasks</em>
where there is a <em>response variable</em> (a category label or value),
and we have examples of past data with labels/values
that help us predict those of future data.
By contrast, clustering is an <em>unsupervised task</em>,
as we are trying to understand
and examine the structure of data without any response variable labels
or values to help us.
This approach has both advantages and disadvantages.
Clustering requires no additional annotation or input on the data.
For example, while it would be nearly impossible to annotate
all the articles on Wikipedia with human-made topic labels,
we can cluster the articles without this information
to find groupings corresponding to topics automatically.
However, given that there is no response variable, it is not as easy to evaluate
the “quality” of a clustering. With classification, we can use a test data set
to assess prediction performance. In clustering, there is not a single good
choice for evaluation. In this book, we will use visualization to ascertain the
quality of a clustering, and leave rigorous evaluation for more advanced
courses.</p>
<p>As in the case of classification,
there are many possible methods that we could use to cluster our observations
to look for subgroups.
In this book, we will focus on the widely used K-means algorithm <span class="citation">(<a href="#ref-kmeans">Lloyd 1982</a>)</span>.
In your future studies, you might encounter hierarchical clustering,
principal component analysis, multidimensional scaling, and more;
see the additional resources section at the end of this chapter
for where to begin learning more about these other methods.</p>
<div style="page-break-after: always;"></div>
<blockquote>
<p><strong>Note:</strong> There are also so-called <em>semisupervised</em> tasks,
where only some of the data come with response variable labels/values,
but the vast majority don’t.
The goal is to try to uncover underlying structure in the data
that allows one to guess the missing labels.
This sort of task is beneficial, for example,
when one has an unlabeled data set that is too large to manually label,
but one is willing to provide a few informative example labels as a “seed”
to guess the labels for all the data.</p>
</blockquote>
</div>
<div class="section level2 hasAnchor" id="an-illustrative-example" number="9.4">
<h2><span class="header-section-number">9.4</span> An illustrative example<a aria-label="Anchor link to header" class="anchor-section link-to-diff" href="clustering.html#an-illustrative-example"></a></h2>
<p>In this chapter we will focus on a data set from
<a href="https://allisonhorst.github.io/palmerpenguins/">the <code>palmerpenguins</code> R package</a> <span class="citation">(<a href="#ref-palmerpenguins">Horst, Hill, and Gorman 2020</a>)</span>. This
data set was collected by Dr. Kristen Gorman and
the Palmer Station, Antarctica Long Term Ecological Research Site, and includes
measurements for adult penguins (Figure <a class="link-to-diff" href="clustering.html#fig:09-penguins">9.1</a>) found near there <span class="citation">(<a href="#ref-penguinpaper">Gorman, Williams, and Fraser 2014</a>)</span>.
Our goal will be to use two
variables—penguin bill and flipper length, both in millimeters—to determine whether
there are distinct types of penguins in our data.
Understanding this might help us with species discovery and classification in a data-driven
way. Note that we have reduced the size of the data set to 18 observations and 2 variables;
this will help us make clear visualizations that illustrate how clustering works for learning purposes.</p>
<div class="figure" style="text-align: center"><span id="fig:09-penguins" style="display:block;"></span>
<img alt="A Gentoo penguin." src="img/clustering/gentoo.jpg" width="60%"/>
<p class="caption">
Figure 9.1: A Gentoo penguin.
</p>
</div>
<p>Before we get started, we will load the <code>tidyverse</code> metapackage
as well as set a random seed.
This will ensure we have access to the functions we need
and that our analysis will be reproducible.
As we will learn in more detail later in the chapter,
setting the seed here is important
because the K-means clustering algorithm uses randomness
when choosing a starting position for each cluster.</p>
<p></p>
<div class="sourceCode" id="cb433"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb433-1"><a class="link-to-diff" href="clustering.html#cb433-1" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb433-2"><a class="link-to-diff" href="clustering.html#cb433-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span></code></pre></div>
<p>Now we can load and preview the <code>penguins</code> data.</p>
<div class="sourceCode" id="cb434"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb434-1"><a class="link-to-diff" href="clustering.html#cb434-1" tabindex="-1"></a>penguins <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">"data/penguins.csv"</span>)</span>
<span id="cb434-2"><a class="link-to-diff" href="clustering.html#cb434-2" tabindex="-1"></a>penguins</span></code></pre></div>
<pre><code>## # A tibble: 18 × 2
##    bill_length_mm flipper_length_mm
##             &lt;dbl&gt;             &lt;dbl&gt;
##  1           39.2               196
##  2           36.5               182
##  3           34.5               187
##  4           36.7               187
##  5           38.1               181
##  6           39.2               190
##  7           36                 195
##  8           37.8               193
##  9           46.5               213
## 10           46.1               215
## 11           47.8               215
## 12           45                 220
## 13           49.1               212
## 14           43.3               208
## 15           46                 195
## 16           46.7               195
## 17           52.2               197
## 18           46.8               189</code></pre>
<p>We will begin by using a version of the data that we have standardized, <code>penguins_standardized</code>,
to illustrate how K-means clustering works (recall standardization from Chapter <a class="link-to-diff" href="classification1.html#classification1">5</a>).
Later in this chapter, we will return to the original <code>penguins</code> data to see how to include standardization automatically
in the clustering pipeline.</p>
<div class="sourceCode" id="cb436"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb436-1"><a class="link-to-diff" href="clustering.html#cb436-1" tabindex="-1"></a>penguins_standardized</span></code></pre></div>
<pre><code>## # A tibble: 18 × 2
##    bill_length_standardized flipper_length_standardized
##                       &lt;dbl&gt;                       &lt;dbl&gt;
##  1                   -0.641                      -0.190
##  2                   -1.14                       -1.33 
##  3                   -1.52                       -0.922
##  4                   -1.11                       -0.922
##  5                   -0.847                      -1.41 
##  6                   -0.641                      -0.678
##  7                   -1.24                       -0.271
##  8                   -0.902                      -0.434
##  9                    0.720                       1.19 
## 10                    0.646                       1.36 
## 11                    0.963                       1.36 
## 12                    0.440                       1.76 
## 13                    1.21                        1.11 
## 14                    0.123                       0.786
## 15                    0.627                      -0.271
## 16                    0.757                      -0.271
## 17                    1.78                       -0.108
## 18                    0.776                      -0.759</code></pre>
<p>Next, we can create a scatter plot using this data set
to see if we can detect subtypes or groups in our data set.</p>
<div style="page-break-after: always;"></div>
<div class="sourceCode" id="cb438"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb438-1"><a class="link-to-diff" href="clustering.html#cb438-1" tabindex="-1"></a><span class="fu">ggplot</span>(penguins_standardized,</span>
<span id="cb438-2"><a class="link-to-diff" href="clustering.html#cb438-2" tabindex="-1"></a>       <span class="fu">aes</span>(<span class="at">x =</span> flipper_length_standardized,</span>
<span id="cb438-3"><a class="link-to-diff" href="clustering.html#cb438-3" tabindex="-1"></a>           <span class="at">y =</span> bill_length_standardized)) <span class="sc">+</span></span>
<span id="cb438-4"><a class="link-to-diff" href="clustering.html#cb438-4" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb438-5"><a class="link-to-diff" href="clustering.html#cb438-5" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">"Flipper Length (standardized)"</span>) <span class="sc">+</span></span>
<span id="cb438-6"><a class="link-to-diff" href="clustering.html#cb438-6" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">"Bill Length (standardized)"</span>) <span class="sc">+</span></span>
<span id="cb438-7"><a class="link-to-diff" href="clustering.html#cb438-7" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">12</span>))</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:10-toy-example-plot" style="display:block;"></span>
<img alt="Scatter plot of standardized bill length versus standardized flipper length." src="_main_files/figure-html/10-toy-example-plot-1.png" width="336"/>
<p class="caption">
Figure 9.2: Scatter plot of standardized bill length versus standardized flipper length.
</p>
</div>
<p>Based on the visualization
in Figure <a class="link-to-diff" href="clustering.html#fig:10-toy-example-plot">9.2</a>,
we might suspect there are a few subtypes of penguins within our data set.
We can see roughly 3 groups of observations in Figure <a class="link-to-diff" href="clustering.html#fig:10-toy-example-plot">9.2</a>,
including:</p>
<ol style="list-style-type: decimal">
<li>a small flipper and bill length group,</li>
<li>a small flipper length, but large bill length group, and</li>
<li>a large flipper and bill length group.</li>
</ol>
<p>Data visualization is a great tool to give us a rough sense of such patterns
when we have a small number of variables.
But if we are to group data—and select the number of groups—as part of
a reproducible analysis, we need something a bit more automated.
Additionally, finding groups via visualization becomes more difficult
as we increase the number of variables we consider when clustering.
The way to rigorously separate the data into groups
is to use a clustering algorithm.
In this chapter, we will focus on the <em>K-means</em> algorithm,
a widely used and often very effective clustering method,
combined with the <em>elbow method</em>
for selecting the number of clusters.
This procedure will separate the data into groups;
Figure <a class="link-to-diff" href="clustering.html#fig:10-toy-example-clustering">9.3</a> shows these groups
denoted by colored scatter points.</p>
<div class="figure" style="text-align: center"><span id="fig:10-toy-example-clustering" style="display:block;"></span>
<img alt="Scatter plot of standardized bill length versus standardized flipper length with colored groups." src="_main_files/figure-html/10-toy-example-clustering-1.png" width="408"/>
<p class="caption">
Figure 9.3: Scatter plot of standardized bill length versus standardized flipper length with colored groups.
</p>
</div>
<p>What are the labels for these groups? Unfortunately, we don’t have any. K-means,
like almost all clustering algorithms, just outputs meaningless “cluster labels”
that are typically whole numbers: 1, 2, 3, etc. But in a simple case like this,
where we can easily visualize the clusters on a scatter plot, we can give
human-made labels to the groups using their positions on
the plot:</p>
<ul>
<li>small flipper length and small bill length (<font color="#D55E00">orange cluster</font>),</li>
<li>small flipper length and large bill length (<font color="#0072B2">blue cluster</font>).</li>
<li>and large flipper length and large bill length (<font color="#F0E442">yellow cluster</font>).</li>
</ul>
<p>Once we have made these determinations, we can use them to inform our species
classifications or ask further questions about our data. For example, we might
be interested in understanding the relationship between flipper length and bill
length, and that relationship may differ depending on the type of penguin we
have.</p>
</div>
<div class="section level2 hasAnchor" id="k-means" number="9.5">
<h2><span class="header-section-number">9.5</span> K-means<a aria-label="Anchor link to header" class="anchor-section link-to-diff" href="clustering.html#k-means"></a></h2>
<div class="section level3 hasAnchor" id="measuring-cluster-quality" number="9.5.1">
<h3><span class="header-section-number">9.5.1</span> Measuring cluster quality<a aria-label="Anchor link to header" class="anchor-section link-to-diff" href="clustering.html#measuring-cluster-quality"></a></h3>
<p>The K-means algorithm is a procedure that groups data into K clusters.
It starts with an initial clustering of the data, and then iteratively
improves it by making adjustments to the assignment of data
to clusters until it cannot improve any further. But how do we measure
the “quality” of a clustering, and what does it mean to improve it?
In K-means clustering, we measure the quality of a cluster
by its <em>within-cluster sum-of-squared-distances</em> (WSSD).
Computing this involves two steps.
First, we find the cluster centers by computing the mean of each variable
over data points in the cluster. For example, suppose we have a
cluster containing four observations, and we are using two variables, <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, to cluster the data.
Then we would compute the coordinates, <span class="math inline">\(\mu_x\)</span> and <span class="math inline">\(\mu_y\)</span>, of the cluster center via</p>
<p><span class="math display">\[\mu_x = \frac{1}{4}(x_1+x_2+x_3+x_4) \quad \mu_y = \frac{1}{4}(y_1+y_2+y_3+y_4).\]</span></p>
<p>In the first cluster from the example, there are 4 data points. These are shown with their cluster center
(standardized flipper length -0.35, standardized bill length 0.99) highlighted
in Figure <a class="link-to-diff" href="clustering.html#fig:10-toy-example-clus1-center">9.4</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:10-toy-example-clus1-center" style="display:block;"></span>
<img alt="Cluster 1 from the penguins_standardized data set example. Observations are small blue points, with the cluster center highlighted as a large blue point with a black outline." src="_main_files/figure-html/10-toy-example-clus1-center-1.png" width="336"/>
<p class="caption">
Figure 9.4: Cluster 1 from the <code>penguins_standardized</code> data set example. Observations are small blue points, with the cluster center highlighted as a large blue point with a black outline.
</p>
</div>
<p>The second step in computing the WSSD is to add up the squared distance
between each point in the cluster and the cluster center.
We use the straight-line / Euclidean distance formula
that we learned about in Chapter <a class="link-to-diff" href="classification1.html#classification1">5</a>.
In the 4-observation cluster example above,
we would compute the WSSD <span class="math inline">\(S^2\)</span> via</p>
<p><span class="math display">\[\begin{align*}
S^2 = \left((x_1 - \mu_x)^2 + (y_1 - \mu_y)^2\right) + \left((x_2 - \mu_x)^2 + (y_2 - \mu_y)^2\right) + \\ \left((x_3 - \mu_x)^2 + (y_3 - \mu_y)^2\right)  +  \left((x_4 - \mu_x)^2 + (y_4 - \mu_y)^2\right).
\end{align*}\]</span></p>
<p>These distances are denoted by lines in Figure <a class="link-to-diff" href="clustering.html#fig:10-toy-example-clus1-dists">9.5</a> for the first cluster of the penguin data example.</p>
<div class="figure" style="text-align: center"><span id="fig:10-toy-example-clus1-dists" style="display:block;"></span>
<img alt="Cluster 1 from the penguins_standardized data set example. Observations are small blue points, with the cluster center highlighted as a large blue point with a black outline. The distances from the observations to the cluster center are represented as black lines." src="_main_files/figure-html/10-toy-example-clus1-dists-1.png" width="336"/>
<p class="caption">
Figure 9.5: Cluster 1 from the <code>penguins_standardized</code> data set example. Observations are small blue points, with the cluster center highlighted as a large blue point with a black outline. The distances from the observations to the cluster center are represented as black lines.
</p>
</div>
<p>The larger the value of <span class="math inline">\(S^2\)</span>, the more spread out the cluster is, since large <span class="math inline">\(S^2\)</span> means
that points are far from the cluster center. Note, however, that “large” is relative to <em>both</em> the
scale of the variables for clustering <em>and</em> the number of points in the cluster. A cluster where points
are very close to the center might still have a large <span class="math inline">\(S^2\)</span> if there are many data points in the cluster.</p>
<p>After we have calculated the WSSD for all the clusters,
we sum them together to get the <em>total WSSD</em>. For our example,
this means adding up all the squared distances for the 18 observations.
These distances are denoted by black lines in
Figure <a class="link-to-diff" href="clustering.html#fig:10-toy-example-all-clus-dists">9.6</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:10-toy-example-all-clus-dists" style="display:block;"></span>
<img alt="All clusters from the penguins_standardized data set example. Observations are small orange, blue, and yellow points with cluster centers denoted by larger points with a black outline. The distances from the observations to each of the respective cluster centers are represented as black lines." src="_main_files/figure-html/10-toy-example-all-clus-dists-1.png" width="408"/>
<p class="caption">
Figure 9.6: All clusters from the <code>penguins_standardized</code> data set example. Observations are small orange, blue, and yellow points with cluster centers denoted by larger points with a black outline. The distances from the observations to each of the respective cluster centers are represented as black lines.
</p>
</div>
<p>Since K-means uses the straight-line distance to measure the quality of a clustering,
it is limited to clustering based on quantitative variables.
However, note that there are variants of the K-means algorithm,
as well as other clustering algorithms entirely,
that use other distance metrics
to allow for non-quantitative data to be clustered.
These are beyond the scope of this book.</p>
<div style="page-break-after: always;"></div>
</div>
<div class="section level3 hasAnchor" id="the-clustering-algorithm" number="9.5.2">
<h3><span class="header-section-number">9.5.2</span> The clustering algorithm<a aria-label="Anchor link to header" class="anchor-section link-to-diff" href="clustering.html#the-clustering-algorithm"></a></h3>
<p>We begin the K-means algorithm by picking K,
and randomly assigning a roughly equal number of observations
to each of the K clusters.
An example random initialization is shown in Figure <a class="link-to-diff" href="clustering.html#fig:10-toy-kmeans-init">9.7</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:10-toy-kmeans-init" style="display:block;"></span>
<img alt="Random initialization of labels." src="_main_files/figure-html/10-toy-kmeans-init-1.png" width="360"/>
<p class="caption">
Figure 9.7: Random initialization of labels.
</p>
</div>
<p>Then K-means consists of two major steps that attempt to minimize the
sum of WSSDs over all the clusters, i.e., the <em>total WSSD</em>:</p>
<ol style="list-style-type: decimal">
<li><strong>Center update:</strong> Compute the center of each cluster.</li>
<li><strong>Label update:</strong> Reassign each data point to the cluster with the nearest center.</li>
</ol>
<p>These two steps are repeated until the cluster assignments no longer change.
We show what the first four iterations of K-means would look like in
Figure <a class="link-to-diff" href="clustering.html#fig:10-toy-kmeans-iter">9.8</a>. There each pair of plots
in each row corresponds to an iteration,
where the left figure in the pair depicts the center update,
and the right figure in the pair depicts the label update (i.e., the reassignment of data to clusters).</p>
<div class="figure" style="text-align: center"><span id="fig:10-toy-kmeans-iter" style="display:block;"></span>
<img alt="First four iterations of K-means clustering on the penguins_standardized example data set. Each pair of plots corresponds to an iteration. Within the pair, the first plot depicts the center update, and the second plot depicts the reassignment of data to clusters. Cluster centers are indicated by larger points that are outlined in black." src="_main_files/figure-html/10-toy-kmeans-iter-1.png" width="768"/>
<p class="caption">
Figure 9.8: First four iterations of K-means clustering on the <code>penguins_standardized</code> example data set. Each pair of plots corresponds to an iteration. Within the pair, the first plot depicts the center update, and the second plot depicts the reassignment of data to clusters. Cluster centers are indicated by larger points that are outlined in black.
</p>
</div>
<p>Note that at this point, we can terminate the algorithm since none of the assignments changed
in the fourth iteration; both the centers and labels will remain the same from this point onward.</p>
<blockquote>
<p><strong>Note:</strong> Is K-means <em>guaranteed</em> to stop at some point, or could it iterate forever? As it turns out,
thankfully, the answer is that K-means is guaranteed to stop after <em>some</em> number of iterations. For the interested reader, the
logic for this has three steps: (1) both the label update and the center update decrease total WSSD in each iteration,
(2) the total WSSD is always greater than or equal to 0, and (3) there are only a finite number of possible
ways to assign the data to clusters. So at some point, the total WSSD must stop decreasing, which means none of the assignments
are changing, and the algorithm terminates.</p>
</blockquote>
</div>
<div class="section level3 hasAnchor" id="random-restarts" number="9.5.3">
<h3><span class="header-section-number">9.5.3</span> Random restarts<a aria-label="Anchor link to header" class="anchor-section link-to-diff" href="clustering.html#random-restarts"></a></h3>
<p>Unlike the classification and regression models we studied in previous chapters, K-means can get “stuck” in a bad solution.
For example, Figure <a class="link-to-diff" href="clustering.html#fig:10-toy-kmeans-bad-init">9.9</a> illustrates an unlucky random initialization by K-means.</p>
<div class="figure" style="text-align: center"><span id="fig:10-toy-kmeans-bad-init" style="display:block;"></span>
<img alt="Random initialization of labels." src="_main_files/figure-html/10-toy-kmeans-bad-init-1.png" width="360"/>
<p class="caption">
Figure 9.9: Random initialization of labels.
</p>
</div>
<p>Figure <a class="link-to-diff" href="clustering.html#fig:10-toy-kmeans-bad-iter">9.10</a> shows what the iterations of K-means would look like with the unlucky random initialization shown in Figure <a class="link-to-diff" href="clustering.html#fig:10-toy-kmeans-bad-init">9.9</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:10-toy-kmeans-bad-iter" style="display:block;"></span>
<img alt="First five iterations of K-means clustering on the penguins_standardized example data set with a poor random initialization. Each pair of plots corresponds to an iteration. Within the pair, the first plot depicts the center update, and the second plot depicts the reassignment of data to clusters. Cluster centers are indicated by larger points that are outlined in black." src="_main_files/figure-html/10-toy-kmeans-bad-iter-1.png" width="768"/>
<p class="caption">
Figure 9.10: First five iterations of K-means clustering on the <code>penguins_standardized</code> example data set with a poor random initialization. Each pair of plots corresponds to an iteration. Within the pair, the first plot depicts the center update, and the second plot depicts the reassignment of data to clusters. Cluster centers are indicated by larger points that are outlined in black.
</p>
</div>
<p>This looks like a relatively bad clustering of the data, but K-means cannot improve it.
To solve this problem when clustering data using K-means, we should randomly re-initialize the labels a few times, run K-means for each initialization,
and pick the clustering that has the lowest final total WSSD.</p>
</div>
<div class="section level3 hasAnchor" id="choosing-k" number="9.5.4">
<h3><span class="header-section-number">9.5.4</span> Choosing K<a aria-label="Anchor link to header" class="anchor-section link-to-diff" href="clustering.html#choosing-k"></a></h3>
<p>In order to cluster data using K-means,
we also have to pick the number of clusters, K.
But unlike in classification, we have no response variable
and cannot perform cross-validation with some measure of model prediction error.
Further, if K is chosen too small, then multiple clusters get grouped together;
if K is too large, then clusters get subdivided.
In both cases, we will potentially miss interesting structure in the data.
Figure <a class="link-to-diff" href="clustering.html#fig:10-toy-kmeans-vary-k">9.11</a> illustrates the impact of K
on K-means clustering of our penguin flipper and bill length data
by showing the different clusterings for K’s ranging from 1 to 9.</p>
<div class="figure" style="text-align: center"><span id="fig:10-toy-kmeans-vary-k" style="display:block;"></span>
<img alt="Clustering of the penguin data for K clusters ranging from 1 to 9. Cluster centers are indicated by larger points that are outlined in black." src="_main_files/figure-html/10-toy-kmeans-vary-k-1.png" width="576"/>
<p class="caption">
Figure 9.11: Clustering of the penguin data for K clusters ranging from 1 to 9. Cluster centers are indicated by larger points that are outlined in black.
</p>
</div>
<p>If we set K less than 3, then the clustering merges separate groups of data; this causes a large
total WSSD, since the cluster center is not close to any of the data in the cluster. On
the other hand, if we set K greater than 3, the clustering subdivides subgroups of data; this does indeed still
decrease the total WSSD, but by only a <em>diminishing amount</em>. If we plot the total WSSD versus the number of
clusters, we see that the decrease in total WSSD levels off (or forms an “elbow shape”) when we reach roughly
the right number of clusters (Figure <a class="link-to-diff" href="clustering.html#fig:10-toy-kmeans-elbow">9.12</a>).</p>
<div class="figure" style="text-align: center"><span id="fig:10-toy-kmeans-elbow" style="display:block;"></span>
<img alt="Total WSSD for K clusters ranging from 1 to 9." src="_main_files/figure-html/10-toy-kmeans-elbow-1.png" width="408"/>
<p class="caption">
Figure 9.12: Total WSSD for K clusters ranging from 1 to 9.
</p>
</div>
</div>
</div>
<div class="section level2 hasAnchor" id="k-means-in-r" number="9.6">
<h2><span class="header-section-number">9.6</span> K-means in R<a aria-label="Anchor link to header" class="anchor-section link-to-diff" href="clustering.html#k-means-in-r"></a></h2>
<p>We can perform K-means clustering in R using a <code>tidymodels</code> workflow similar
to those in the earlier classification and regression chapters.
We will begin by loading the <code>tidyclust</code> library, which contains the necessary
functionality.</p>
<div class="sourceCode" id="cb439"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb439-1"><a class="link-to-diff" href="clustering.html#cb439-1" tabindex="-1"></a><span class="fu">library</span>(tidyclust)</span></code></pre></div>
<p>Returning to the original (unstandardized) <code>penguins</code> data,
recall that K-means clustering uses straight-line
distance to decide which points are similar to
each other. Therefore, the <em>scale</em> of each of the variables in the data
will influence which cluster data points end up being assigned.
Variables with a large scale will have a much larger
effect on deciding cluster assignment than variables with a small scale.
To address this problem, we need to create a recipe that
standardizes our data
before clustering using the <code>step_scale</code> and <code>step_center</code> preprocessing steps.
Standardization will ensure that each variable has a mean
of 0 and standard deviation of 1 prior to clustering.
We will designate that all variables are to be used in clustering via
the model formula <code>~ .</code>.</p>
<blockquote>
<p><strong>Note:</strong> Recipes were originally designed specifically for <em>predictive</em> data
analysis problems—like classification and regression—not clustering
problems. So the functions in R that we use to construct recipes are a little bit
awkward in the setting of clustering In particular, we will have to treat
“predictors” here as if it meant “variables to be used in clustering”. So the
model formula <code>~ .</code> specifies that all variables are “predictors”, i.e., all variables
should be used for clustering. Similarly, when we use the <code>all_predictors()</code> function
in the preprocessing steps, we really mean “apply this step to all variables used for
clustering.”</p>
</blockquote>
<div class="sourceCode" id="cb440"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb440-1"><a class="link-to-diff" href="clustering.html#cb440-1" tabindex="-1"></a>kmeans_recipe <span class="ot">&lt;-</span> <span class="fu">recipe</span>(<span class="sc">~</span> ., <span class="at">data=</span>penguins) <span class="sc">|&gt;</span></span>
<span id="cb440-2"><a class="link-to-diff" href="clustering.html#cb440-2" tabindex="-1"></a>    <span class="fu">step_scale</span>(<span class="fu">all_predictors</span>()) <span class="sc">|&gt;</span></span>
<span id="cb440-3"><a class="link-to-diff" href="clustering.html#cb440-3" tabindex="-1"></a>    <span class="fu">step_center</span>(<span class="fu">all_predictors</span>())</span>
<span id="cb440-4"><a class="link-to-diff" href="clustering.html#cb440-4" tabindex="-1"></a>kmeans_recipe</span></code></pre></div>
<pre><code>## 
## ── Recipe ──────────
## 
## ── Inputs 
## Number of variables by role
## predictor: 2
## 
## ── Operations 
## • Scaling for: all_predictors()
## • Centering for: all_predictors()</code></pre>
<p>To indicate that we are performing K-means clustering, we will use the <code>k_means</code>
model specification. We will use the <code>num_clusters</code> argument to specify the number
of clusters (here we choose K = 3), and specify that we are using the <code>"stats"</code> engine.</p>
<div class="sourceCode" id="cb442"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb442-1"><a class="link-to-diff" href="clustering.html#cb442-1" tabindex="-1"></a>kmeans_spec <span class="ot">&lt;-</span> <span class="fu">k_means</span>(<span class="at">num_clusters =</span> <span class="dv">3</span>) <span class="sc">|&gt;</span></span>
<span id="cb442-2"><a class="link-to-diff" href="clustering.html#cb442-2" tabindex="-1"></a>    <span class="fu">set_engine</span>(<span class="st">"stats"</span>)</span>
<span id="cb442-3"><a class="link-to-diff" href="clustering.html#cb442-3" tabindex="-1"></a>kmeans_spec</span></code></pre></div>
<pre><code>## K Means Cluster Specification (partition)
## 
## Main Arguments:
##   num_clusters = 3
## 
## Computational engine: stats</code></pre>
<p>To actually run the K-means clustering, we combine the recipe and model
specification in a workflow, and use the <code>fit</code> function. Note that the
K-means algorithm uses a random initialization of assignments; but since
we set the random seed earlier, the clustering will be reproducible.</p>
<div class="sourceCode" id="cb444"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb444-1"><a class="link-to-diff" href="clustering.html#cb444-1" tabindex="-1"></a>kmeans_fit <span class="ot">&lt;-</span> <span class="fu">workflow</span>() <span class="sc">|&gt;</span></span>
<span id="cb444-2"><a class="link-to-diff" href="clustering.html#cb444-2" tabindex="-1"></a>    <span class="fu">add_recipe</span>(kmeans_recipe) <span class="sc">|&gt;</span></span>
<span id="cb444-3"><a class="link-to-diff" href="clustering.html#cb444-3" tabindex="-1"></a>    <span class="fu">add_model</span>(kmeans_spec) <span class="sc">|&gt;</span></span>
<span id="cb444-4"><a class="link-to-diff" href="clustering.html#cb444-4" tabindex="-1"></a>    <span class="fu">fit</span>(<span class="at">data =</span> penguins)</span>
<span id="cb444-5"><a class="link-to-diff" href="clustering.html#cb444-5" tabindex="-1"></a></span>
<span id="cb444-6"><a class="link-to-diff" href="clustering.html#cb444-6" tabindex="-1"></a>kmeans_fit</span></code></pre></div>
<pre><code>## ══ Workflow [trained] ══════════
## Preprocessor: Recipe
## Model: k_means()
## 
## ── Preprocessor ──────────
## 2 Recipe Steps
## 
## • step_scale()
## • step_center()
## 
## ── Model ──────────
## K-means clustering with 3 clusters of sizes 4, 6, 8
## 
## Cluster means:
##   bill_length_mm flipper_length_mm
## 1      0.9858721        -0.3524358
## 2      0.6828058         1.2606357
## 3     -1.0050404        -0.7692589
## 
## Clustering vector:
##  [1] 3 3 3 3 3 3 3 3 2 2 2 2 2 2 1 1 1 1
## 
## Within cluster sum of squares by cluster:
## [1] 1.098928 1.247042 2.121932
##  (between_SS / total_SS =  86.9 %)
## 
## Available components:
## 
## [1] "cluster"      "centers"      "totss"        "withinss"     "tot.withinss"
## [6] "betweenss"    "size"         "iter"         "ifault"</code></pre>
<p>As you can see above, the fit object has a lot of information
that can be used to visualize the clusters, pick K, and evaluate the total WSSD.
Let’s start by visualizing the clusters as a colored scatter plot! In
order to do that, we first need to augment our
original data frame with the cluster assignments. We can
achieve this using the <code>augment</code> function from <code>tidyclust</code>.</p>
<div class="sourceCode" id="cb446"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb446-1"><a class="link-to-diff" href="clustering.html#cb446-1" tabindex="-1"></a>clustered_data <span class="ot">&lt;-</span> kmeans_fit <span class="sc">|&gt;</span></span>
<span id="cb446-2"><a class="link-to-diff" href="clustering.html#cb446-2" tabindex="-1"></a>                    <span class="fu">augment</span>(penguins)</span>
<span id="cb446-3"><a class="link-to-diff" href="clustering.html#cb446-3" tabindex="-1"></a>clustered_data</span></code></pre></div>
<pre><code>## # A tibble: 18 × 3
##    bill_length_mm flipper_length_mm .pred_cluster
##             &lt;dbl&gt;             &lt;dbl&gt; &lt;fct&gt;        
##  1           39.2               196 Cluster_1    
##  2           36.5               182 Cluster_1    
##  3           34.5               187 Cluster_1    
##  4           36.7               187 Cluster_1    
##  5           38.1               181 Cluster_1    
##  6           39.2               190 Cluster_1    
##  7           36                 195 Cluster_1    
##  8           37.8               193 Cluster_1    
##  9           46.5               213 Cluster_2    
## 10           46.1               215 Cluster_2    
## 11           47.8               215 Cluster_2    
## 12           45                 220 Cluster_2    
## 13           49.1               212 Cluster_2    
## 14           43.3               208 Cluster_2    
## 15           46                 195 Cluster_3    
## 16           46.7               195 Cluster_3    
## 17           52.2               197 Cluster_3    
## 18           46.8               189 Cluster_3</code></pre>
<p>Now that we have the cluster assignments included in the <code>clustered_data</code> tidy data frame, we can
visualize them as shown in Figure <a class="link-to-diff" href="clustering.html#fig:10-plot-clusters-2">9.13</a>.
Note that we are plotting the <em>un-standardized</em> data here; if we for some reason wanted to
visualize the <em>standardized</em> data from the recipe, we would need to use the <code>bake</code> function
to obtain that first.</p>
<div class="sourceCode" id="cb448"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb448-1"><a class="link-to-diff" href="clustering.html#cb448-1" tabindex="-1"></a>cluster_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(clustered_data,</span>
<span id="cb448-2"><a class="link-to-diff" href="clustering.html#cb448-2" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">x =</span> flipper_length_mm,</span>
<span id="cb448-3"><a class="link-to-diff" href="clustering.html#cb448-3" tabindex="-1"></a>      <span class="at">y =</span> bill_length_mm,</span>
<span id="cb448-4"><a class="link-to-diff" href="clustering.html#cb448-4" tabindex="-1"></a>      <span class="at">color =</span> .pred_cluster),</span>
<span id="cb448-5"><a class="link-to-diff" href="clustering.html#cb448-5" tabindex="-1"></a>  <span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb448-6"><a class="link-to-diff" href="clustering.html#cb448-6" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb448-7"><a class="link-to-diff" href="clustering.html#cb448-7" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Flipper Length"</span>,</span>
<span id="cb448-8"><a class="link-to-diff" href="clustering.html#cb448-8" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">"Bill Length"</span>,</span>
<span id="cb448-9"><a class="link-to-diff" href="clustering.html#cb448-9" tabindex="-1"></a>       <span class="at">color =</span> <span class="st">"Cluster"</span>) <span class="sc">+</span></span>
<span id="cb448-10"><a class="link-to-diff" href="clustering.html#cb448-10" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"steelblue"</span>,</span>
<span id="cb448-11"><a class="link-to-diff" href="clustering.html#cb448-11" tabindex="-1"></a>                                <span class="st">"darkorange"</span>,</span>
<span id="cb448-12"><a class="link-to-diff" href="clustering.html#cb448-12" tabindex="-1"></a>                                <span class="st">"goldenrod1"</span>)) <span class="sc">+</span></span>
<span id="cb448-13"><a class="link-to-diff" href="clustering.html#cb448-13" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">12</span>))</span>
<span id="cb448-14"><a class="link-to-diff" href="clustering.html#cb448-14" tabindex="-1"></a></span>
<span id="cb448-15"><a class="link-to-diff" href="clustering.html#cb448-15" tabindex="-1"></a>cluster_plot</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:10-plot-clusters-2" style="display:block;"></span>
<img alt="The data colored by the cluster assignments returned by K-means." src="_main_files/figure-html/10-plot-clusters-2-1.png" width="408"/>
<p class="caption">
Figure 9.13: The data colored by the cluster assignments returned by K-means.
</p>
</div>
<p>As mentioned above, we also need to select K by finding
where the “elbow” occurs in the plot of total WSSD versus the number of clusters.
We can obtain the total WSSD (<code>tot.withinss</code>) from our
clustering with 3 clusters using the <code>glance</code> function.</p>
<div class="sourceCode" id="cb449"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb449-1"><a class="link-to-diff" href="clustering.html#cb449-1" tabindex="-1"></a><span class="fu">glance</span>(kmeans_fit)</span></code></pre></div>
<pre><code>## # A tibble: 1 × 4
##   totss tot.withinss betweenss  iter
##   &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;
## 1    34         4.47      29.5     2</code></pre>
<p>To calculate the total WSSD for a variety of Ks, we will
create a data frame with a column named <code>num_clusters</code> with rows containing
each value of K we want to run K-means with (here, 1 to 9).</p>
<div class="sourceCode" id="cb451"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb451-1"><a class="link-to-diff" href="clustering.html#cb451-1" tabindex="-1"></a>penguin_clust_ks <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">num_clusters =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">9</span>)</span>
<span id="cb451-2"><a class="link-to-diff" href="clustering.html#cb451-2" tabindex="-1"></a>penguin_clust_ks</span></code></pre></div>
<pre><code>## # A tibble: 9 × 1
##   num_clusters
##          &lt;int&gt;
## 1            1
## 2            2
## 3            3
## 4            4
## 5            5
## 6            6
## 7            7
## 8            8
## 9            9</code></pre>
<p>Then we construct our model specification again, this time
specifying that we want to tune the <code>num_clusters</code> parameter.</p>
<div class="sourceCode" id="cb453"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb453-1"><a class="link-to-diff" href="clustering.html#cb453-1" tabindex="-1"></a>kmeans_spec <span class="ot">&lt;-</span> <span class="fu">k_means</span>(<span class="at">num_clusters =</span> <span class="fu">tune</span>()) <span class="sc">|&gt;</span></span>
<span id="cb453-2"><a class="link-to-diff" href="clustering.html#cb453-2" tabindex="-1"></a>    <span class="fu">set_engine</span>(<span class="st">"stats"</span>)</span>
<span id="cb453-3"><a class="link-to-diff" href="clustering.html#cb453-3" tabindex="-1"></a>kmeans_spec</span></code></pre></div>
<pre><code>## K Means Cluster Specification (partition)
## 
## Main Arguments:
##   num_clusters = tune()
## 
## Computational engine: stats</code></pre>
<p>We combine the recipe and specification in a workflow, and then
use the <code>tune_cluster</code> function to run K-means on each of the different
settings of <code>num_clusters</code>. The <code>grid</code> argument controls which values of
K we want to try—in this case, the values from 1 to 9 that are
stored in the <code>penguin_clust_ks</code> data frame. We set the <code>resamples</code>
argument to <code>apparent(penguins)</code> to tell K-means to run on the whole
data set for each value of <code>num_clusters</code>. Finally, we collect the results
using the <code>collect_metrics</code> function.</p>
<div class="sourceCode" id="cb455"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb455-1"><a class="link-to-diff" href="clustering.html#cb455-1" tabindex="-1"></a>kmeans_results <span class="ot">&lt;-</span> <span class="fu">workflow</span>() <span class="sc">|&gt;</span></span>
<span id="cb455-2"><a class="link-to-diff" href="clustering.html#cb455-2" tabindex="-1"></a>    <span class="fu">add_recipe</span>(kmeans_recipe) <span class="sc">|&gt;</span></span>
<span id="cb455-3"><a class="link-to-diff" href="clustering.html#cb455-3" tabindex="-1"></a>    <span class="fu">add_model</span>(kmeans_spec) <span class="sc">|&gt;</span></span>
<span id="cb455-4"><a class="link-to-diff" href="clustering.html#cb455-4" tabindex="-1"></a>    <span class="fu">tune_cluster</span>(<span class="at">resamples =</span> <span class="fu">apparent</span>(penguins), <span class="at">grid =</span> penguin_clust_ks) <span class="sc">|&gt;</span></span>
<span id="cb455-5"><a class="link-to-diff" href="clustering.html#cb455-5" tabindex="-1"></a>    <span class="fu">collect_metrics</span>()</span>
<span id="cb455-6"><a class="link-to-diff" href="clustering.html#cb455-6" tabindex="-1"></a>kmeans_results</span></code></pre></div>
<pre><code>## # A tibble: 18 × 7
##    num_clusters .metric          .estimator   mean     n std_err .config        
##           &lt;int&gt; &lt;chr&gt;            &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;          
##  1            1 sse_total        standard   34         1      NA Preprocessor1_…
##  2            1 sse_within_total standard   34         1      NA Preprocessor1_…
##  3            2 sse_total        standard   34         1      NA Preprocessor1_…
##  4            2 sse_within_total standard   10.9       1      NA Preprocessor1_…
##  5            3 sse_total        standard   34         1      NA Preprocessor1_…
##  6            3 sse_within_total standard    4.47      1      NA Preprocessor1_…
##  7            4 sse_total        standard   34         1      NA Preprocessor1_…
##  8            4 sse_within_total standard    3.54      1      NA Preprocessor1_…
##  9            5 sse_total        standard   34         1      NA Preprocessor1_…
## 10            5 sse_within_total standard    2.23      1      NA Preprocessor1_…
## 11            6 sse_total        standard   34         1      NA Preprocessor1_…
## 12            6 sse_within_total standard    1.75      1      NA Preprocessor1_…
## 13            7 sse_total        standard   34         1      NA Preprocessor1_…
## 14            7 sse_within_total standard    2.06      1      NA Preprocessor1_…
## 15            8 sse_total        standard   34         1      NA Preprocessor1_…
## 16            8 sse_within_total standard    2.46      1      NA Preprocessor1_…
## 17            9 sse_total        standard   34         1      NA Preprocessor1_…
## 18            9 sse_within_total standard    0.906     1      NA Preprocessor1_…</code></pre>
<p>The total WSSD results correspond to the <code>mean</code> column when the <code>.metric</code> variable is equal to <code>sse_within_total</code>.
We can obtain a tidy data frame with this information using <code>filter</code> and <code>mutate</code>.</p>
<div class="sourceCode" id="cb457"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb457-1"><a class="link-to-diff" href="clustering.html#cb457-1" tabindex="-1"></a>kmeans_results <span class="ot">&lt;-</span> kmeans_results <span class="sc">|&gt;</span></span>
<span id="cb457-2"><a class="link-to-diff" href="clustering.html#cb457-2" tabindex="-1"></a>    <span class="fu">filter</span>(.metric <span class="sc">==</span> <span class="st">"sse_within_total"</span>) <span class="sc">|&gt;</span></span>
<span id="cb457-3"><a class="link-to-diff" href="clustering.html#cb457-3" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">total_WSSD =</span> mean) <span class="sc">|&gt;</span></span>
<span id="cb457-4"><a class="link-to-diff" href="clustering.html#cb457-4" tabindex="-1"></a>    <span class="fu">select</span>(num_clusters, total_WSSD)</span>
<span id="cb457-5"><a class="link-to-diff" href="clustering.html#cb457-5" tabindex="-1"></a>kmeans_results</span></code></pre></div>
<pre><code>## # A tibble: 9 × 2
##   num_clusters total_WSSD
##          &lt;int&gt;      &lt;dbl&gt;
## 1            1     34    
## 2            2     10.9  
## 3            3      4.47 
## 4            4      3.54 
## 5            5      2.23 
## 6            6      1.75 
## 7            7      2.06 
## 8            8      2.46 
## 9            9      0.906</code></pre>
<p>Now that we have <code>total_WSSD</code> and <code>num_clusters</code> as columns in a data frame, we can make a line plot
(Figure <a class="link-to-diff" href="clustering.html#fig:10-plot-choose-k">9.14</a>) and search for the “elbow” to find which value of K to use. </p>
<div class="sourceCode" id="cb459"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb459-1"><a class="link-to-diff" href="clustering.html#cb459-1" tabindex="-1"></a>elbow_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(kmeans_results, <span class="fu">aes</span>(<span class="at">x =</span> num_clusters, <span class="at">y =</span> total_WSSD)) <span class="sc">+</span></span>
<span id="cb459-2"><a class="link-to-diff" href="clustering.html#cb459-2" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb459-3"><a class="link-to-diff" href="clustering.html#cb459-3" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb459-4"><a class="link-to-diff" href="clustering.html#cb459-4" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">"K"</span>) <span class="sc">+</span></span>
<span id="cb459-5"><a class="link-to-diff" href="clustering.html#cb459-5" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">"Total within-cluster sum of squares"</span>) <span class="sc">+</span></span>
<span id="cb459-6"><a class="link-to-diff" href="clustering.html#cb459-6" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">9</span>) <span class="sc">+</span></span>
<span id="cb459-7"><a class="link-to-diff" href="clustering.html#cb459-7" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">12</span>))</span>
<span id="cb459-8"><a class="link-to-diff" href="clustering.html#cb459-8" tabindex="-1"></a></span>
<span id="cb459-9"><a class="link-to-diff" href="clustering.html#cb459-9" tabindex="-1"></a>elbow_plot</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:10-plot-choose-k" style="display:block;"></span>
<img alt="A plot showing the total WSSD versus the number of clusters." src="_main_files/figure-html/10-plot-choose-k-1.png" width="408"/>
<p class="caption">
Figure 9.14: A plot showing the total WSSD versus the number of clusters.
</p>
</div>
<p>It looks like 3 clusters is the right choice for this data.
But why is there a “bump” in the total WSSD plot here?
Shouldn’t total WSSD always decrease as we add more clusters?
Technically yes, but remember: K-means can get “stuck” in a bad solution.
Unfortunately, for K = 8 we had an unlucky initialization
and found a bad clustering!
We can help prevent finding a bad clustering
by trying a few different random initializations
via the <code>nstart</code> argument in the model specification.
Here we will try using 10 restarts.</p>
<div class="sourceCode" id="cb460"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb460-1"><a class="link-to-diff" href="clustering.html#cb460-1" tabindex="-1"></a>kmeans_spec <span class="ot">&lt;-</span> <span class="fu">k_means</span>(<span class="at">num_clusters =</span> <span class="fu">tune</span>()) <span class="sc">|&gt;</span></span>
<span id="cb460-2"><a class="link-to-diff" href="clustering.html#cb460-2" tabindex="-1"></a>    <span class="fu">set_engine</span>(<span class="st">"stats"</span>, <span class="at">nstart =</span> <span class="dv">10</span>)</span>
<span id="cb460-3"><a class="link-to-diff" href="clustering.html#cb460-3" tabindex="-1"></a>kmeans_spec</span></code></pre></div>
<pre><code>## K Means Cluster Specification (partition)
## 
## Main Arguments:
##   num_clusters = tune()
## 
## Engine-Specific Arguments:
##   nstart = 10
## 
## Computational engine: stats</code></pre>
<p>Now if we rerun the same workflow with the new model specification,
K-means clustering will be performed <code>nstart = 10</code> times for each value of K.
The <code>collect_metrics</code> function will then pick the best clustering of the 10 runs for each value of K,
and report the results for that best clustering.
Figure <a class="link-to-diff" href="clustering.html#fig:10-choose-k-nstart">9.15</a> shows the resulting
total WSSD plot from using 10 restarts; the bump is gone and the total WSSD decreases as expected.
The more times we perform K-means clustering,
the more likely we are to find a good clustering (if one exists).
What value should you choose for <code>nstart</code>? The answer is that it depends
on many factors: the size and characteristics of your data set,
as well as how powerful your computer is.
The larger the <code>nstart</code> value the better from an analysis perspective,
but there is a trade-off that doing many clusterings
could take a long time.
So this is something that needs to be balanced.</p>
<div class="sourceCode" id="cb462"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb462-1"><a class="link-to-diff" href="clustering.html#cb462-1" tabindex="-1"></a>kmeans_results <span class="ot">&lt;-</span> <span class="fu">workflow</span>() <span class="sc">|&gt;</span></span>
<span id="cb462-2"><a class="link-to-diff" href="clustering.html#cb462-2" tabindex="-1"></a>    <span class="fu">add_recipe</span>(kmeans_recipe) <span class="sc">|&gt;</span></span>
<span id="cb462-3"><a class="link-to-diff" href="clustering.html#cb462-3" tabindex="-1"></a>    <span class="fu">add_model</span>(kmeans_spec) <span class="sc">|&gt;</span></span>
<span id="cb462-4"><a class="link-to-diff" href="clustering.html#cb462-4" tabindex="-1"></a>    <span class="fu">tune_cluster</span>(<span class="at">resamples =</span> <span class="fu">apparent</span>(penguins), <span class="at">grid =</span> penguin_clust_ks) <span class="sc">|&gt;</span></span>
<span id="cb462-5"><a class="link-to-diff" href="clustering.html#cb462-5" tabindex="-1"></a>    <span class="fu">collect_metrics</span>() <span class="sc">|&gt;</span></span>
<span id="cb462-6"><a class="link-to-diff" href="clustering.html#cb462-6" tabindex="-1"></a>    <span class="fu">filter</span>(.metric <span class="sc">==</span> <span class="st">"sse_within_total"</span>) <span class="sc">|&gt;</span></span>
<span id="cb462-7"><a class="link-to-diff" href="clustering.html#cb462-7" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">total_WSSD =</span> mean) <span class="sc">|&gt;</span></span>
<span id="cb462-8"><a class="link-to-diff" href="clustering.html#cb462-8" tabindex="-1"></a>    <span class="fu">select</span>(num_clusters, total_WSSD)</span>
<span id="cb462-9"><a class="link-to-diff" href="clustering.html#cb462-9" tabindex="-1"></a></span>
<span id="cb462-10"><a class="link-to-diff" href="clustering.html#cb462-10" tabindex="-1"></a>elbow_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(kmeans_results, <span class="fu">aes</span>(<span class="at">x =</span> num_clusters, <span class="at">y =</span> total_WSSD)) <span class="sc">+</span></span>
<span id="cb462-11"><a class="link-to-diff" href="clustering.html#cb462-11" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb462-12"><a class="link-to-diff" href="clustering.html#cb462-12" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb462-13"><a class="link-to-diff" href="clustering.html#cb462-13" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">"K"</span>) <span class="sc">+</span></span>
<span id="cb462-14"><a class="link-to-diff" href="clustering.html#cb462-14" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">"Total within-cluster sum of squares"</span>) <span class="sc">+</span></span>
<span id="cb462-15"><a class="link-to-diff" href="clustering.html#cb462-15" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">9</span>) <span class="sc">+</span></span>
<span id="cb462-16"><a class="link-to-diff" href="clustering.html#cb462-16" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">12</span>))</span>
<span id="cb462-17"><a class="link-to-diff" href="clustering.html#cb462-17" tabindex="-1"></a></span>
<span id="cb462-18"><a class="link-to-diff" href="clustering.html#cb462-18" tabindex="-1"></a>elbow_plot</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:10-choose-k-nstart" style="display:block;"></span>
<img alt="A plot showing the total WSSD versus the number of clusters when K-means is run with 10 restarts." src="_main_files/figure-html/10-choose-k-nstart-1.png" width="408"/>
<p class="caption">
Figure 9.15: A plot showing the total WSSD versus the number of clusters when K-means is run with 10 restarts.
</p>
</div>
</div>
<div class="section level2 hasAnchor" id="exercises-8" number="9.7">
<h2><span class="header-section-number">9.7</span> Exercises<a aria-label="Anchor link to header" class="anchor-section link-to-diff" href="clustering.html#exercises-8"></a></h2>
<p>Practice exercises for the material covered in this chapter<ins class="diff"> </ins>can be found in the<ins class="diff">accompanying</ins> <del class="diff">accompanying</del><a href="https://worksheets.datasciencebook.ca">worksheets repository</a><ins class="diff"> in</ins>
<del class="diff">in </del>the “Clustering” row.<ins class="diff"> </ins>You can <ins class="diff">preview</ins><del class="diff">launch</del> <ins class="diff">anon-interactive</ins><del class="diff">an</del> <del class="diff">interactive </del>version of the worksheet <ins class="diff">for</ins><del class="diff">in</del> <ins class="diff">this</ins><del class="diff">your</del> <ins class="diff">chapter</ins><del class="diff">browser</del> by clicking <ins class="diff">“view</ins><del class="diff">the “launch binder” button.</del>
<ins class="diff">worksheet.”</ins><del class="diff">You</del> <ins class="diff">To</ins><del class="diff">can</del> <ins class="diff">work</ins><del class="diff">also</del> <ins class="diff">on</ins><del class="diff">preview</del> <ins class="diff">the</ins><del class="diff">a</del> <ins class="diff">exercises</ins><del class="diff">non-interactive</del> <ins class="diff">interactively,</ins><del class="diff">version</del> <ins class="diff">follow</ins><del class="diff">of</del> the <ins class="diff">instructions</ins><del class="diff">worksheet</del> <ins class="diff">in</ins><del class="diff">by clicking “view worksheet.”</del>
<ins class="diff">the</ins><del class="diff">If</del> <ins class="diff">worksheets</ins><del class="diff">you</del> <ins class="diff">repository</ins><del class="diff">instead</del> <del class="diff">decide </del>to download <ins class="diff">all</ins><del class="diff">the</del> <ins class="diff">worksheets,</ins><del class="diff">worksheet</del> and <del class="diff">run it on your own machine,make sure to </del>follow the<del class="diff"> </del>instructions for computer setup<ins class="diff"> </ins>found in Chapter <a class="link-to-diff" href="setup.html#setup">13</a>. This will ensure<del class="diff"> </del>that the automated feedback<ins class="diff"> </ins>and guidance that the worksheets provide will<del class="diff"> </del>function as intended.</p>
</div>
<div class="section level2 hasAnchor" id="additional-resources-5" number="9.8">
<h2><span class="header-section-number">9.8</span> Additional resources<a aria-label="Anchor link to header" class="anchor-section link-to-diff" href="clustering.html#additional-resources-5"></a></h2>
<ul>
<li>Chapter 10 of <em>An Introduction to Statistical
Learning</em> <span class="citation">(<a href="#ref-james2013introduction">James et al. 2013</a>)</span> provides a
great next stop in the process of learning about clustering and unsupervised
learning in general. In the realm of clustering specifically, it provides a
great companion introduction to K-means, but also covers <em>hierarchical</em>
clustering for when you expect there to be subgroups, and then subgroups within
subgroups, etc., in your data. In the realm of more general unsupervised
learning, it covers <em>principal components analysis (PCA)</em>, which is a very
popular technique for reducing the number of predictors in a data set.</li>
</ul>
</div>
</div>
<h3>References<a aria-label="Anchor link to header" class="anchor-section" href="references.html#references"></a></h3>
<div class="references csl-bib-body hanging-indent" id="refs">
<div class="csl-entry" id="ref-penguinpaper">
Gorman, Kristen, Tony Williams, and William Fraser. 2014. <span>“Ecological Sexual Dimorphism and Environmental Variability Within a Community of <span>A</span>ntarctic Penguins (Genus Pygoscelis).”</span> <em>PLoS ONE</em> 9 (3).
</div>
<div class="csl-entry" id="ref-palmerpenguins">
Horst, Allison, Alison Hill, and Kristen Gorman. 2020. <em><span class="nocase">palmerpenguins</span>: Palmer Archipelago Penguin Data</em>. <a href="https://allisonhorst.github.io/palmerpenguins/">https://allisonhorst.github.io/palmerpenguins/</a>.
</div>
<div class="csl-entry" id="ref-james2013introduction">
James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning</em>. 1st ed. Springer. <a href="https://www.statlearning.com/">https://www.statlearning.com/</a>.
</div>
<div class="csl-entry" id="ref-kmeans">
Lloyd, Stuart. 1982. <span>“Least Square Quantization in <span>PCM</span>.”</span> <em>IEEE Transactions on Information Theory</em> 28 (2): 129–37.
</div>
</div>
</section>
</div>
</div>
</div>
<a aria-label="Previous page" class="navigation navigation-prev link-to-diff" href="regression2.html"><i class="fa fa-angle-left"></i></a>
<a aria-label="Next page" class="navigation navigation-next link-to-diff" href="inference.html"><i class="fa fa-angle-right"></i></a>
</div>
</div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>
</html>
